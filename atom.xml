<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hero&#39;s notebooks</title>
  <icon>https://www.gravatar.com/avatar/ba13505eb0c4bc0e7771060ab0cb2b31</icon>
  <subtitle>Sometimes naive.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chenzk1.github.io/"/>
  <updated>2019-11-19T02:29:09.705Z</updated>
  <id>https://chenzk1.github.io/</id>
  
  <author>
    <name>Hero</name>
    <email>chenzk666@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>稀疏编码</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81/"/>
    <id>https://chenzk1.github.io/2019/11/19/稀疏编码/</id>
    <published>2019-11-19T02:23:10.455Z</published>
    <updated>2019-11-19T02:29:09.705Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>稀疏表示</p><ul><li><p>稀疏：p的维度较大，且尽可能使α中的系数多为0（只有少数的non-zero elements）</p></li><li><p>作用：</p><p>1）拥有更强大表达能力(Representation Power）（去掉冗余的信息）</p><p>2）拥有识别和约束能力（Discriminative, or Regularization Power）（加速运算，减小存储（可以通过一些表达方式来表达稀疏矩阵））</p></li><li><p>用途：</p><p>1）信号处理领域：自然界中的信号低频居多，高频基本都是噪声。因此在做基矩阵时，表达系数只在少数低频基上较大，高频基的系数基本都接近于0.所以在一些问题，例如逆问题中，从一些损坏或者噪声中提取某个信号，相当于解不定方程，不加约束的话会有很多可行解。于是在解决这类问题的时候，会加上约束，即稀疏性约束</p><p>2）推荐系统：用户行为，例如用户评价是一个低秩矩阵</p><p>3）深度学习：l1正则抗拟合</p></li><li><p>表示：如下图，原来的特征为x，编码后为α。相当于某向量在基下的坐标。</p></li></ul><p><img src="https://img-blog.csdn.net/20151223214504471" alt></p></li></ol><ul><li><p>实现：</p><p>使得表示前后误差尽可能小：<br>$$<br>J(D, α)=|D α-x|<em>{2}<br>$$<br>加入限制条件——稀疏：<br>$$<br>J(D, α)=|D α-x|</em>{2}+\lambda|α|_{1}<br>$$</p></li><li><p>理解：</p><p>1）有点像l1正则化</p><p>2）降维是原向量空间的子集，稀疏是原向量空间子集的集合</p></li></ul><ol start="2"><li><p>非NN中特征稀疏的解决</p><p>1）推荐系统FM模型解决</p><p>2）</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;p&gt;稀疏表示&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;稀疏：p的维度较大，且尽可能使α中的系数多为0（只有少数的non-zero elements）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;作用：&lt;/p&gt;
&lt;p&gt;1）拥有更强大表达能力(Representation Powe
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>梯度下降及其优化算法</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
    <id>https://chenzk1.github.io/2019/11/19/梯度下降及其优化算法/</id>
    <published>2019-11-19T02:23:10.440Z</published>
    <updated>2019-11-19T02:28:04.632Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GD及其变体"><a href="#GD及其变体" class="headerlink" title="GD及其变体"></a>GD及其变体</h1><h2 id="Batch-GD-Vanilla-GD"><a href="#Batch-GD-Vanilla-GD" class="headerlink" title="Batch GD / Vanilla GD"></a>Batch GD / Vanilla GD</h2><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%2B1%7D%3D+%5Ctheta_t+-+%5Ceta+g_t" alt></p><p>ALL SAMPLES</p><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>求梯度的时候，要经过所有样本，计算量会很大，随机梯度下降是<strong>每次迭代随机选取</strong>一个样本计算梯度。</p><p>每次用到的梯度方差大，收敛过程不稳定</p><p>问题是可能到不了局部最优。</p><h2 id="mini-batch-gd"><a href="#mini-batch-gd" class="headerlink" title="mini-batch gd"></a>mini-batch gd</h2><p>降低了更新参数的方差，收敛过程更稳定</p><h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><h3 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h3><p>$$<br>v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}<br>$$<br>$$<br>\begin{array}{l}{v_{0}=0} \ {v_{1}=\beta v_{0}+(1-\beta) \theta_{1}} \ {v_{2}=\beta v_{1}+(1-\beta) \theta_{2}=\beta\left(\beta v_{0}+\theta_{1}\right)+(1-\beta) \theta_{2}} \ {\vdots} \ {v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}=\sum_{i=1}^{t} \beta^{t-i}(1-\beta) \theta_{t}}\end{array}<br>$$</p><ul><li>使用之前的观测值以及当前观测值的平均来作为所需要的值，以达到平滑的目的。距离当前时刻越近的观测值对求得移动平均值的影响越大。</li><li>指数的意思是：按照公式归纳，之前观测值的权值为设定常数的次方。</li></ul><h3 id="动量梯度下降"><a href="#动量梯度下降" class="headerlink" title="动量梯度下降"></a>动量梯度下降</h3><p>$$<br>\begin{aligned} v_{t} &amp;=\beta v_{t-1}+(1-\beta) \nabla J(\theta)<em>{t} \ \theta &amp;=\theta-\alpha v</em>{t} \end{aligned}<br>$$<br>也即：<br>$$<br>\begin{aligned} v_{t} &amp;=\gamma v_{t-1}+\eta \nabla J(\theta)<em>{t} \ \theta &amp;=\theta-v</em>{t} \end{aligned}<br>$$</p><ul><li>动量梯度下降即对梯度进行指数加权平均，以达到平滑的作用，利用到了历史梯度值。</li><li>平滑：例如对于有些参数梯度正负不断震荡，导致loss也在这些参数对应的方向上不断震荡，那加入历史值则会在一定程度上抵消震荡；而如果某些参数的变化是一致的，平均后不会产生太大影响，只是使得具体值变化变小了而已。</li></ul><h2 id="Nesterov-Accelerated-Gradient-NAG"><a href="#Nesterov-Accelerated-Gradient-NAG" class="headerlink" title="Nesterov Accelerated Gradient,NAG"></a>Nesterov Accelerated Gradient,NAG</h2><ul><li>之前的方法是利用梯度方向来进行参数更新，Nesterov对梯度加入了预测功能：即预测参数未来的近似位置，并利用此位置处的参数梯度来进行更新当前参数</li></ul><p>$$<br>\begin{aligned} v_{t} &amp;=\gamma v_{t-1}+\eta \nabla J\left(\theta-\gamma v_{t-1}\right) \ \theta &amp;=\theta-v_{t} \end{aligned}<br>$$<br><img src="https://img2018.cnblogs.com/blog/439761/201903/439761-20190313101736137-755682989.jpg" alt></p><ul><li>动量下降为蓝色，短蓝为当前梯度，常蓝为动量；NAG为，先在之前的动量项（棕色）前进一步，并计算此时的梯度，然后用这个梯度与之前的动量项指数平均。</li><li>相当于先判断下一阶段可能要去的地方，然后使用那个地方的梯度，相当于提前预测，做个修正，少走冤枉路。</li></ul><h2 id="Adagrad-Adaptive-Gradient"><a href="#Adagrad-Adaptive-Gradient" class="headerlink" title="Adagrad(Adaptive Gradient)"></a>Adagrad(Adaptive Gradient)</h2><ul><li>之前的参数更新时，不同的参数使用相同的步长，为了对不同的参数使用不同的步长，引入Adagrad，当梯度大时，步长小，梯度小时，步长大。</li><li>AdaGrad对每个变量更新时，利用该变量历史积累的梯度来修正其学习速率。这样，已经下降的很多的变量则会有小的学习率，而下降较少的变量则仍然保持较大的学习率。<br>$$<br>\theta_{(t+1, i)}=\theta_{(t, i)}-\frac{\eta}{\sqrt{\sum_{\tau=1}^{t} \nabla J\left(\theta_{i}\right)<em>{\tau}+\epsilon}} \cdot \nabla J\left(\theta</em>{i}\right)_{t+1}<br>$$</li><li>缺点：<ul><li>仍然需要设初始学习率；</li><li>学习率不断衰减，到后期会很小，导致训练过早停止。</li></ul></li></ul><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>解决Adagrad学习率衰减的问题</p><ul><li>学习率衰减：引入动量（指数加权平均），引入超参γ，在累积梯度的<strong>平方</strong>项近似衰减：</li></ul><p>$$<br>\begin{array}{l}{s_{(t, i)}=\gamma s_{(t-1, i)}+(1-\gamma) \nabla J\left(\theta_{i}\right)<em>{t} \odot \nabla J\left(\theta</em>{i}\right)<em>{t}} \ {\theta</em>{(t, i)}=\theta_{(t-1, i)}-\frac{\eta}{\sqrt{s_{(t, i)}+\epsilon}} \odot \nabla J\left(\theta_{i}\right)_{t}}\end{array}<br>$$</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><strong>RMSprop+Momentum</strong></p><p>即梯度的指数平均和梯度平方的指数平均的结合：<br>$$<br>\begin{aligned} m_{t} &amp;=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \ v_{t} &amp;=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \end{aligned}<br>$$<br>mt和vt的初始值为0，训练初期可能较小，因此需要对其放大：<br>$$<br>\begin{aligned} \hat{m}<em>{t} &amp;=\frac{m</em>{t}}{1-\beta_{1}^{t}} \ \hat{v}<em>{t} &amp;=\frac{v</em>{t}}{1-\beta_{2}^{t}} \end{aligned}<br>$$<br>即：<br>$$<br>\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}<em>{t}}+\epsilon} \hat{m}</em>{t}<br>$$<br>作者建议β1设置为0.9,β2设置为0.999，取ϵ=10−8。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>从Vanilla GD到动量项，解决了梯度震荡的问题；<strong>Momentum</strong></li><li>加入历史梯度累计作为修正项，解决了每个参数的步长一样的问题；<strong>Adagrad</strong></li><li>一阶梯度累计项变二阶梯度累计项的指数加权平均，解决了Adagrad训练后期学习率小的问题；<strong>RMSprop</strong></li><li><strong>Momentum+RMSprop=Adam</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GD及其变体&quot;&gt;&lt;a href=&quot;#GD及其变体&quot; class=&quot;headerlink&quot; title=&quot;GD及其变体&quot;&gt;&lt;/a&gt;GD及其变体&lt;/h1&gt;&lt;h2 id=&quot;Batch-GD-Vanilla-GD&quot;&gt;&lt;a href=&quot;#Batch-GD-Vanilla-G
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统FM/FMM</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9FFM%E3%80%81FFM/"/>
    <id>https://chenzk1.github.io/2019/11/19/推荐系统FM、FFM/</id>
    <published>2019-11-19T02:23:10.440Z</published>
    <updated>2019-11-19T02:28:57.793Z</updated>
    
    <content type="html"><![CDATA[<p>CTR预估时，除了单特征，还要对特征进行组合。组合方法：FM系列和Tree系列。</p><ol><li><p>FM：为了解决特征稀疏</p><p>one-hot编码后，特征空间非常稀疏，进而造成一些问题。</p></li><li><p>线性模型：<br>$$<br>y=\omega_{0}+\sum_{i=1}^{n} \omega_{i} x_{i}<br>$$</p></li><li><p>FM：加入线性组合，只加入两个特征的组合：</p></li></ol><p>$$<br>y=\omega_{0}+\sum_{i=1}^{n} \omega_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \omega_{i j} x_{i} x_{j}<br>$$</p><pre><code>- 问题：特征稀疏，xi, xj都不为0的情况比较少，所以wij无法训练得出</code></pre><ul><li><p>解法：</p><ul><li>共n个特征，对每个特征分量xi引入辅助向量Vi，Vi的长度为k<br>$$<br>\mathbf{V}=\left(\begin{array}{cccc}{v_{11}} &amp; {v_{12}} &amp; {\cdots} &amp; {v_{1 k}} \ {v_{21}} &amp; {v_{22}} &amp; {\cdots} &amp; {v_{2 k}} \ {\vdots} &amp; {\vdots} &amp; {} &amp; {\vdots} \ {v_{n 1}} &amp; {v_{n 2}} &amp; {\cdots} &amp; {v_{n k}}\end{array}\right)<em>{n \times k}=\left(\begin{array}{c}{\mathbf{v}</em>{1}} \ {\mathbf{v}<em>{2}} \ {\vdots} \ {\mathbf{v}</em>{n}}\end{array}\right)<br>$$<br>有：<br>$$<br>\hat{\mathbf{W}}=\mathbf{V} \mathbf{V}^{T}=\left(\begin{array}{c}{\mathbf{v}<em>{1}} \ {\mathbf{v}</em>{2}} \ {\vdots} \ {\mathbf{v}<em>{n}}\end{array}\right)\left(\begin{array}{cccc}{\mathbf{v}</em>{1}} \ {\mathbf{v}<em>{1}^{T}} &amp; {\mathbf{v}</em>{2}^{T}} &amp; {\cdots} &amp; {\left.\mathbf{v}<em>{n}^{T}\right)} \ {\mathbf{v}</em>{n}} &amp; {} &amp; {} &amp; {}\end{array}\right.<br>$$<br>从求解w变为求解v：<br>$$<br>\begin{aligned} &amp; \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{j}\right\rangle x_{i} x_{j} \=&amp; \frac{1}{2} \sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{j}\right\rangle x_{i} x_{j}-\frac{1}{2} \sum_{i=1}^{n}\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{i}\right\rangle x_{i} x_{i} \=&amp; \frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n}\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{j, f}, x_{i} x_{j}-\sum_{i=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{i, f} x_{i} x_{i}\right)\right.\=&amp; \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \=&amp; \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \end{aligned}<br>$$<br>即：从求ab变为求解<br>$$<br>\left((a+b+c)^{2}-a^{2}-b^{2}-c^{2}\right.<br>$$</li></ul></li></ul><ol start="4"><li><p>FFM</p><ul><li><p>加入field的概念：FM中，v针对特征是经过one-hot后处理后的，有些特征是从同一个类别特征one-hot得来的</p></li><li><p>隐向量不仅与特征有关，也与field有关</p></li><li><p>n个特征属于f个field，FM相当于把所有特征归属于一个field时的FFM。</p></li><li><p>FFM有nfk个参数，FM有nk个</p></li><li><p>$$<br>y(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i ,f_j}, \mathbf{v}<em>{j ,f</em>{i}}\right\rangle x_{i} x_{j}<br>$$</p></li><li><p>一种实现：将问题定义为分类问题，使用加入正则项的logloss（1，-1的logloss）<br>$$<br>\min <em>{\mathbf{w}} \sum</em>{i=1}^{L} \log \left(1+\exp \left{-y_{i} \phi\left(\mathbf{w}, \mathbf{x}_{i}\right)\right}\right)+\frac{\lambda}{2}|\mathbf{w}|^{2}<br>$$<br>利用SGD进行训练。</p></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;CTR预估时，除了单特征，还要对特征进行组合。组合方法：FM系列和Tree系列。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;FM：为了解决特征稀疏&lt;/p&gt;
&lt;p&gt;one-hot编码后，特征空间非常稀疏，进而造成一些问题。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;线性模型：&lt;br&gt;$$&lt;br
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="推荐系统" scheme="https://chenzk1.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统实验方法与评测</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95%E4%B8%8E%E8%AF%84%E6%B5%8B/"/>
    <id>https://chenzk1.github.io/2019/11/19/推荐系统实验方法与评测/</id>
    <published>2019-11-19T02:23:10.440Z</published>
    <updated>2019-11-19T02:28:34.033Z</updated>
    
    <content type="html"><![CDATA[<h1 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h1><h2 id="离线"><a href="#离线" class="headerlink" title="离线"></a>离线</h2><p>训练集、测试集</p><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>不需要对实际系统的控制</td><td>无法计算商业上关心的指标</td></tr><tr><td>无需用户参与实验</td><td>离线与在线的差距</td></tr><tr><td>速度快，可以测试大量算法</td></tr></tbody></table><h2 id="用户调查"><a href="#用户调查" class="headerlink" title="用户调查"></a>用户调查</h2><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>可以获得体现主观感受的指标</td><td>招募测试用户代价大</td></tr><tr><td>相对在线实验风险低</td><td>难以组织大量用户</td></tr></tbody></table><h2 id="在线"><a href="#在线" class="headerlink" title="在线"></a>在线</h2><p>ABtest</p><h1 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h1><ul><li><p>用户满意度</p><p>用户调查 / 在线实验</p></li><li><p>准确度</p><ul><li><p>评分预测: rmse, mae</p></li><li><p>Top N: recall, precision<br>$$<br>\begin{array}{l}{\quad \text { Recall }=\frac{\sum_{u v}|R(u) \cap T(u)|}{\sum_{u=U}|T(u)|}} \ {\text { Precision }=\frac{\sum_{u v}|R(u) \cap T(u)|}{\sum_{u=v}|R(u)|}}\end{array}<br>$$</p></li></ul></li><li><p>覆盖率：需体现推荐系统挖掘长尾的能力</p><ul><li><p>信息熵 H = - sigma p(i)logp(i)</p></li><li><p>基尼系数<br>$$<br>G=\frac{1}{n-1} \sum_{j=1}^{n}(2 j-n-1) p\left(i_{j}\right)<br>$$<br>这里ij是按照物品流行度p()从小到大排序的物品列表中第j个物品</p><p><img src="/Users/didi/Library/Application Support/typora-user-images/image-20190924155235973.png" alt="image-20190924155235973"></p></li></ul></li><li><p>多样性：用户兴趣是广泛的</p></li><li>新颖性：给用户推荐其之前没有听过或看过的物品</li><li>精细度：推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个 推荐结果。</li><li>信任度<ul><li>提高推荐系统透明度</li><li>考虑用户社交网络信息，利用好友信息给用户作推荐，并用好友进行推荐解释</li></ul></li><li>实时性：物品的时效性、用户兴趣的时效性</li><li>健壮性（鲁棒性）：反作弊</li><li>商业目标</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;实验方法&quot;&gt;&lt;a href=&quot;#实验方法&quot; class=&quot;headerlink&quot; title=&quot;实验方法&quot;&gt;&lt;/a&gt;实验方法&lt;/h1&gt;&lt;h2 id=&quot;离线&quot;&gt;&lt;a href=&quot;#离线&quot; class=&quot;headerlink&quot; title=&quot;离线&quot;&gt;&lt;/a&gt;离线&lt;/h
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="推荐系统" scheme="https://chenzk1.github.io/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
  </entry>
  
  <entry>
    <title>滴滴实习总结</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
    <id>https://chenzk1.github.io/2019/11/19/实习总结/</id>
    <published>2019-11-19T02:23:10.424Z</published>
    <updated>2019-11-19T02:27:33.922Z</updated>
    
    <content type="html"><![CDATA[<h1 id="信用分"><a href="#信用分" class="headerlink" title="信用分"></a>信用分</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>平台上每天对乘客有10+w差评投诉，专快<strong>坏账</strong>每月3千万，<strong>司机nps -17%</strong>，<strong>乘客诈骗</strong>引起pr事件，专车乘客封禁但依然能活跃在其他业务线等，乘客管控一片空白，业务迫切希望有一个抓手来解决这些问题。在这个背景下，乘客行为分应运而生。</p><p>乘客行为分的本质是<strong>按照乘客在平台上的行为对其进行分层</strong>。乘客行为分是构建良好司机／乘客生态的一部分，<strong>能为平台提供更多GMV／运力，为司机提供更好体验（NPS）</strong>，为乘客提供更好体验（NPS）。</p><ul><li>平台资损：补券、有责取消（取消费用、取消次数、取消率）</li><li>平台体验：司机投诉（司机nps）、乘客投诉（虚假投诉）</li></ul><p>定位：<strong>乘客行为分是连接B端和C端的工具。其中，C端是乘客，公司内的B端主要包括客服部门 （乘客管控方向）／乘客补贴部门（补贴方向）和乘客运营部门（拉新方向）。</strong></p><p>价值：<strong>乘客行为分的核心价值在于为B端提供用户分层服务，为C端提供用户权益（差异化服务）。</strong></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul><li>准入（乘客运营）：<ul><li>乘客行为分替代芝麻分作为海棠湾（出行单车事业部）和黑马（电单车）的准入条件，分别提高<strong>31%</strong>、<strong>30%</strong>的转化率</li></ul></li><li>免押（乘客运营）：<ul><li>粤港车减低未播率 提高乘客体验</li></ul></li><li>降资损（乘客管控）：<ul><li>恶意补券</li><li>低信用乘客不给预付</li></ul></li><li>乘客管控与乘客教育：<ul><li>海棠湾和黑马 行为分露出，坏行为下降，好行为上升</li><li>深圳地区乘客教育与乘客管控</li></ul></li><li><strong>司机体验优化（低信用乘客豁免）</strong></li><li>分单倾斜</li><li>用作其他业务的参考依据：为滴滴金融等借贷业务提供决策的依据</li></ul><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p><img src="C:\Users\Chenzk\Documents\Learning\滴滴\cmd\image\image2019-8-2_11-47-48.png" alt="image2019-8-2_11-47-48"></p><p>收益评估：正向收益与负向收益</p><h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>履约表现、行为健康度、净贡献值、身份特征</p><h3 id="履约表现"><a href="#履约表现" class="headerlink" title="履约表现"></a>履约表现</h3><ul><li>标签：表现期内是否违约（网约车业务逾期30天（平台对坏账的定义）以上，滴水贷违约）</li><li>特征：履约能力（固定资产、流动资产、平台流水（发单次数、gmv、完单次数））和信用历史（个人借贷还款、逾期（逾期支付次数、逾期支付金额、滴水贷还款总金额、当前逾期未还本金、逾期次数、历史最大逾期天数）等情况的统计信息）</li><li>baseline：网约车规则、粤港车规则</li><li>样本<ul><li>观察期内有行为的乘客，没有行为发生给默认分</li><li>抽样：全量粤港车（1万）+全量滴水贷（45万）+随机抽样网约车（ 200万）</li></ul></li><li>分箱<ul><li>等频、等距、卡方</li></ul></li><li>模型：见卡方分布及卡方检验、评分卡</li><li>评估：（大盘抽样或整个大盘，例如ks、gini等可以直接用整个大盘数据）（如果不合常理问题一般是woc值的分布规律不正常）<ul><li>准召（固定一个调整另一个，注重于精确率，粤港车注重于召回率（因为未付订单较少，则降资损更重要，所以查全））、f1、ks（0.42）、auc（0.82）</li><li>分布：平滑 正态 有无异常值，如突出的毛刺（对应了分箱或者数据中的异常值，例如企业支付、代叫号，如果企业支付是按乘客算的，那就算看起来是异常数据也要加入）</li><li>分数与GMV、坏账、未播率、逾期单数、渗透率、滴水贷逾期金额、滴水贷逾期次数的分布</li><li>应用于业务时的影响面、未播率、资损delta</li></ul></li><li>思考：<ul><li>label选择的方案：逾期31天和逾期29天在特征方面差别很小，如何设计？考虑去除灰色部分，即训练时只选取逾期25天以内的作为label=0，35天以上作为label=1，但测试的时候全部加入，特征和测试的label不变，依然为30天，即只是为了提高模型的表现</li></ul></li></ul><h3 id="行为健康度"><a href="#行为健康度" class="headerlink" title="行为健康度"></a>行为健康度</h3><ul><li><p>Label：根据乘客在历史X个月的行为表现预测在未来X个月的行为表现。好：label=0，坏：label=1.（其中历史x选取6，未来x选取1）</p><p>其中坏行为是有责投诉(费用类有责投诉和服务类有责投诉)、有责取消和迟到。</p><p><strong>label选取方式：</strong></p><ol><li>人群范围：历史六个月发单&gt;=10单 &amp; 未来一个月发单&gt;=5单</li><li>label1：未来一个月的坏行为发生率&gt;=0.4，其中坏行为发生率=（有责投诉数+有责取消数+迟到）/ 发单数。</li><li>label0：未来一个月的坏行为发生次数&lt;=1</li></ol></li><li><p>数据选取：</p><ol><li>label1：选取满足条件的全部数据量。</li><li>label0：从满足条件的数据中随机选取14w。正：负=1：1。</li><li>数据划分：训练集：测试集=8:2</li></ol></li><li><p>特征：[快专出豪顺]完单数、投诉数、费用有责投诉、服务类有责投诉（不包括取消和费用投诉）、有责取消次数、无责取消次数、应答后取消次数、对司机好评、差评、迟到…</p></li><li><p>评估：</p><ul><li>虚假投诉乘客管控：高投诉高补偿（发单&gt;=10 &amp; 投诉工单&gt;=5 &amp;（投诉率&gt;=0.3 or 投诉数&gt;= 15） &amp; (补偿率&gt;= 0.3 or 补偿订单数&gt;= 10) &amp; 用户价值&lt;0 ）乘客分布集中在低分段</li></ul></li><li><p>应用：</p><ul><li>区域乘客教育：取消、迟到、投诉、费用投诉、费用有责投诉的分布、与分数的分布、率的分布。最终使用规则+分数的方式做乘客教育。其中规则和分数的阈值选取：考虑影响面（8万左右）。取消次数大于等于3次，分数小于等于620分，对应人数80934，作为教育的对象；迟到次数大于等于3次，分数小于等于569，影响人数19239，作为教育的对象。<strong>高取消/高迟到乘客次月的重犯次数依旧很高，说明教育必要性</strong></li><li>乘客管控：石锤虚假投诉封禁</li></ul></li></ul><h3 id="分数更新"><a href="#分数更新" class="headerlink" title="分数更新"></a>分数更新</h3><ol><li>更新方式<ol><li>全量更新：选取固定时间周期，滑动窗口，重新训练模型。<ol><li>优势：模型是用最新的数据产生的模型，产生的分数在固定维度内具有区分度。</li><li>劣势：每次生成新的分数，分数变动不稳定。</li></ol></li><li>增量更新：在初始分数的基础上，根据增量的数据做分数的增量变动。<ol><li>优势：分数变动波动性小。</li><li>劣势：增量分数的累积会使区分度变差。</li></ol></li></ol></li><li>更新周期<ol><li>参考选取label的周期。label选取是未来一个月，更新周期是一个月更新一次。</li><li>更新周期太频繁，分数变动太快，不够稳定。更新周期太长，分数不够准确体现乘客当前的行为。</li></ol></li></ol><p>为了使模型向后兼容&amp;&amp;打通流程，在更新时选取增量更新的方式，初试分数的计算和增量更新时的分数需要具有可解释性。</p><h2 id="问题与挑战"><a href="#问题与挑战" class="headerlink" title="问题与挑战"></a>问题与挑战</h2><h3 id="多个业务场景"><a href="#多个业务场景" class="headerlink" title="多个业务场景"></a>多个业务场景</h3><p>分多维度、维度下分子维度，不同维度与子维度都会生成相应的分数，对不同维度设置不同阈值，从而建立个性化的门槛，就可以赋能于不同的业务场景（<strong>用户权益／乘客管控／司机体验</strong>）。</p><h3 id="可解释性"><a href="#可解释性" class="headerlink" title="可解释性"></a>可解释性</h3><p>乘客行为分在最终理想态下需要透传。这意味着，需要考虑行为分（加分和扣分）的可解释性。对用户透传分数时有以下几种不同的力度和方案。</p><ol><li>直接透传当前的总分（类似芝麻分）。这种形式可控性强，但用户对于自己的分数怎么计算的会有疑惑，透传的话会有大量的用户进线。</li><li>透传每种行为项的分数。<ul><li>这种形式可解释性强，用户对于分数的构成很清晰，但是会造成用户恶意刷分的情况。同时不在计算范围的行为，用户可能不会care，从而缺乏管控的能力。</li><li>这种形式可控性很差。如果后期每项行为的分值发生变化，分数的稳定性会受到用户的质疑。</li></ul></li></ol><ol><li>两种形式的折衷（类似内测的橙信值）。对于坏行为给用户完全透传出来，使用户意识到哪些形式要明令禁止的；对于好行为给出加分的总和。</li><li>两种形式的折衷（类似内测的橙信值）。对于坏行为给用户完全透传出来，使用户意识到哪些形式要明令禁止的；对于好行为给出加分的总和。</li></ol><p>增强行为分可解释性，意味着牺牲策略空间，并且增加了刷分的风险。所以在一些维度（例如乘客身份特征：如实名认证等）上需要尽可能提供高可解释性，而在另一些维度（例如行为健康度：如被司机投诉等）上则需要尽可能保持高度灵活。</p><p><strong><em>xgboost更具解释性？存疑 每个叶子都有权重</em></strong></p><h3 id="数据稀疏"><a href="#数据稀疏" class="headerlink" title="数据稀疏"></a>数据稀疏</h3><ul><li>采样方式，解决有些特征难以用到的问题</li></ul><p>调研了推荐系统中的数据稀疏解决方式，见wiki</p><p>两篇论文：</p><p><a href="http://link.zhihu.com/?target=https%3A//github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BAirbnb%20Embedding%5D%20Real-time%20Personalization%20using%20Embeddings%20for%20Search%20Ranking%20at%20Airbnb%20%28Airbnb%202018%29.pdf" target="_blank" rel="noopener">Real-time Personalization using Embeddings for Search Ranking at Airbnb (Airbnb 2018)</a></p><p><a href="http://link.zhihu.com/?target=https%3A//github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BAlibaba%20Embedding%5D%20Billion-scale%20Commodity%20Embedding%20for%20E-commerce%20Recommendation%20in%20Alibaba%20%28Alibaba%202018%29.pdf" target="_blank" rel="noopener">[Alibaba Embedding] Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba (Alibaba 2018)</a></p><h2 id="体会"><a href="#体会" class="headerlink" title="体会"></a>体会</h2><ul><li><p>数据：自己取；大规模；</p></li><li><p>建模目标、label：自定义</p></li><li><p>问题：采样方法、数据稀疏</p></li><li><p>Q：在一个具体项目中，前期通过简单的规则作为baseline的必要性在哪？</p><p>A：1）前期实现时，模型缺乏对照，规则是最简单的可以提供此对照的实现；</p><p>2）公司中不仅有算法/策略部门，还有产品、运营、开发部门，为什么产品、运营、开发给一份规则不能解决问题？算法相对规则可以带来多少delta收益？这也是方案评估的时候会被考虑的几个标准，换言之，baseline作为对照也是对算法/策略部门存在必要性的一个说明。</p><p>Q：机器学习模型中，有些标签本身就是离散的，例如男/女，有房/无房等，而有些标签是连续特征离散化的，例如青年/中年，再例如判责中有有责/无责，也有判不清，这些标签是人为从某些连续标签经过离散化得到的。在拟合连续标签离散化得到的标签的时候，就存在一个灰色样本的问题：人为设定的界线附近的标签区分度不高（例如长得很像狗的猫和长得很像猫的狗就很难给出区分的定义），导致在训练集中打label的时候很难打；就算打好了label，预测时灰色样本的置信度也会比较低，这该如何解决？</p><p>A：1）二分类问题模型输出是一个概率值，再通过设置阈值的方式可以规避掉这些灰色样本的存在，但这种方法是回避了问题，并没有真正判清样本；2）有些分类模型是通过设置阈值来分类的，也有些是通过排序来分类，应用排序的方式可以避免概率值接近这种衡量方式上的判不清；3）有些连续标签的离散化相当于模糊集合的应用，因此在模型中恰当地引入模糊集合理论也可以实现对问题的解决。</p><p>A2：工业界中的数据是自己产生的，label有时是真实数据，有时是要自己打的，信用分三个子维度的问题，是通过已有数据打label，这个打的过程本身就存在打不清楚的问题；如果在训练集中剔除灰色样本，则—-→训练集测试集分布不一致，用不存在预测存在，出现很多问题。</p><p>泓州提到，分布要一致这玩意是理论，我们现在在实践，实践以实际业务需求以及效果为衡量王道，其他不care，能否满足业务需求，能否达到更好的效果才是所关心的，所以分布一致这条原则可以舍去。</p><p>感想：没那么多条条框框，思维要开阔，限制不应该是理论前提的不满足，而是业务、效果。？业务、效果也别限制了吧，怎么爽怎么来。</p><p><img src="C:\Users\Chenzk\Documents\Learning\滴滴\cmd\image\微信图片_20191115101713.png" alt="微信图片_20191115101713"></p></li><li><p>业务方：模型的本质是用历史行为来判责当次发生的问题，说服力不强。</p><p>业务方要求定制化分数?</p><p>可解释性？</p><p>Q：为什么要有可解释性，对这个分数负责不就好了，用户真的有申诉信用分的必要吗？</p><p>A：权益激励/限制权益等不需要，类似于芝麻分信用分不够不能免费骑车也不会有人申诉。但是一些更大的处置动作，例如准出、处罚等需要有可解释性。不care前85%的好人好在哪里，关注的是尾部分布的15%有问题的乘客问题在哪里，以及问题会导致什么 ——&gt; 管控</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;信用分&quot;&gt;&lt;a href=&quot;#信用分&quot; class=&quot;headerlink&quot; title=&quot;信用分&quot;&gt;&lt;/a&gt;信用分&lt;/h1&gt;&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="实习" scheme="https://chenzk1.github.io/tags/%E5%AE%9E%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>贪心算法</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"/>
    <id>https://chenzk1.github.io/2019/11/19/贪心算法/</id>
    <published>2019-11-19T02:23:10.424Z</published>
    <updated>2019-11-19T02:27:50.033Z</updated>
    
    <content type="html"><![CDATA[<h4 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h4><p>顾名思义，贪心算法总是作出在当前看来最好的选择。也就是说贪心算法并不从整体最优考虑，它所作出的选择只是在某种意义上的局部最优选择。在一些情况下，即使贪心算法不能得到整体最优解，其最终结果却是最优解的很好近似。</p><ul><li><p>贪心选择性质。所谓贪心选择性质是指所求问题的整体最优解可以通过一系列局部最优的选择，即贪心选择来达到。这是贪心算法可行的第一个基本要素，也是贪心算法与动态规划算法的主要区别。</p><p>动态规划算法通常以自底向上的方式解各子问题，而贪心算法则通常以自顶向下的方式进行，以迭代的方式作出相继的贪心选择，每作一次贪心选择就将所求问题简化为规模更小的子问题。</p><p>对于一个具体问题，要确定它是否具有贪心选择性质，必须证明每一步所作的贪心选择最终导致问题的整体最优解。</p></li><li><p>当一个问题的最优解包含其子问题的最优解时，称此问题具有最优子结构性质。问题的最优子结构性质是该问题可用动态规划算法或贪心算法求解的关键特征。</p></li></ul><h4 id="活动时间安排"><a href="#活动时间安排" class="headerlink" title="活动时间安排"></a>活动时间安排</h4><p>设有N个活动时间集合，每个活动都要使用同一个资源，比如说会议场，而且同一时间内只能有一个活动使用，每个活动都有一个使用活动的开始si和结束时间fi，即他的使用区间为（si,fi）,现在要求你分配活动占用时间表，即哪些活动占用该会议室，哪些不占用，使得他们不冲突，要求是尽可能多的使参加的活动最大化，即所占时间区间最大化。</p><p><img src="https://img-my.csdn.net/uploads/201303/29/1364554174_8902.jpg" alt="img"></p><ul><li>重点：<strong>结束时间按顺序排列</strong>。时间从前往后推移，若以当时时间为开始时间的活动的开始时间≥前一个活动的结束时间，选择此活动即可 –&gt; 原因是<strong>结束时间按顺序排列</strong>。</li></ul><figure class="highlight plain"><figcaption><span>main(int argc, char* argv[])</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">int main(int argc,char* argv[])</span><br><span class="line">&#123;</span><br><span class="line">int s[11] =&#123;1,3,0,5,3,5,6,8,8,2,12&#125;;</span><br><span class="line">int f[11] =&#123;4,5,6,7,8,9,10,11,12,13,14&#125;;</span><br><span class="line"> </span><br><span class="line">bool mark[11] = &#123;0&#125;;</span><br><span class="line"> </span><br><span class="line">GreedyChoose(11,s,f,mark);</span><br><span class="line">for(int i=0;i&lt;11;i++)</span><br><span class="line">if(mark[i])</span><br><span class="line">cout&lt;&lt;i&lt;&lt;&quot; &quot;;</span><br><span class="line">system(&quot;pause&quot;);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">void GreedyChoose(int len,int *s,int *f,bool *flag)</span><br><span class="line">&#123;</span><br><span class="line">flag[0] = true;</span><br><span class="line">int j = 0;</span><br><span class="line">for(int i=1;i&lt;len;++i)</span><br><span class="line">if(s[i] &gt;= f[j])</span><br><span class="line">&#123;</span><br><span class="line">flag[i] = true;</span><br><span class="line">j = i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="线段覆盖"><a href="#线段覆盖" class="headerlink" title="线段覆盖"></a>线段覆盖</h4><p>在一维空间中告诉你N条线段的起始坐标与终止坐标，要求求出这些线段一共覆盖了多大的长度。</p><p><img src="https://img-my.csdn.net/uploads/201303/30/1364630415_9404.jpg" alt="img"></p><p>重点：<strong>将线段按起始坐标排序</strong>。</p><figure class="highlight plain"><figcaption><span>main(int argc, char* argv[])</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">int s[10] = &#123;2,3,4,5,6,7,8,9,10,11&#125;;</span><br><span class="line">int f[10] = &#123;3,5,7,6,9,8,12,10,13,15&#125;;</span><br><span class="line">int TotalLength = (3-2);                 </span><br><span class="line"> </span><br><span class="line">for(int i=1,int j=0; i&lt;10 ; ++i)</span><br><span class="line">&#123;</span><br><span class="line">if(s[i] &gt;= f[j])</span><br><span class="line">&#123;</span><br><span class="line">TotalLength += (f[i]-s[i]);</span><br><span class="line">j = i;</span><br><span class="line">&#125;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">if(f[i] &lt;= f[j])</span><br><span class="line">continue;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">TotalLength += f[i] - f[j];</span><br><span class="line">j = i;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">cout&lt;&lt;TotalLength&lt;&lt;endl;</span><br><span class="line">system(&quot;pause&quot;);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="找零钱问题"><a href="#找零钱问题" class="headerlink" title="找零钱问题"></a>找零钱问题</h4><p>贪心：每次选择可选的最大面值的钱。</p><p>本质上还是<strong>排序</strong>。</p><h4 id="Leetcode"><a href="#Leetcode" class="headerlink" title="Leetcode:"></a>Leetcode:</h4><p>给定一个数组，它的第 <em>i</em> 个元素是一支给定股票第 <em>i</em> 天的价格。</p><p>设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。</p><p><strong>注意：</strong>你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。</p><p><a href="https://leetcode-cn.com/explore/interview/card/top-interview-questions-easy/1/array/22/" target="_blank" rel="noopener">https://leetcode-cn.com/explore/interview/card/top-interview-questions-easy/1/array/22/</a></p><p>使用贪心算法的一个假设：同一天可以卖出又可以买入</p><figure class="highlight plain"><figcaption><span>Solution</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def maxProfit(self, prices: List[int]) -&gt; int:</span><br><span class="line">    ans = 0</span><br><span class="line">    for i in range(len(prices)-1):</span><br><span class="line">        if prices[i+1]&gt;prices[i]:</span><br><span class="line">            ans += prices[i+1]-prices[i]</span><br><span class="line">    return ans</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;贪心算法&quot;&gt;&lt;a href=&quot;#贪心算法&quot; class=&quot;headerlink&quot; title=&quot;贪心算法&quot;&gt;&lt;/a&gt;贪心算法&lt;/h4&gt;&lt;p&gt;顾名思义，贪心算法总是作出在当前看来最好的选择。也就是说贪心算法并不从整体最优考虑，它所作出的选择只是在某种意义上的局部最优
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="DS" scheme="https://chenzk1.github.io/tags/DS/"/>
    
  </entry>
  
  <entry>
    <title>评分卡</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E8%AF%84%E5%88%86%E5%8D%A1/"/>
    <id>https://chenzk1.github.io/2019/11/19/评分卡/</id>
    <published>2019-11-19T02:23:10.346Z</published>
    <updated>2019-11-19T02:26:55.827Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>模型流程</p><ul><li><p>分箱：等频、等距、卡方（掌握卡方分箱的原理和操作方式：卡方值最小的相邻区间合并（卡方越小说明俩区间区别越小，卡方衡量都是区间中label的分布））</p></li><li><p>根据分箱求得woe=ln(# good占比/ # bad占比)</p></li><li><p>LR模型获得W</p></li><li><p>score = A - B*ln(odds) </p><ul><li>直观理解：对一条数据来说，其属于好样本的概率与属于坏样本的概率的比值越大，其分数越大。只不过用了LR来间接表示这个概率，且未直接用到特征值，用到的是对label有可解释性的woe值。</li><li>A, B: 设置比率为θ0（也就是odds）的特定点分值为P0，比率为2θ0的点的分值为P0+PDO。带入上面公式可得到：</li></ul><p>$$<br>\left{\begin{array}{ll}{P_{0}} &amp; {=A-B \ln \left(\theta_{0}\right)} \ {P_{0}+P D O} &amp; {=A-\operatorname{Bln}\left(2 \theta_{0}\right)}\end{array}\right.<br>$$</p><p>即：<br>$$<br>\left{\begin{array}{l}{B=\frac{P D O}{l n 2}} \ {A=P_{0}+B \ln \left(\theta_{0}\right)}\end{array}\right.<br>$$</p></li></ul></li></ul><pre><code>（可以设置P0=600，PDO=20, θ0=1/30）</code></pre><ul><li><p>实际应用时</p><ul><li>LR模型中：</li></ul><p>$$<br>p=\frac{1}{1+e^{-\theta^{T} x}}<br>$$</p><p>$$<br>\begin{array}{c}{\ln \left(\frac{p}{1-p}\right)=\theta^{T} x} \ {\ln \left(\frac{p}{1-p}\right)=\ln (\text {odds})} \ {\ln (\text {odds})=\theta^{T} x=w_{0}+w_{1} x_{1}+\cdots+w_{n} x_{n}}\end{array}<br>$$</p><p>$$<br>\begin{aligned} \text {score}<em>{\mathbb{Z}} &amp;=A+B <em>\left(\theta^{T} x\right)=A+B </em>\left(w</em>{0}+w_{1} x_{1}+\cdots+w_{n} x_{n}\right) \ &amp;=\left(A+B <em> w_{0}\right)+B </em> w_{1} x_{1}+\cdots+B * w_{n} x_{n} \end{aligned}<br>$$</p></li></ul><pre><code>先通过lr模型对(woe, label)训练得到权重矩阵，并获得初始分A+B*w0；每个变量的分箱对应的分数为，权重\*每个分箱对应的woe- 新加入的用户，根据之前得到的分箱dict得到其特征的分箱区间对其特征进行分箱，在根据上一步得到的每个分箱的分数来获得每个特征的分数，并逐个相加，得到总分数</code></pre><ul><li><p>特征选择与分箱调参</p><ul><li><p><strong>并不是维度越多越好。</strong>一个评分卡中，一般不超过15个维度。</p><ol><li>可根据Logistic Regression模型系数来确定每个变量的权重，保留权重高的变量。通过协方差计算的相关性大于0.7的变量一般只保留IV值最高的那一个。</li><li>IV可以衡量特征的预测能力：</li></ol><p>$$<br>I V=\sum_{i=1}^{N}\left(g o o d_{占比} -bad_{占比} \right) * W O E_{i}<br>$$</p></li></ul><p>| IV        | 预测能力   |<br>| ——— | ———- |<br>| &lt;0.03     | 无预测能力 |<br>| 0.03~0.09 | 低         |<br>| 0.1~0.29  | 中         |<br>| 0.3~0.49  | 高         |<br>| &gt;=0.5     | 极高       |</p><p>预测能力低则需重新调整分箱。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;模型流程&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;分箱：等频、等距、卡方（掌握卡方分箱的原理和操作方式：卡方值最小的相邻区间合并（卡方越小说明俩区间区别越小，卡方衡量都是区间中label的分布））&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;根据分箱求得woe=ln(# g
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="评分卡" scheme="https://chenzk1.github.io/tags/%E8%AF%84%E5%88%86%E5%8D%A1/"/>
    
  </entry>
  
  <entry>
    <title>评分卡优化</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E8%AF%84%E5%88%86%E5%8D%A1%E4%BC%98%E5%8C%96/"/>
    <id>https://chenzk1.github.io/2019/11/19/评分卡优化/</id>
    <published>2019-11-19T02:23:10.346Z</published>
    <updated>2019-11-19T02:27:16.959Z</updated>
    
    <content type="html"><![CDATA[<ul><li>A-Bln(odds)：应有weight&gt;0</li></ul><h1 id="优化1"><a href="#优化1" class="headerlink" title="优化1"></a>优化1</h1><ul><li>加入更多的网约车数据</li><li>等宽</li><li>删特征 –&gt; weight&lt;0的特征减少了 –&gt;平滑 </li><li>异常值处理：做异常值截断</li><li>长尾分布取对数</li></ul><ul><li>只用ygc数据，且进行上采样，precision不超过5% –&gt; 无效果</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;A-Bln(odds)：应有weight&amp;gt;0&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&quot;优化1&quot;&gt;&lt;a href=&quot;#优化1&quot; class=&quot;headerlink&quot; title=&quot;优化1&quot;&gt;&lt;/a&gt;优化1&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;加入更多的网约车数据&lt;/li
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="评分卡" scheme="https://chenzk1.github.io/tags/%E8%AF%84%E5%88%86%E5%8D%A1/"/>
    
  </entry>
  
  <entry>
    <title>卡方分布及卡方检验</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83%E5%8F%8A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/"/>
    <id>https://chenzk1.github.io/2019/11/19/卡方分布及卡方检验/</id>
    <published>2019-11-19T02:23:10.331Z</published>
    <updated>2019-11-19T02:25:59.737Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/cuitzjd/article/details/80755310" target="_blank" rel="noopener">Doc1</a><br><a href="https://cosx.org/2010/11/hypotheses-testing/" target="_blank" rel="noopener">Doc2</a></p><h3 id="统计检验"><a href="#统计检验" class="headerlink" title="统计检验"></a>统计检验</h3><ul><li>出发点：小概率事件发生则假设不成立。可以检验一个样本与总体的差异，也可以检验一个样本集与另一个样本集的差异。</li><li>e.g. 观察到某个班级男女生身高的均值有差距，要证明这一点，先假设其”没有差距“，然后做相关的检验，如果检验出来有差距这个事件的概率小于某个值，则说明假设不成立。即一个具有足够小概率的事件发生了，这不是偶然。</li><li>第一类错误：原假设为真，检验的结果劝你放弃原假设。其概率为α。显著性水平。</li><li>第二类错误：原假设为假，检验的结果劝你接受原假设。概率为1-α。</li><li>显著性检验，即只限定第一类错误。当经过检验发现p&gt;α，则说明检验、总体这两样本之间不存在显著性差异，因此接受原假设；否则，说明一个很小概率的事件发生了，不接受原假设。</li><li>不同的检验方式有不同的前提。</li></ul><h3 id="T"><a href="#T" class="headerlink" title="T"></a>T</h3><ul><li><p><strong>用于样本量较小，总体标准差σ未知的正态分布。</strong></p></li><li><p>单总体检验：一个样本的平均数与已知的总体平均数的差异是否显著。</p></li><li><p>双总体样本检验：一个样本与另一个样本的差异。</p></li></ul><h3 id="F"><a href="#F" class="headerlink" title="F"></a>F</h3><ul><li>两个样本的总体方差是否相等。</li></ul><h3 id="卡方"><a href="#卡方" class="headerlink" title="卡方"></a>卡方</h3><h4 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h4><p><img src="https://ss0.bdstatic.com/94oJfD_bAAcT8t7mm9GUKT-xh_/timg?image&amp;quality=100&amp;size=b4000_4000&amp;sec=1512961717&amp;di=6133a04e10dc72b22bfb76fca025156d&amp;src=http://pic.baike.soso.com/p/20130619/20130619123748-1653885327.jpg" alt></p><ul><li><p>原假设为两者分布无差异、不相关等。</p></li><li><p>卡方值越大，实际（假设）与理论（根据原假设求得的概率值/统计量）差异越大，即假设的错误程度越大，超过阈值，则推翻假设。</p><p>若求得的卡方值落在阈值之上，说明小概率事件(e.g. 5%)发生了，则推翻原假设，说明两者差异的程度确实够大，即两者有差异；否则有95%的置信程度说明两者无差异。</p></li><li><p>参数有：自由度、置信水平。</p><p>例如，看喝牛奶与感冒有没有关系。给定置信度β，确定卡方值阈值x。</p><p>原假设：无关。</p><p>卡方值：先在原假设的基础上求出理论分布，然后看理论分布与实际分布的差异性（卡方值）。</p><p>查表，看有没有超过阈值，若超过，说明小概率（显著性水平：1-β）事件发生，推翻原假设，两者有关。</p></li></ul><h4 id="卡方分布"><a href="#卡方分布" class="headerlink" title="卡方分布"></a>卡方分布</h4><p>n个相互独立且均服从标准正态分布的随机变量，其平方和服从卡方分布。</p><p><img src="https://img-blog.csdn.net/20180109180130184?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc25vd2Ryb3B0dWxpcA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h4 id="卡方分箱"><a href="#卡方分箱" class="headerlink" title="卡方分箱"></a>卡方分箱</h4><ul><li><p>算法步骤：</p><ul><li><p>初始化：排序，每个实例属于一个一个区间</p></li><li><p>合并区间：</p><ul><li><p>计算每一对相邻区间的卡方值</p></li><li><p>将卡方值最小的一对区间合并</p><p>$$  \mathrm{X}^{2}=\sum_{i=1}^{2} \sum_{j=1}^{2} \frac{\left(A_{i j}-E_{i j}\right)^{2}}{E_{i j}}  $$</p><p>其中，Ai,j为第i区间第j类的实例的数量，Ei,j为期望频率（整体分布，整体分布计算的时候还是用的这俩区间的数值）=Ni*(Cj/N)，Cj为j类在整体分布中的比例，Ni为i区间样本数</p><p>例如label有0和1，有两个区间，即[[3,4], [5,7]]</p><p>A11=3,A12=4… E11=7*8/19</p></li></ul></li></ul></li><li><p>原理：假设这俩区间没差异，其卡方值代表了该假设（由实际分布得出的统计量）与理论（整体样本[特征]分布）的差异程度，越小则说明俩越接近，假设越成立。</p><p>评分卡模型中，第i区间第j类的实例：类是指区间中有多少类特征，实例指其对应的label</p></li><li><p>阈值：显著性水平+自由度（比类别数量少1）确定。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/cuitzjd/article/details/80755310&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Doc1&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://cosx.org/201
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>LR-logloss</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E4%B8%A4%E7%A7%8Dlogloss/"/>
    <id>https://chenzk1.github.io/2019/11/19/两种logloss/</id>
    <published>2019-11-19T02:23:10.331Z</published>
    <updated>2019-11-19T02:26:20.737Z</updated>
    
    <content type="html"><![CDATA[<ol><li>LR中，若0，1分类：<br>$$<br>\begin{array}{l}{\mathbb{P}(y=1 | z)=\sigma(z)=\frac{1}{1+e^{-z}}} \ {\mathbb{P}(y=0 | z)=1-\sigma(z)=\frac{1}{1+e^{z}}}\end{array}<br>$$<br>即：<br>$$<br>\mathbb{P}(y | z)=\sigma(z)^{y}(1-\sigma(z))^{1-y}<br>$$<br>极大似然：<br>$$<br>L(z)=\log \left(\prod_{i}^{m} \mathbb{P}\left(y_{i} | z_{i}\right)\right)=-\sum_{i}^{m} \log \left(\mathbb{P}\left(y_{i} | z_{i}\right)\right)=\sum_{i}^{m}-y_{i} z_{i}+\log \left(1+e^{z_{i}}\right)<br>$$<br>损失函数：<br>$$<br>l(z)=-\log \left(\prod_{i}^{m} \mathbb{P}\left(y_{i} | z_{i}\right)\right)=-\sum_{i}^{m} \log \left(\mathbb{P}\left(y_{i} | z_{i}\right)\right)=\sum_{i}^{m}-y_{i} z_{i}+\log \left(1+e^{z_{i}}\right)<br>$$</li></ol><ol start="2"><li><p>1，-1分类：<br>$$<br>\begin{aligned} \mathbb{P}(y | z) &amp;=\sigma(y z) \ &amp;=\frac{1}{1+e^{-y z}} \end{aligned}<br>$$<br>分开表示跟0，1分类是一样的</p><p>损失函数：<br>$$<br>L(z)=-\log \left(\prod_{j}^{m} \mathbb{P}\left(y_{j} | z_{j}\right)\right)=-\sum_{j}^{m} \log \left(\mathbb{P}\left(y_{j} | z_{j}\right)\right)=\sum_{j}^{m} \log \left(1+e^{-y_{j} z_{j}}\right)<br>$$</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;LR中，若0，1分类：&lt;br&gt;$$&lt;br&gt;\begin{array}{l}{\mathbb{P}(y=1 | z)=\sigma(z)=\frac{1}{1+e^{-z}}} \ {\mathbb{P}(y=0 | z)=1-\sigma(z)=\frac{1}
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>链表-反转链表</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E9%93%BE%E8%A1%A8-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/"/>
    <id>https://chenzk1.github.io/2019/11/19/链表-反转链表/</id>
    <published>2019-11-19T02:23:10.331Z</published>
    <updated>2019-11-18T02:24:06.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="反转整个链表"><a href="#反转整个链表" class="headerlink" title="反转整个链表"></a>反转整个链表</h1><h2 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h2><ul><li>递归往上走的时候，每进一轮迭代，当前节点与之前节点的链接未变化，且head.next节点总是在已反转的链表末端，因此可以用head.next.next = head。</li><li>递归需要badcase</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseList(self, head: ListNode) -&gt; ListNode:</span><br><span class="line">        if head.next==None:</span><br><span class="line">            return head</span><br><span class="line">        last = reverseList(head.next)</span><br><span class="line">        head.next.next = head</span><br><span class="line">        head.next = null</span><br><span class="line">        return last</span><br></pre></td></tr></table></figure><h2 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseList(self, head: ListNode) -&gt; ListNode:</span><br><span class="line">        curr = head</span><br><span class="line">        prev = None</span><br><span class="line">        while curr != None:</span><br><span class="line">            temp = curr.next</span><br><span class="line">            prev = curr</span><br><span class="line">            curr.next = prev</span><br><span class="line">            curr = temp</span><br><span class="line">        return prev</span><br></pre></td></tr></table></figure><h1 id="反转前n个链表"><a href="#反转前n个链表" class="headerlink" title="反转前n个链表"></a>反转前n个链表</h1><h2 id="递归-1"><a href="#递归-1" class="headerlink" title="递归"></a>递归</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseN(self, head: ListNode, n: int) -&gt; ListNode:</span><br><span class="line">        if n==1:</span><br><span class="line">            successor = head.next</span><br><span class="line">            return head</span><br><span class="line"></span><br><span class="line">        last = self.reverseN(head.next, n-1)</span><br><span class="line">        head.next.next = head</span><br><span class="line">        head.next = successor</span><br><span class="line">        </span><br><span class="line">        return last</span><br></pre></td></tr></table></figure><h1 id="反转一部分"><a href="#反转一部分" class="headerlink" title="反转一部分"></a>反转一部分</h1><h2 id="递归-使用前有函数"><a href="#递归-使用前有函数" class="headerlink" title="递归 使用前有函数"></a>递归 使用前有函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseBetween(self, head: ListNode, m: int, n: int) -&gt; ListNode:</span><br><span class="line">        if m==1:</span><br><span class="line">            return self.reverseN(head, n)</span><br><span class="line">        head.next = self.reverseBetween(head.next, m-1, n-1)</span><br><span class="line">        return head</span><br></pre></td></tr></table></figure><h2 id="迭代-1"><a href="#迭代-1" class="headerlink" title="迭代"></a>迭代</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseBetween(self, head: ListNode, m: int, n: int) -&gt; ListNode:</span><br><span class="line">        dummy = ListNode(-1)</span><br><span class="line">        dummy.next = head</span><br><span class="line">        prev = dummy</span><br><span class="line">        for _ in range(m-1):</span><br><span class="line">            prev = prev.next</span><br><span class="line">        curr = prev.next</span><br><span class="line">        node = None</span><br><span class="line">        for _ in range(n-m+1):</span><br><span class="line">            temp = curr.next</span><br><span class="line">            curr.next = node</span><br><span class="line">            node = curr</span><br><span class="line">            curr = temp</span><br><span class="line"></span><br><span class="line">        prev.next.next = curr</span><br><span class="line">        prev.next = node</span><br><span class="line">        return dummy.next</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseBetween(self, head: ListNode, m: int, n: int) -&gt; ListNode:</span><br><span class="line">        dummy = ListNode(-1)</span><br><span class="line">        dummy.next = head</span><br><span class="line">        prev = dummy</span><br><span class="line">        for _ in range(m-1):</span><br><span class="line">            prev = prev.next</span><br><span class="line">        start = prev.next</span><br><span class="line">        tail = start.next</span><br><span class="line">        for _ in range(n-m):</span><br><span class="line">            start.next = tail.next</span><br><span class="line">            tail.next = start</span><br><span class="line">            prev.next = tail</span><br><span class="line">            tail = start.next</span><br><span class="line">        return dummy.next</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;反转整个链表&quot;&gt;&lt;a href=&quot;#反转整个链表&quot; class=&quot;headerlink&quot; title=&quot;反转整个链表&quot;&gt;&lt;/a&gt;反转整个链表&lt;/h1&gt;&lt;h2 id=&quot;递归&quot;&gt;&lt;a href=&quot;#递归&quot; class=&quot;headerlink&quot; title=&quot;递归&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="DS" scheme="https://chenzk1.github.io/tags/DS/"/>
    
      <category term="链表" scheme="https://chenzk1.github.io/tags/%E9%93%BE%E8%A1%A8/"/>
    
  </entry>
  
  <entry>
    <title>CNN-手写字的识别</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E6%89%8B%E5%86%99%E5%AD%97%E7%9A%84%E8%AF%86%E5%88%AB)/"/>
    <id>https://chenzk1.github.io/2019/11/19/卷积神经网络(手写字的识别)/</id>
    <published>2019-11-19T02:23:10.318Z</published>
    <updated>2019-11-19T02:26:23.400Z</updated>
    
    <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据集：MNIST手写数字集</span><br><span class="line">训练集：<span class="number">42</span>,<span class="number">000</span>个<span class="number">0</span><span class="number">-9</span>手写数字的图像</span><br><span class="line">测试集：有<span class="number">28</span>,<span class="number">000</span>个无label样本</span><br><span class="line">每个图像的大小是<span class="number">28</span>×<span class="number">28</span>=<span class="number">784</span>个像素</span><br><span class="line">目标：使用卷积神经网络识别图像是什么数字</span><br></pre></td></tr></table></figure><h3 id="导入相关包"><a href="#导入相关包" class="headerlink" title="导入相关包"></a>导入相关包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python的内置垃圾收集。用来删除一些变量，并收集必要的空间来保存RAM。</span></span><br><span class="line"><span class="keyword">import</span> gc </span><br><span class="line"><span class="comment"># 用来生成随机数。</span></span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd </span><br><span class="line"><span class="comment">#用来检查运行时间。</span></span><br><span class="line"><span class="keyword">import</span> time </span><br><span class="line"><span class="comment"># 在数据增强部分，我们使用圆周率旋转图像。</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> pi </span><br><span class="line"><span class="comment"># 用Keras来构建我们的CNN模型。它使用TensorFlow作为后端。</span></span><br><span class="line"><span class="keyword">import</span> keras </span><br><span class="line"><span class="comment"># 绘制手写的数字图像。</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="comment"># 矩阵操作。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment"># 操作数据，比如加载和输出</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 用TensorFlow作为数据增强部分</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 用来建立学习速率衰减的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ReduceLROnPlateau, EarlyStopping</span><br><span class="line"><span class="comment"># 构建CNN所需要的一些基本构件。 </span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> (BatchNormalization, Conv2D, Dense, Dropout, Flatten,</span><br><span class="line">                          MaxPool2D, ReLU)</span><br><span class="line"><span class="comment"># 图像显示。</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment"># 将数据分解为训练和验证两部分。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p><strong>导入数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Loading..."</span>)</span><br><span class="line">path = <span class="string">"E:/机器学习/Tensorflow学习/data/"</span></span><br><span class="line">data_train = pd.read_csv(path + <span class="string">"train.csv"</span>,engine=<span class="string">"python"</span>)</span><br><span class="line">data_test = pd.read_csv(path + <span class="string">"test.csv"</span>,engine=<span class="string">"python"</span>)</span><br><span class="line">print(<span class="string">"Done!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Loading...Done!</code></pre><p><strong>查看数据集的大小</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Training data: &#123;&#125; rows, &#123;&#125; columns."</span>.format(data_train.shape[<span class="number">0</span>], data_train.shape[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">"Test data: &#123;&#125; rows, &#123;&#125; columns."</span>.format(data_test.shape[<span class="number">0</span>], data_test.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>Training data: 42000 rows, 785 columns.Test data: 28000 rows, 784 columns.</code></pre><p>训练集有42000行，785列，其中包括784个像素和一个标签，标注了这张图片是什么数字。</p><p>测试数据有28000行，没有标签。</p><p><strong>数据集拆分成x（图像数据）和y（标签）</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = data_train.values[:, <span class="number">1</span>:]</span><br><span class="line">y_train = data_train.values[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_2d</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d numpy array. m*n data image.</span></span><br><span class="line"><span class="string">       return a 3d image data. m * height * width * channel."""</span></span><br><span class="line">    <span class="keyword">if</span> len(x.shape) == <span class="number">1</span>:</span><br><span class="line">        m = <span class="number">1</span></span><br><span class="line">        height = width = int(np.sqrt(x.shape[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        m = x.shape[<span class="number">0</span>]</span><br><span class="line">        height = width = int(np.sqrt(x.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    x_2d = np.reshape(x, (m, height, width, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x_2d</span><br></pre></td></tr></table></figure><p><strong>查看图像</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_display = convert_2d(data_train.values[<span class="number">0</span>, <span class="number">1</span>:])</span><br><span class="line">plt.imshow(x_display.squeeze())</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x22b013e7780&gt;</code></pre><p><img src="output_14_1.png" alt="png"></p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在这里，我们直接研究数据增强。<br>当您没有足够的数据或想要扩展数据以提高性能时，数据增强是一种非常有用的技术。<br>在这场比赛中，数据增强基本上是指在不损害图像可识别性的前提下，对图像进行切割、旋转和缩放。<br>这里我使用了缩放、平移、白噪声和旋转。<br>随着数据的增加，您可以预期1-2%的准确性提高。</p><p><strong>放大</strong></p><p>使用crop_image函数来裁剪围绕中心的图像的一部分，调整其大小并将其保存为增强数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crop_image</span><span class="params">(x, y, min_scale)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d(m*n) numpy array. 1-dimension image data;</span></span><br><span class="line"><span class="string">       y: 1d numpy array. The ground truth label;</span></span><br><span class="line"><span class="string">       min_scale: float. The minimum scale for cropping.</span></span><br><span class="line"><span class="string">       return zoomed images.</span></span><br><span class="line"><span class="string">    # 该函数对图像进行裁剪，放大裁剪后的部分，并将其作为增强数据"""</span></span><br><span class="line">    <span class="comment"># 将数据转换为二维图像。图像应该是一个m*h*w*c数字数组。</span></span><br><span class="line">    images = convert_2d(x)</span><br><span class="line">    <span class="comment"># m是图像的个数。由于这是从0到255的灰度图像，所以它只有一个通道。</span></span><br><span class="line">    m, height, width, channel = images.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 原始图像的tf张量</span></span><br><span class="line">    img_tensor = tf.placeholder(tf.int32, [<span class="number">1</span>, height, width, channel])</span><br><span class="line">    <span class="comment"># tf tensor for 4 coordinates for corners of the cropped image</span></span><br><span class="line">    box_tensor = tf.placeholder(tf.float32, [<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">    box_idx = [<span class="number">0</span>]</span><br><span class="line">    crop_size = np.array([height, width])</span><br><span class="line">    <span class="comment"># 裁剪并调整图像张量</span></span><br><span class="line">    cropped_img_tensor = tf.image.crop_and_resize(img_tensor, box_tensor, box_idx, crop_size)</span><br><span class="line">    <span class="comment"># numpy array for the cropped image</span></span><br><span class="line">    cropped_img = np.zeros((m, height, width, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># randomly select a scale between [min_scale, min(min_scale + 0.05, 1)]</span></span><br><span class="line">            rand_scale = np.random.randint(min_scale * <span class="number">100</span>, np.minimum(min_scale * <span class="number">100</span> + <span class="number">5</span>, <span class="number">100</span>)) / <span class="number">100</span></span><br><span class="line">            <span class="comment"># calculate the 4 coordinates</span></span><br><span class="line">            x1 = y1 = <span class="number">0.5</span> - <span class="number">0.5</span> * rand_scale</span><br><span class="line">            x2 = y2 = <span class="number">0.5</span> + <span class="number">0.5</span> * rand_scale</span><br><span class="line">            <span class="comment"># lay down the cropping area</span></span><br><span class="line">            box = np.reshape(np.array([y1, x1, y2, x2]), (<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">            <span class="comment"># save the cropped image</span></span><br><span class="line">            cropped_img[i:i + <span class="number">1</span>, :, :, :] = sess.run(cropped_img_tensor, feed_dict=&#123;img_tensor: images[i:i + <span class="number">1</span>], box_tensor: box&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># flat the 2d image</span></span><br><span class="line">    cropped_img = np.reshape(cropped_img, (m, <span class="number">-1</span>))</span><br><span class="line">    cropped_img = np.concatenate((y.reshape((<span class="number">-1</span>, <span class="number">1</span>)), cropped_img), axis=<span class="number">1</span>).astype(int)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cropped_img</span><br></pre></td></tr></table></figure><p><strong>平移</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(x, y, dist)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d(m*n) numpy array. 1-dimension image data;</span></span><br><span class="line"><span class="string">       y: 1d numpy array. The ground truth label;</span></span><br><span class="line"><span class="string">       dist: float. Percentage of height/width to shift.</span></span><br><span class="line"><span class="string">       return translated images.</span></span><br><span class="line"><span class="string">       这个函数将图像移动到4个不同的方向。</span></span><br><span class="line"><span class="string">       裁剪图像的一部分，移动，用0填充左边的部分"""</span></span><br><span class="line">    <span class="comment"># 将一维图像数据转换为m*h*w*c数组</span></span><br><span class="line">    images = convert_2d(x)</span><br><span class="line">    m, height, width, channel = images.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set 4 groups of anchors. The first 4 int in a certain group lay down the area we crop.</span></span><br><span class="line">    <span class="comment"># The last 4 sets the area to be moved to. E.g.,</span></span><br><span class="line">    <span class="comment"># new_img[new_top:new_bottom, new_left:new_right] = img[top:bottom, left:right]</span></span><br><span class="line">    anchors = []</span><br><span class="line">    anchors.append((<span class="number">0</span>, height, int(dist * width), width, <span class="number">0</span>, height, <span class="number">0</span>, width - int(dist * width)))</span><br><span class="line">    anchors.append((<span class="number">0</span>, height, <span class="number">0</span>, width - int(dist * width), <span class="number">0</span>, height, int(dist * width), width))</span><br><span class="line">    anchors.append((int(dist * height), height, <span class="number">0</span>, width, <span class="number">0</span>, height - int(dist * height), <span class="number">0</span>, width))</span><br><span class="line">    anchors.append((<span class="number">0</span>, height - int(dist * height), <span class="number">0</span>, width, int(dist * height), height, <span class="number">0</span>, width))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># new_images: d*m*h*w*c array. The first dimension is the 4 directions.</span></span><br><span class="line">    new_images = np.zeros((<span class="number">4</span>, m, height, width, channel))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># shift the image</span></span><br><span class="line">        top, bottom, left, right, new_top, new_bottom, new_left, new_right = anchors[i]</span><br><span class="line">        new_images[i, :, new_top:new_bottom, new_left:new_right, :] = images[:, top:bottom, left:right, :]</span><br><span class="line">    </span><br><span class="line">    new_images = np.reshape(new_images, (<span class="number">4</span> * m, <span class="number">-1</span>))</span><br><span class="line">    y = np.tile(y, (<span class="number">4</span>, <span class="number">1</span>)).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">    new_images = np.concatenate((y, new_images), axis=<span class="number">1</span>).astype(int)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_images</span><br></pre></td></tr></table></figure><p><strong>添加白噪声</strong></p><p>现在我们给图像添加一些白噪声。我们随机选取一些像素，用均匀分布的噪声代替它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_noise</span><span class="params">(x, y, noise_lvl)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d(m*n) numpy array. 1-dimension image data;</span></span><br><span class="line"><span class="string">       y: 1d numpy array. The ground truth label;</span></span><br><span class="line"><span class="string">       noise_lvl: float. Percentage of pixels to add noise in.</span></span><br><span class="line"><span class="string">       return images with white noise.</span></span><br><span class="line"><span class="string">       This function randomly picks some pixels and replace them with noise."""</span></span><br><span class="line">    m, n = x.shape</span><br><span class="line">    <span class="comment"># calculate the # of pixels to add noise in</span></span><br><span class="line">    noise_num = int(noise_lvl * n)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># generate n random numbers, sort it and choose the first noise_num indices</span></span><br><span class="line">        <span class="comment"># which equals to generate random numbers w/o replacement</span></span><br><span class="line">        noise_idx = np.random.randint(<span class="number">0</span>, n, n).argsort()[:noise_num]</span><br><span class="line">        <span class="comment"># replace the chosen pixels with noise from 0 to 255</span></span><br><span class="line">        x[i, noise_idx] = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, noise_num)</span><br><span class="line"></span><br><span class="line">    noisy_data = np.concatenate((y.reshape((<span class="number">-1</span>, <span class="number">1</span>)), x), axis=<span class="number">1</span>).astype(<span class="string">"int"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> noisy_data</span><br></pre></td></tr></table></figure><p><strong>旋转</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate_image</span><span class="params">(x, y, max_angle)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d(m*n) numpy array. 1-dimension image data;</span></span><br><span class="line"><span class="string">       y: 1d numpy array. The ground truth label;</span></span><br><span class="line"><span class="string">       max_angle: int. The maximum degree for rotation.</span></span><br><span class="line"><span class="string">       return rotated images.</span></span><br><span class="line"><span class="string">       This function rotates the image for some random degrees(0.5 to 1 * max_angle degree)."""</span></span><br><span class="line">    images = convert_2d(x)</span><br><span class="line">    m, height, width, channel = images.shape</span><br><span class="line">    </span><br><span class="line">    img_tensor = tf.placeholder(tf.float32, [m, height, width, channel])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># half of the images are rotated clockwise. The other half counter-clockwise</span></span><br><span class="line">    <span class="comment"># positive angle: [max/2, max]</span></span><br><span class="line">    <span class="comment"># negative angle: [360-max/2, 360-max]</span></span><br><span class="line">    rand_angle_pos = np.random.randint(max_angle / <span class="number">2</span>, max_angle, int(m / <span class="number">2</span>))</span><br><span class="line">    rand_angle_neg = np.random.randint(-max_angle, -max_angle / <span class="number">2</span>, m - int(m / <span class="number">2</span>)) + <span class="number">360</span></span><br><span class="line">    rand_angle = np.transpose(np.hstack((rand_angle_pos, rand_angle_neg)))</span><br><span class="line">    np.random.shuffle(rand_angle)</span><br><span class="line">    <span class="comment"># convert the degree to radian</span></span><br><span class="line">    rand_angle = rand_angle / <span class="number">180</span> * pi</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rotate the images</span></span><br><span class="line">    rotated_img_tensor = tf.contrib.image.rotate(img_tensor, rand_angle)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        rotated_imgs = sess.run(rotated_img_tensor, feed_dict=&#123;img_tensor: images&#125;)</span><br><span class="line">    </span><br><span class="line">    rotated_imgs = np.reshape(rotated_imgs, (m, <span class="number">-1</span>))</span><br><span class="line">    rotated_imgs = np.concatenate((y.reshape((<span class="number">-1</span>, <span class="number">1</span>)), rotated_imgs), axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_imgs</span><br></pre></td></tr></table></figure><p><strong>合并</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">start = time.clock()</span><br><span class="line">print(<span class="string">"Augment the data..."</span>)</span><br><span class="line">cropped_imgs = crop_image(x_train, y_train, <span class="number">0.9</span>)</span><br><span class="line">translated_imgs = translate(x_train, y_train, <span class="number">0.1</span>)</span><br><span class="line">noisy_imgs = add_noise(x_train, y_train, <span class="number">0.1</span>)</span><br><span class="line">rotated_imgs = rotate_image(x_train, y_train, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">data_train = np.vstack((data_train, cropped_imgs, translated_imgs, noisy_imgs, rotated_imgs))</span><br><span class="line">np.random.shuffle(data_train)</span><br><span class="line">print(<span class="string">"Done!"</span>)</span><br><span class="line">time_used = int(time.clock() - start)</span><br><span class="line">print(<span class="string">"Time used: &#123;&#125;s."</span>.format(time_used))</span><br></pre></td></tr></table></figure><pre><code>G:\Anaconda\lib\site-packages\ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead  &quot;&quot;&quot;Entry point for launching an IPython kernel.Augment the data...WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.For more information, please see:  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md  * https://github.com/tensorflow/addonsIf you depend on functionality not listed there, please file an issue.Done!Time used: 26s.G:\Anaconda\lib\site-packages\ipykernel_launcher.py:11: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead  # This is added back by InteractiveShellApp.init_path()</code></pre><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p><strong>检查数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = data_train[:, <span class="number">1</span>:]</span><br><span class="line">y_train = data_train[:, <span class="number">0</span>]</span><br><span class="line">x_test = data_test.values</span><br><span class="line">print(<span class="string">"Augmented training data: &#123;&#125; rows, &#123;&#125; columns."</span>.format(data_train.shape[<span class="number">0</span>], data_train.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>Augmented training data: 336000 rows, 785 columns.</code></pre><p>使用数据增强之后的训练数据总共有33万6千行，是原来的8倍。</p><p><strong>向量转化为一个矩阵</strong></p><p>因为CNN接受的是输入是二维的图像，我们需要将向量转化为一个矩阵<br>格式：$m(图像数量)×h(图像高度)×w(图像宽度)×c(图像通道数量)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = convert_2d(x_train)</span><br><span class="line">x_test = convert_2d(x_test)</span><br></pre></td></tr></table></figure><p><strong>将类别型数据转换成哑变量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">y_train = keras.utils.to_categorical(y_train, num_classes)</span><br></pre></td></tr></table></figure><p>为了加快CNN优化速度，缩小像素值的范围。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = x_train / <span class="number">255</span></span><br><span class="line">x_test = x_test / <span class="number">255</span></span><br></pre></td></tr></table></figure><p><strong>划分训练集，验证集</strong></p><p>为了验证模型的好坏，用sklearn提供的一个函数来将数据按照9:1进行分割，90%为训练集，10%为验证集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate a random seed for train-test-split</span></span><br><span class="line">seed = np.random.randint(<span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=<span class="number">0.1</span>, random_state=seed)</span><br></pre></td></tr></table></figure><p><strong>清理内存</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> data_train</span><br><span class="line"><span class="keyword">del</span> data_test</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure><pre><code>69</code></pre><h3 id="搭建CNN模型"><a href="#搭建CNN模型" class="headerlink" title="搭建CNN模型"></a>搭建CNN模型</h3><p>一个普通的CNN通常包括三种类型的层，卷积层，池化层和全连接层。<br>我还在模型中添加了标准化层和dropout层。</p><ul><li><p>这里使用了5×5的卷积核，而不是3×3的。5×5的卷积核感受野更大，效果更好。</p></li><li><p>这里的批量归一化放在了ReLU激活函数之后，当然也可以放在激活函数之前。</p></li><li><p>Dropout使用了0.2的drop概率，意味着在Dropout层的输入中20%的像素点会被重置为0。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个卷积层的信道数。 </span></span><br><span class="line">filters = (<span class="number">32</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line"><span class="comment"># 每个conv层使用一个5x5内核</span></span><br><span class="line">kernel = (<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 在Dropout层的输入中20%的像素点会被重置为0。</span></span><br><span class="line">drop_prob = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters[<span class="number">0</span>], kernel, padding=<span class="string">"same"</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>),</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(Conv2D(filters[<span class="number">0</span>], kernel, padding=<span class="string">"same"</span>,</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters[<span class="number">1</span>], kernel, padding=<span class="string">"same"</span>,</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters[<span class="number">2</span>], kernel, padding=<span class="string">"same"</span>,</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters[<span class="number">3</span>], kernel, padding=<span class="string">"same"</span>,</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line"></span><br><span class="line"><span class="comment"># several fully-connected layers after the conv layers</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line">model.add(Dense(num_classes, activation=<span class="string">"softmax"</span>))</span><br><span class="line"><span class="comment"># use the Adam optimizer to accelerate convergence</span></span><br><span class="line">model.compile(keras.optimizers.Adam(), <span class="string">"categorical_crossentropy"</span>, metrics=[<span class="string">"accuracy"</span>])</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From G:\Anaconda\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.Instructions for updating:Colocations handled automatically by placer.WARNING:tensorflow:From G:\Anaconda\lib\site-packages\keras\backend\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.Instructions for updating:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.</code></pre><p><strong>查看模型架构</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       _________________________________________________________________batch_normalization_1 (Batch (None, 28, 28, 32)        128       _________________________________________________________________re_lu_1 (ReLU)               (None, 28, 28, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 28, 28, 32)        25632     _________________________________________________________________batch_normalization_2 (Batch (None, 28, 28, 32)        128       _________________________________________________________________re_lu_2 (ReLU)               (None, 28, 28, 32)        0         _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         _________________________________________________________________dropout_1 (Dropout)          (None, 14, 14, 32)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 14, 14, 32)        25632     _________________________________________________________________batch_normalization_3 (Batch (None, 14, 14, 32)        128       _________________________________________________________________re_lu_3 (ReLU)               (None, 14, 14, 32)        0         _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         _________________________________________________________________dropout_2 (Dropout)          (None, 7, 7, 32)          0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 7, 7, 64)          51264     _________________________________________________________________batch_normalization_4 (Batch (None, 7, 7, 64)          256       _________________________________________________________________re_lu_4 (ReLU)               (None, 7, 7, 64)          0         _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         _________________________________________________________________dropout_3 (Dropout)          (None, 3, 3, 64)          0         _________________________________________________________________conv2d_5 (Conv2D)            (None, 3, 3, 64)          102464    _________________________________________________________________batch_normalization_5 (Batch (None, 3, 3, 64)          256       _________________________________________________________________re_lu_5 (ReLU)               (None, 3, 3, 64)          0         _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 1, 1, 64)          0         _________________________________________________________________dropout_4 (Dropout)          (None, 1, 1, 64)          0         _________________________________________________________________flatten_1 (Flatten)          (None, 64)                0         _________________________________________________________________dropout_5 (Dropout)          (None, 64)                0         _________________________________________________________________dense_1 (Dense)              (None, 128)               8320      _________________________________________________________________dropout_6 (Dropout)          (None, 128)               0         _________________________________________________________________dense_2 (Dense)              (None, 10)                1290      =================================================================Total params: 216,330Trainable params: 215,882Non-trainable params: 448_________________________________________________________________</code></pre><p>The list above is the structure of my CNN model. It goes:</p><ul><li>(Conv-ReLU-BatchNormalization-MaxPooling-Dropout) x 4;</li><li><p>3 fully-connected(dense) layers with 1 dropout layer. Dense(64)-Dense(128)-Dropout-Dense(with softmax activation).</p></li><li><p>In CNN people often use 3x3 or 5x5 kernel. I found that with a 5x5 kernel, the model’s accuracy improved about 0.125%, which is quite a lot when you pass 99% threshold.</p></li><li>Convolutional layers and max pooling layers can extract some high-level traits from the pixels. With the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="noopener">ReLU</a>) unit the and max pooling, we also add non-linearity into the network;</li><li>Batch normalization helps the network converge faster since it keeps the input of every layer at the same scale;</li><li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout" target="_blank" rel="noopener">Dropout</a> layers help us prevent overfitting by randomly drop some of the input units. With dropout our model won’t overfit to some specific extreme data or some noisy pixels;</li><li>The <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam" target="_blank" rel="noopener">Adam optimizer</a> also accelerates the optimization. Usually when the dataset is too large, we use mini-batch gradient descent or stochastic gradient descent to save some training time. The randomness in MBGD or SGD means that the steps towards the optimum are zig-zag rather than straight forward. Adam, or Adaptive Moment Estimation, uses exponential moving average on the gradients and the secend moment of gradients to make the steps straight and in turn accelerate the optimization.</li></ul><h3 id="训练CNN"><a href="#训练CNN" class="headerlink" title="训练CNN"></a>训练CNN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># number of epochs we run</span></span><br><span class="line">iters = <span class="number">100</span></span><br><span class="line"><span class="comment"># batch size. Number of images we train before we take one step in MBGD.</span></span><br><span class="line">batch_size = <span class="number">1024</span></span><br></pre></td></tr></table></figure><p>当我们接近最佳状态时，我们需要降低学习速度以防止过度学习。高学习率会使我们远离最佳状态。因此，当验证数据的准确性不再提高时，我将这个学习率衰减设置为降低它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># monitor: :要监视的数量。当它不再显著改善时，我们就降低了学习速度</span></span><br><span class="line"><span class="comment"># factor: 新学习率=旧学习率 * factor</span></span><br><span class="line"><span class="comment"># patience:在降低学习速度之前，我们要等待的时间</span></span><br><span class="line"><span class="comment"># verbose: 是否显示信息</span></span><br><span class="line"><span class="comment"># min_lr: 最小的学习率</span></span><br><span class="line"></span><br><span class="line">lr_decay = ReduceLROnPlateau(monitor=<span class="string">"val_acc"</span>, factor=<span class="number">0.5</span>, patience=<span class="number">3</span>, verbose=<span class="number">1</span>, min_lr=<span class="number">1e-5</span>)</span><br><span class="line"><span class="comment"># 如果模型在验证数据上没有得到任何改善，可以设置早期停止，以防止过度拟合，并节省一些时间。当监控量没有提高时，提前停止训练。</span></span><br><span class="line">early_stopping = EarlyStopping(monitor=<span class="string">"val_acc"</span>, patience=<span class="number">7</span>, verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>训练模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Training model..."</span>)</span><br><span class="line">fit_params = &#123;</span><br><span class="line">    <span class="string">"batch_size"</span>: batch_size,</span><br><span class="line">    <span class="string">"epochs"</span>: iters,</span><br><span class="line">    <span class="string">"verbose"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"callbacks"</span>: [lr_decay, early_stopping],</span><br><span class="line">    <span class="string">"validation_data"</span>: (x_dev, y_dev)     <span class="comment"># data for monitoring the model accuracy</span></span><br><span class="line">&#125;</span><br><span class="line">model.fit(x_train, y_train, **fit_params)</span><br><span class="line">print(<span class="string">"Done!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Training model...WARNING:tensorflow:From G:\Anaconda\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.Instructions for updating:Use tf.cast instead.Train on 302400 samples, validate on 33600 samplesEpoch 1/100  3072/302400 [..............................] - ETA: 32:43 - loss: 2.6548 - acc: 0.1156</code></pre><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(x_dev, y_dev)</span><br></pre></td></tr></table></figure><pre><code>33600/33600 [==============================] - 3s 75us/step[0.0018058670724439621, 0.9994047619047619]</code></pre><p>evaluate这个方法会输出两个值，第一个是当期的损失函数值，第二个是模型的准确率。我们可以看到，模型的准确率在验证集上达到了99.84%！</p><h3 id="输出预测"><a href="#输出预测" class="headerlink" title="输出预测"></a>输出预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(x_test, batch_size=batch_size)</span><br><span class="line">y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">idx = np.reshape(np.arange(<span class="number">1</span>, len(y_pred) + <span class="number">1</span>), (len(y_pred), <span class="number">-1</span>))</span><br><span class="line">y_pred = np.hstack((idx, y_pred))</span><br><span class="line">y_pred = pd.DataFrame(y_pred, columns=[<span class="string">'ImageId'</span>, <span class="string">'Label'</span>])</span><br><span class="line">y_pred.to_csv(<span class="string">'y_pred.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>基向量与坐标变换</title>
    <link href="https://chenzk1.github.io/2019/11/19/%E5%9F%BA%E5%90%91%E9%87%8F%E4%B8%8E%E5%9D%90%E6%A0%87%E5%8F%98%E6%8D%A2/"/>
    <id>https://chenzk1.github.io/2019/11/19/基向量与坐标变换/</id>
    <published>2019-11-19T02:23:10.311Z</published>
    <updated>2019-11-19T02:25:18.164Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>基与维数</p><ul><li>F：一个数集</li><li>V：F上的非空子集，相当于一个向量空间。{α1，α2…αm}是V中的一个有序向量组。</li></ul><p>基：{α1，α2…αm}线性无关且V中的向量都可以用{α1，α2…αm}，{α1，α2…αm}为V的一组基。V = L(α1，α2…αm)</p><p>维数：基中向量的个数。</p></li><li><p>坐标：向量关于基的坐标<br>$$<br>\alpha=x_{1} \alpha 1+x_{2} \alpha 2+\cdots, x_{m} \alpha m=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}\right)\left(\begin{array}{c}{x_{1}} \ {x_{2}} \ {\vdots} \ {x_{m}}\end{array}\right)<br>$$<br>(X1…xm)是向量α在基下的坐标</p></li><li><p>基变换与坐标变换</p><ul><li><p>基变换</p><p>(β1… βm) = A(α1… αm) </p><p>A为基α到基β的过渡矩阵</p></li><li><p>坐标变换</p><p>同一向量关于一个基的坐标x到关于另一个基的坐标y的变换</p></li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;p&gt;基与维数&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;F：一个数集&lt;/li&gt;
&lt;li&gt;V：F上的非空子集，相当于一个向量空间。{α1，α2…αm}是V中的一个有序向量组。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;基：{α1，α2…αm}线性无关且V中的向量都可以用{α1，α2…αm}，
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="线性代数" scheme="https://chenzk1.github.io/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>隐马尔科夫</title>
    <link href="https://chenzk1.github.io/2019/11/19/HMM%EF%BC%88%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%EF%BC%89/"/>
    <id>https://chenzk1.github.io/2019/11/19/HMM（隐马尔可夫）/</id>
    <published>2019-11-19T02:23:10.281Z</published>
    <updated>2019-11-19T02:31:00.456Z</updated>
    
    <content type="html"><![CDATA[<h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><ul><li><p>出发点：保留所有不确定性，将风险降到最小。</p></li><li><p>条件熵：$ H(Y|X) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(y_i|x_i) = \sum\limits_{j=1}^{n}p(x_j)H(Y|x_j) $</p></li><li><p>最大熵模型：假设分类模型为一个条件概率分布P(Y|X)，X为特征，Y为输出，给定(X,Y)，用最大熵模型选择一个最好的分类模型。</p><p><a href="https://www.cnblogs.com/pinard/p/6093948.html" target="_blank" rel="noopener">Doc</a></p></li></ul><h2 id="隐马尔可夫"><a href="#隐马尔可夫" class="headerlink" title="隐马尔可夫"></a>隐马尔可夫</h2><ul><li>要解决的问题：1）基于序列；2）问题中有两类数据，一类是可以观测到的（观测序列），另一类是不能观测的（状态序列）。</li></ul><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul><li><p>假设Q={q1,q2…qN}是<strong>所有可能</strong>的隐藏状态的集合，V={v1,v2…vM}是<strong>所有可能</strong>的观测状态的集合。</p></li><li><p>对于一个长度为T的序列，I={i1,i2…iT}为状态序列，O={o1,o2…oT}为观察序列。其中i∈q, o∈v</p></li><li><p>假设：</p><ul><li><p>齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态。如果在时刻tt的隐藏状态是it=qi,在时刻t+1t+1的隐藏状态是it+1=qj, 则从时刻t到时刻t+1的HMM状态转移概率aij可以表示为：</p><p>$ a_{ij} = P(i_{t+1} = q_j | i_t= q_i) $</p><p>aij的状态转移矩阵A=「aij」NxN</p></li><li><p>观察独立性假设。任意时刻的观察状态只依赖于当前时刻的隐藏状态。如果在时刻t的隐藏状态是it=qj, 而对应的观察状态为ot=vk, 则该时刻观察状态vk在隐藏状态qj下生成的概率为bj(k),满足：</p><p>$ b_j(k) = P(o_t = v_k | i_t= q_j) $</p><p>这样bj(k)可以组成观测状态生成的概率矩阵B:</p><p>$ B = \Big [b_j(k) \Big ]_{N \times M} $</p></li><li><p>此外，需要t=1时的隐藏状态概率分布II：</p><p>$ B = \Big [b_j(k) \Big ]_{N \times M} $</p></li></ul></li><li><p>一个HMM模型，可以由隐藏状态初始概率分布II，A、B状态转移矩阵和状态概率矩阵决定：</p><p>λ=(A,B,II)</p></li></ul><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol><li>评估观察序列概率。即给定模型λ=(A,B,II)和观测序列O={o1,o2,…oT}，计算在模型λ下观测序列O出现的概率P(O|λ)。这个问题的求解需要用到前向后向算法。</li><li>模型参数学习问题。即给定观测序列O={o1,o2,…oT}，估计模型λ=(A,B,II)的参数，使该模型下观测序列的条件概率P(O|λ)最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法， 我们在这个系列的第三篇会详细讲解。这个问题是HMM模型三个问题中最复杂的。</li><li>预测问题，也称为解码问题。即给定模型λ=(A,B,II)和观测序列O={o1,o2,…oT}，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，我们在这个系列的第四篇会详细讲解。这个问题是HMM模型三个问题中复杂度居中的算法。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;最大熵模型&quot;&gt;&lt;a href=&quot;#最大熵模型&quot; class=&quot;headerlink&quot; title=&quot;最大熵模型&quot;&gt;&lt;/a&gt;最大熵模型&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;出发点：保留所有不确定性，将风险降到最小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;条件熵：$ H(Y|
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>KS-GINI</title>
    <link href="https://chenzk1.github.io/2019/11/19/K-S%E3%80%81GINI/"/>
    <id>https://chenzk1.github.io/2019/11/19/K-S、GINI/</id>
    <published>2019-11-19T02:23:10.281Z</published>
    <updated>2019-11-19T02:31:37.790Z</updated>
    
    <content type="html"><![CDATA[<p>##<strong>都是用来衡量模型区分度的</strong></p><h4 id="K-S"><a href="#K-S" class="headerlink" title="K-S"></a>K-S</h4><ul><li><p>KS(Kolmogorov-Smirnov)：KS用于模型风险区分能力进行评估，<br>指标衡量的是好坏样本累计分部之间的差值。<br>好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。</p></li><li><p>KS的计算步骤如下： </p><ol><li><p>计算每个评分区间的好坏账户数。 </p></li><li><p>计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 </p></li><li><p>计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。</p></li></ol></li></ul><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ks</span><span class="params">(df, y_true, y_pre, num=<span class="number">10</span>, good=<span class="number">0</span>, bad=<span class="number">1</span>)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  df为包含真实label和预测的概率值的DataFrame</span></span><br><span class="line"><span class="string">  y_true</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">    <span class="comment"># 1.将数据从小到大平均分成num组</span></span><br><span class="line">    df_ks = df.sort_values(y_pre).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_ks[<span class="string">'rank'</span>] = np.floor((df_ks.index / len(df_ks) * num) + <span class="number">1</span>)</span><br><span class="line">    df_ks[<span class="string">'set_1'</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 2.统计结果</span></span><br><span class="line">    result_ks = pd.DataFrame()</span><br><span class="line">    result_ks[<span class="string">'group_sum'</span>] = df_ks.groupby(<span class="string">'rank'</span>)[<span class="string">'set_1'</span>].sum()</span><br><span class="line">    result_ks[<span class="string">'group_min'</span>] = df_ks.groupby(<span class="string">'rank'</span>)[y_pre].min()</span><br><span class="line">    result_ks[<span class="string">'group_max'</span>] = df_ks.groupby(<span class="string">'rank'</span>)[y_pre].max()</span><br><span class="line">    result_ks[<span class="string">'group_mean'</span>] = df_ks.groupby(<span class="string">'rank'</span>)[y_pre].mean()</span><br><span class="line">    <span class="comment"># 3.最后一行添加total汇总数据</span></span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'group_sum'</span>] = df_ks[<span class="string">'set_1'</span>].sum()</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'group_min'</span>] = df_ks[y_pre].min()</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'group_max'</span>] = df_ks[y_pre].max()</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'group_mean'</span>] = df_ks[y_pre].mean()</span><br><span class="line">    <span class="comment"># 4.好用户统计</span></span><br><span class="line">    result_ks[<span class="string">'good_sum'</span>] = df_ks[df_ks[y_true] == good].groupby(<span class="string">'rank'</span>)[<span class="string">'set_1'</span>].sum()</span><br><span class="line">    result_ks.good_sum.replace(np.nan, <span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'good_sum'</span>] = result_ks[<span class="string">'good_sum'</span>].sum()</span><br><span class="line">    result_ks[<span class="string">'good_percent'</span>] = result_ks[<span class="string">'good_sum'</span>] / result_ks.loc[<span class="string">'total'</span>, <span class="string">'good_sum'</span>]</span><br><span class="line">    result_ks[<span class="string">'good_percent_cum'</span>] = result_ks[<span class="string">'good_sum'</span>].cumsum() / result_ks.loc[<span class="string">'total'</span>, <span class="string">'good_sum'</span>]</span><br><span class="line">    <span class="comment"># 5.坏用户统计</span></span><br><span class="line">    result_ks[<span class="string">'bad_sum'</span>] = df_ks[df_ks[y_true] == bad].groupby(<span class="string">'rank'</span>)[<span class="string">'set_1'</span>].sum()</span><br><span class="line">    result_ks.bad_sum.replace(np.nan, <span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'bad_sum'</span>] = result_ks[<span class="string">'bad_sum'</span>].sum()</span><br><span class="line">    result_ks[<span class="string">'bad_percent'</span>] = result_ks[<span class="string">'bad_sum'</span>] / result_ks.loc[<span class="string">'total'</span>, <span class="string">'bad_sum'</span>]</span><br><span class="line">    result_ks[<span class="string">'bad_percent_cum'</span>] = result_ks[<span class="string">'bad_sum'</span>].cumsum() / result_ks.loc[<span class="string">'total'</span>, <span class="string">'bad_sum'</span>]</span><br><span class="line">    <span class="comment"># 6.计算ks值</span></span><br><span class="line">    result_ks[<span class="string">'diff'</span>] = result_ks[<span class="string">'bad_percent_cum'</span>] - result_ks[<span class="string">'good_percent_cum'</span>]</span><br><span class="line">    <span class="comment"># 7.更新最后一行total的数据</span></span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'bad_percent_cum'</span>] = np.nan</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'good_percent_cum'</span>] = np.nan</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'diff'</span>] = result_ks[<span class="string">'diff'</span>].max()</span><br><span class="line">result_ks = result_ks.reset_index()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> result_ks</span><br></pre></td></tr></table></figure></code></pre><h4 id="GINI"><a href="#GINI" class="headerlink" title="GINI"></a>GINI</h4><ul><li>GINI统计值衡量坏账户数在好账户数上的的累积分布与随机分布曲线之间的面积，好账户与坏账户分布之间的差异越大，GINI指标越高，表明模型的风险区分能力越强。</li><li>GINI系数的计算步骤如下： <ol><li>计算每个评分区间的好坏账户数。 </li><li>计算每个评分区间的累计好账户数占总好账户数比率（累计good%）和累计坏账户数占总坏账户数比率(累计bad%)。 </li><li>按照累计好账户占比和累计坏账户占比得出下图所示曲线ADC。 </li><li>计算出图中阴影部分面积，阴影面积占直角三角形ABC面积的百分比，即为GINI系数。</li></ol></li></ul><p><img src="https://img-blog.csdn.net/20171012171836445?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzQyMTYyOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##&lt;strong&gt;都是用来衡量模型区分度的&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&quot;K-S&quot;&gt;&lt;a href=&quot;#K-S&quot; class=&quot;headerlink&quot; title=&quot;K-S&quot;&gt;&lt;/a&gt;K-S&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;KS(Kolmogorov-Smir
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>DS绪论</title>
    <link href="https://chenzk1.github.io/2019/11/19/DS-%E7%BB%AA%E8%AE%BA/"/>
    <id>https://chenzk1.github.io/2019/11/19/DS-绪论/</id>
    <published>2019-11-19T02:23:10.265Z</published>
    <updated>2019-11-19T02:30:34.072Z</updated>
    
    <content type="html"><![CDATA[<ul><li><p>时间复杂度：<br>bigO bigΩ bigθ</p><p>分别为上、下、中</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;p&gt;时间复杂度：&lt;br&gt;bigO bigΩ bigθ&lt;/p&gt;
&lt;p&gt;分别为上、下、中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;

      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="DS" scheme="https://chenzk1.github.io/tags/DS/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://chenzk1.github.io/2019/11/19/7%E6%9C%8824%E6%97%A5/"/>
    <id>https://chenzk1.github.io/2019/11/19/7月24日/</id>
    <published>2019-11-19T02:23:10.250Z</published>
    <updated>2019-07-25T06:39:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="与滴滴金融的会议"><a href="#与滴滴金融的会议" class="headerlink" title="与滴滴金融的会议"></a>与滴滴金融的会议</h2><h3 id="乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉及到POI特征数据？讨论POI数据缺失的可能替换方案。"><a href="#乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉及到POI特征数据？讨论POI数据缺失的可能替换方案。" class="headerlink" title="乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉及到POI特征数据？讨论POI数据缺失的可能替换方案。"></a>乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉及到POI特征数据？讨论POI数据缺失的可能替换方案。</h3><p>当前有几类方案。</p><h3 id="金融视角，产品形态是车辆抵押贷款，需要聚焦在平台内有车乘客和橘子司机，我们需要重点围绕稳定性、收入、反欺诈、行为类四大类进行分析，具体情况开会介绍。"><a href="#金融视角，产品形态是车辆抵押贷款，需要聚焦在平台内有车乘客和橘子司机，我们需要重点围绕稳定性、收入、反欺诈、行为类四大类进行分析，具体情况开会介绍。" class="headerlink" title="金融视角，产品形态是车辆抵押贷款，需要聚焦在平台内有车乘客和橘子司机，我们需要重点围绕稳定性、收入、反欺诈、行为类四大类进行分析，具体情况开会介绍。"></a>金融视角，产品形态是车辆抵押贷款，需要聚焦在平台内有车乘客和橘子司机，我们需要重点围绕稳定性、收入、反欺诈、行为类四大类进行分析，具体情况开会介绍。</h3><ul><li><p>通过出行平台数据判断乘客稳定性时，乘客行为分需要加入哪些特征。</p><p>稳定性：</p></li></ul><p><img src="https://pic1.zhimg.com/80/v2-fea6a1111cb1274ca3368fad8d2e7a88_hd.jpg" alt="img"></p><ul><li>收入（消费水平）：平台价值维度。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-88541a4212c7678e589df5badace3a07_hd.jpg" alt="img"></p><ul><li><p>反欺诈&amp;行为：</p><p>| 维度     | 子维度             | 因子                                         | 行为好坏                            | 备注 |<br>| ——– | —————— | ——————————————– | ———————————– | —- |<br>| 履约能力 | 历史借贷及履约表现 | 1.最近7天逾期支付率                          | 好：连续n单按时支付（3，5，10，20） |      |<br>|          |                    | 2.最近30天逾期支付率                         | 坏：7天内支付率                     |      |<br>|          |                    | 3.30天以上逾期支付率                         |                                     |      |<br>|          | 行为规范           | 1.平台作弊行为及表现（刷单，历史封禁）；     |                                     |      |<br>|          | 身份属性           | 1.实名认证？2.是否有司机账号                 |                                     |      |<br>|          | 发单习惯           | 平台发单行为（活跃度，发单习惯，收入稳定性） |                                     |      |<br>|          |                    |                                              |                                     |      |<br>|          |                    |                                              |                                     |      |<br>|          |                    |                                              |                                     |      |</p></li></ul><h3 id="乘客行为分在滴滴金融车抵贷产品形态下，数据风控层面的应用，从特征提取，数据建模，到输出决策。"><a href="#乘客行为分在滴滴金融车抵贷产品形态下，数据风控层面的应用，从特征提取，数据建模，到输出决策。" class="headerlink" title="乘客行为分在滴滴金融车抵贷产品形态下，数据风控层面的应用，从特征提取，数据建模，到输出决策。"></a>乘客行为分在滴滴金融车抵贷产品形态下，数据风控层面的应用，从特征提取，数据建模，到输出决策。</h3><h3 id="会议记录"><a href="#会议记录" class="headerlink" title="会议记录"></a>会议记录</h3><h4 id="进度"><a href="#进度" class="headerlink" title="进度"></a>进度</h4><h5 id="金融"><a href="#金融" class="headerlink" title="金融"></a>金融</h5><pre><code>- 以大数据风控作为下半年发展方向。- 识别乘客的信用风险、需求，数据不是金融的，判断一个人是否有还贷意向。一部分是乘客，一部分是司机，并不是活跃用户，那如何判断他们还贷的概率需要数据。</code></pre><ul><li>信用分的场景与典押有点类似，探讨可以协同的地方。</li></ul><h4 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h4><ul><li><p>不活跃的用户如何区分：80-20定律，大部分用户数据量很少；解决方法：提高数据丰度</p></li><li><p>如何区分是否虚假/恶意投诉：投诉率、进线率太高，人工进行审核</p></li></ul><h4 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h4><ul><li>平台有车乘客规模，乘客是否有房、是否有车的标签</li></ul><h4 id="介绍四方面的特征"><a href="#介绍四方面的特征" class="headerlink" title="介绍四方面的特征"></a>介绍四方面的特征</h4><ol><li><p>稳定性</p><p>异地高频生活、POI特征、统计类数据</p></li><li><p>反欺诈</p><p>是否高危职业客户</p></li><li><p>收入</p><p>车辆持有满3个月，固定收入预估区间，名下车产信息是否一致…</p></li><li><p>行为</p><p>近期生活轨迹重大变故、喜好预测…</p></li></ol><h4 id="介绍行为分"><a href="#介绍行为分" class="headerlink" title="介绍行为分"></a>介绍行为分</h4><p>初始化、权重计算、影响因子、增量更新</p><p>行为健康度介绍</p><p>用于免押的应用场景</p><h4 id="芝麻分"><a href="#芝麻分" class="headerlink" title="芝麻分"></a>芝麻分</h4><h2 id="待办"><a href="#待办" class="headerlink" title="待办"></a>待办</h2><p>排序之前的问题</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;与滴滴金融的会议&quot;&gt;&lt;a href=&quot;#与滴滴金融的会议&quot; class=&quot;headerlink&quot; title=&quot;与滴滴金融的会议&quot;&gt;&lt;/a&gt;与滴滴金融的会议&lt;/h2&gt;&lt;h3 id=&quot;乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://chenzk1.github.io/2019/11/19/7%E6%9C%8823%E6%97%A5/"/>
    <id>https://chenzk1.github.io/2019/11/19/7月23日/</id>
    <published>2019-11-19T02:23:10.234Z</published>
    <updated>2019-07-23T07:48:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>##吴文栋邮件</p><h3 id="先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。"><a href="#先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。" class="headerlink" title="先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。"></a>先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。</h3><ul><li>提供了平台，就得对平台进行管控，矛盾不可避免，而出现司乘之间无法自行调和的矛盾，则只能靠平台进行决断，因此这个权威性必须树立。要做的应该是尽量减少矛盾的出现，但矛盾一旦出现，要有足够的权威给予适当的调节手段。</li><li>减少矛盾：淘宝也是第三方平台，参考淘宝，是否可以有乘客自主选单的功能，设置距离优先、服务优先、车辆类型优先等，然后再按此派单，并在出现距离较远等情况时提醒用户是否确定发单，来减少取消订单的情况。</li><li>管控：司乘之间的协商，检测到绕路、高度收费时提醒司乘，并在客户端提示乘客做选择。</li><li>坏账能不能接入政府或其他的信用评分，以此增大乘客毁约的成本。</li></ul><h3 id="民不究官不治，与上同理，第三方平台，只有司乘出现自身无法调和的矛盾时才发挥作用。"><a href="#民不究官不治，与上同理，第三方平台，只有司乘出现自身无法调和的矛盾时才发挥作用。" class="headerlink" title="民不究官不治，与上同理，第三方平台，只有司乘出现自身无法调和的矛盾时才发挥作用。"></a>民不究官不治，与上同理，第三方平台，只有司乘出现自身无法调和的矛盾时才发挥作用。</h3><h2 id="网约车"><a href="#网约车" class="headerlink" title="网约车"></a>网约车</h2><h3 id="乘客信用分需要透传的底层数据必须要实锤，可用卷宗数据-也存在非实锤的问题-所有使用客服的数据的业务都存在这个问题，相信这个问题终会解决-卷宗？"><a href="#乘客信用分需要透传的底层数据必须要实锤，可用卷宗数据-也存在非实锤的问题-所有使用客服的数据的业务都存在这个问题，相信这个问题终会解决-卷宗？" class="headerlink" title="乘客信用分需要透传的底层数据必须要实锤，可用卷宗数据(也存在非实锤的问题, 所有使用客服的数据的业务都存在这个问题，相信这个问题终会解决);卷宗？"></a>乘客信用分需要透传的底层数据必须要实锤，可用卷宗数据(也存在非实锤的问题, 所有使用客服的数据的业务都存在这个问题，相信这个问题终会解决);卷宗？</h3><ul><li>乘客信用很重要，是我们的抓手之一，但是好像跟我们的业务关系不太大，希望定制化分数; 信用是历史的表现行为的累计，用历史的行为来判责判罚当次发生的问题，持谨慎态度”</li><li>那如何解决呢？对判责仅提供参考，主要还是用来例如降低押金或免押金，或恶意乘客识别（说明历史上有多次恶意或不好的行为）</li><li>理想中是有很多作用，具体实施要考虑到业务（足够的依据）、用户的…等</li><li>目前定位是分层管控，但是管控却没有足够的依据。判责的标准不好改变，是否可以考虑多次不好的行为后实施惩罚或者账号的某些动作限制？例如信用分太低限制在发单几分钟之后取消订单？ </li></ul><h3 id="一期信用分的目的是对乘客去尾，尾部乘客可以分成两部分，其中一部分是高频小问题的乘客。"><a href="#一期信用分的目的是对乘客去尾，尾部乘客可以分成两部分，其中一部分是高频小问题的乘客。" class="headerlink" title="一期信用分的目的是对乘客去尾，尾部乘客可以分成两部分，其中一部分是高频小问题的乘客。"></a>一期信用分的目的是对乘客去尾，尾部乘客可以分成两部分，其中一部分是高频小问题的乘客。</h3><ul><li>因为客服不会对服务类投诉判责(比如抽烟、醉酒、超载等)所以无实锤证据，由于我们的实锤机制，我们无法对这类行为做扣分处理，这就会导致高频小问题的乘客不会被我们管控。不做管控，但间接提醒，例如既然同类问题高频出现，那认为此人确实有此类问题的置信度比较高，因此可以考虑发送短信提醒（如果在提醒中加上，如若再犯会影响平台对你之后行为的判责以及打车发单的优先级，这样之后再应用行为分做管控的时候合理吗？或者不做判责）</li></ul><h3 id="方案细节"><a href="#方案细节" class="headerlink" title="方案细节"></a>方案细节</h3><h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><ul><li>分5个等级评价，但是大部分人是默认好评，直接混入好评差评率作为特征会引入噪声吧，是否可以考虑标记默认好评数，再排除默认好评求取差评率好评率。</li><li>很多行为，例如抽烟、吃东西、喝酒等，并不是每位司机都会在平台上报备，这样的话直接拿这些行为用作特征，这个合理吗？因为这虽然是现实数据，但其实并不是实际中的真实分布（有人抽烟，但平台记录为无），所以此类行为加入到模型中怎样应用呢？</li><li>以投诉或差评为label，如果是恶意投诉或恶意差评呢？怎样去躁？或者提示模型的鲁棒性？这个用起来应该更长久一点。</li><li>性骚扰、盗窃、抢劫，这些极端恶劣性行为不一定有石锤，考虑直接客服介入？</li></ul><h4 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h4><ul><li>一次坏行为，一月坏行为率，行为*权重。这是用于评分卡模型的。有没有其他模型呢？更多的模型就能考虑更多要素。</li><li>所有新用户有一个基本分，后续增量变化，以此生成信用分，但是这个现实吗？早期数据够不够？</li></ul><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><ul><li>信用分增量更新的具体考量。高分速率放缓、加减分权重…</li><li>相当于异常检测，所以直接做统计检验来产生概率是否可以作为一个模型</li></ul><h2 id="信用分调研2019"><a href="#信用分调研2019" class="headerlink" title="信用分调研2019"></a>信用分调研2019</h2><h3 id="项目推进节奏"><a href="#项目推进节奏" class="headerlink" title="项目推进节奏"></a>项目推进节奏</h3><ul><li><p>乘客行为分 推动 乘客露出 比较难，水下则会容易得多。出现这种情况的原因主要在于，乘客行为分本质需要乘客完成隐私授权，作为对等交换，应当通过提供权益的方式达成这一目的。之前推动乘客侧的露出，主要是希望达成管控目标，这和乘客的期待有本质的冲突。</p><p>就是说向乘客露出了行为分那就得给乘客提供相应权益（乘客付出了隐私；乘客对信用分没有需求，所以用其他手段刺激需求），免押？打折/优惠券？满级奖励？接入芝麻分/信用评级？</p></li></ul><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ul><li><p><strong>评分卡模型</strong></p><p><em>F: 未来一段时间内是否出现恶劣行为 是很好的建模目标么？如何评估一个建模目标的好坏？</em></p><p>A: </p><ul><li>评分卡模型本质是一个二分类，因此离线指标为分类模型可用的指标：AUC/Recall/Precision/F1/Accuracy…</li><li>模型生成的分数在线上评估时有两个维度，区分度评估：K-S, GINI（还有其他的吗？树模型里面选择特征以及分裂点的指标例如信息增益、信息增益比等本质也是衡量区分度的，应该改改可以用吧）；稳定度评估：中位数</li><li>业务当中，为评价此指标的引入创造的价值还应该有业务指标的评价：坏账率、注册完单率、CPO、司机&amp;乘客NPS、GMV…（多个维度）<strong>业务上的评价应该是根本性的评价</strong></li></ul></li><li><p>信用分是历史行为的积累，但是用历史的行为来判责当次发生的问题显得依据不够，因此在内部推行的时候也遇到了不少困难，是否可以考虑对判责仅提供参考，但可以应用于例如降低押金、免押金、或恶意乘客识别（说明历史上有多次恶意行为）</p><p><em>F: “仅提供参考”是高价值／高优先级 的目标么？如果不是，哪些是高价值／高优先级 的目标。</em></p><p>A: 既然是只提供参考，那就不是高价值 / 高优先级的目标，高价值 / 高优先级的目标：履约能力（免押金、免预付，减少坏账）；减少取消率；提高司机NPS。BTW: 既然是在判责的时候用历史行为显得依据不够，那能不能把信用分应用于一个单子生命后期的更早期呢，例如预防坏账、预防取消、增强司机的体验…。</p></li><li><p>目前信用分对内部的定位是分层管控，但是管控却没有足够的依据，那是否可以考虑多次恶意行为后实施惩罚或者账号的某些动作限制，例如信用分太低则适当限制在发单之后取消订单等操作。 </p><p><em>F: 信用分的定位</em></p><p>A: </p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##吴文栋邮件&lt;/p&gt;
&lt;h3 id=&quot;先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。&quot;&gt;&lt;a href=&quot;#先问能不能？再问如何？“滴滴如何树立自己在出行方面
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>RF &amp; GBDT &amp; XGBoost面试题目</title>
    <link href="https://chenzk1.github.io/2019/07/03/RF%E3%80%81GBDT%E3%80%81XGBoost/"/>
    <id>https://chenzk1.github.io/2019/07/03/RF、GBDT、XGBoost/</id>
    <published>2019-07-03T00:52:13.797Z</published>
    <updated>2019-07-03T08:02:18.812Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RF与GBDT之间的区别"><a href="#RF与GBDT之间的区别" class="headerlink" title="RF与GBDT之间的区别"></a>RF与GBDT之间的区别</h1><h2 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h2><ul><li>都是由多棵树组成</li><li>最终的结果都是由多棵树一起决定</li></ul><h2 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h2><ul><li>组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成</li><li>组成随机森林的树可以并行生成，而GBDT是串行生成</li><li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li><li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li><li>随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的</li><li>随机森林不需要进行数据预处理，即特征归一化。而GBDT则需要进行特征归一化</li></ul><h3 id="分类树和回归树的区别"><a href="#分类树和回归树的区别" class="headerlink" title="分类树和回归树的区别"></a>分类树和回归树的区别</h3><ul><li>回归树和分类树的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。</li><li>分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。</li><li>回归树使用最小化均方差划分节点；每个节点样本的均值作为测试样本的回归预测值</li></ul><h1 id="Xgboost和GBDT的区别"><a href="#Xgboost和GBDT的区别" class="headerlink" title="Xgboost和GBDT的区别"></a>Xgboost和GBDT的区别</h1><ul><li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li><li>节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的。</li><li>Xgboost在代价函数里加入了正则项，用于控制模型的复杂度，降低了过拟合的可能性。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。</li><li>shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）</li><li>列采样</li><li><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。</p><ul><li>为什么xgboost要用泰勒展开，优势在哪里？xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。</li></ul></li><li><p>Xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行<strong>不是tree粒度的并行</strong>，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。<strong>xgboost的并行是在特征粒度上的</strong>。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，<strong>预先对数据进行了排序</strong>，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行</p></li></ul><h2 id="N问GBDT"><a href="#N问GBDT" class="headerlink" title="N问GBDT"></a>N问GBDT</h2><ul><li>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量</li><li>怎样设置单棵树的停止生长条件？<ul><li>A. 节点分裂时的最小样本数</li><li>B. 最大深度</li><li>C. 最多叶子节点数</li><li>D. loss满足约束条件</li></ul></li><li>如何评估特征的权重大小？<ul><li>A. 通过计算每个特征在训练集下的信息增益，最后计算每个特征信息增益与所有特征信息增益之和的比例为权重值。</li><li>B. 借鉴投票机制。用相同的gbdt参数对w每个特征训练出一个模型，然后在该模型下计算每个特征正确分类的个数，最后计算每个特征正确分类的个数与所有正确分类个数之和的比例为权重值。</li></ul></li><li>当增加样本数量时，训练时长是线性增加吗？<ul><li>不是。因为生成单棵决策树时，损失函数极小值与样本数量N不是线性相关</li></ul></li><li>当增加树的棵数时，训练时长是线性增加吗？<ul><li>不是。因为每棵树的生成的时间复杂度不一样。</li></ul></li><li>当增加一个棵树叶子节点数目时，训练时长是线性增加吗？<ul><li>不是。叶子节点数和每棵树的生成的时间复杂度不成正比。</li></ul></li><li>每个节点上都保存什么信息？<ul><li>中间节点保存某个特征的分割值，叶结点保存预测是某个类别的概率。</li></ul></li><li>如何防止过拟合？<ul><li>a. 增加样本（data bias or small data的缘故），移除噪声。</li><li>b. 减少特征，保留重要的特征（可以用PCA等对特征进行降维）。</li><li>c. 对样本进行采样（类似bagging）。就是建树的时候，不是把所有的样本都作为输入，而是选择一个子集。</li><li>d. 对特征进行采样。类似样本采样一样, 每次建树的时候，只对部分的特征进行切分。</li></ul></li><li>gbdt在训练和预测的时候都用到了步长，这两个步长一样么？都有什么用，如果不一样，为什么？怎么设步长的大小？（太小？太大？）在预测时，设太大对排序结果有什么影响？跟shrinking里面的步长一样么这两个步长一样么？<ul><li>训练跟预测时，两个步长是一样的，也就是预测时的步长为训练时的步长，从训练的过程可以得知（更新当前迭代模型的时候）。</li><li>都有什么用，如果不一样，为什么？答：它的作用就是使得每次更新模型的时候，使得loss能够平稳地沿着负梯度的方向下降，不至于发生震荡。</li><li>那么怎么设步长的大小?<ul><li>有两种方法，一种就是按策略来决定步长，另一种就是在训练模型的同时，学习步长。</li><li>策略：a.每个树步长恒定且相等，一般设较小的值；b.开始的时候给步长设一个较小值，随着迭代次数动态改变，或者说衰减。</li><li>学习：因为在训练第k棵树的时候，前k-1棵树时已知的，而且求梯度的时候是利用前k-1棵树来获得。所以这个时候，就可以把步长当作一个变量来学习。</li></ul></li><li>（太小？太大？）在预测时，对排序结果有什么影响？<ul><li>如果步长过大，在训练的时候容易发生震荡，使得模型学不好，或者完全没有学好，从而导致模型精度不好。</li><li>而步长过小，导致训练时间过长，即迭代次数较大，从而生成较多的树，使得模型变得复杂，容易造成过拟合以及增加计算量。</li></ul></li><li>跟shrinking里面的步长一样么？<ul><li>这里的步长跟shrinking里面的步长是一致的。</li></ul></li></ul></li><li>boosting的本意是是什么？跟bagging，random forest，adaboost，gradient boosting有什么区别？<ul><li>Bagging<ul><li>放回抽样，多数表决（分类）或简单平均（回归）</li><li>可以看成是一种圆桌会议，或是投票选举的形式。通过训练多个模型，将这些训练好的模型进行加权组合来获得最终的输出结果(分类/回归)，一般这类方法的效果，都会好于单个模型的效果。在实践中，在特征一定的情况下，大家总是使用Bagging的思想去提升效果。例如kaggle上的问题解决，因为大家获得的数据都是一样的，特别是有些数据已经过预处理。</li><li>基本的思路：训练时，使用replacement的sampling方法，sampling一部分训练数据k次并训练k个模型；预测时，使用k个模型，如果为分类，则让k个模型均进行分类并选择出现次数最多的类（每个类出现的次数占比可以视为置信度）；如为回归，则为各分类器返回的结果的平均值。在该处，Bagging算法可以认为每个分类器的权重都一样由于每次迭代的采样是独立的，所以bagging可以并行。</li></ul></li><li>Random forest<ul><li>随机森林在bagging的基础上做了修改。<ul><li>A. 从样本集散用Boostrap采样选出n个样本，预建立CART</li><li>B. 在树的每个节点上，从所有属性中随机选择k个属性/特征，选择出一个最佳属性/特征作为节点</li><li>C. 重复上述两步m次，i.e.build m棵cart</li><li>D. 这m棵cart形成random forest。</li></ul></li><li>随机森林可以既处理属性是离散的量，比如ID3算法，也可以处理属性为连续值得量，比如C4.5算法。这里的random就是指：<ul><li>A. boostrap中的随机选择样本</li><li>B. random subspace的算法中从属性/特征即中随机选择k个属性/特征，每棵树节点分裂时，从这随机的k个属性/特征，选择最优的。</li></ul></li></ul></li><li>Boosting:<ul><li>一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。</li><li>boosting的采样或者更改样本的权重依赖于上一次迭代的结果，在迭代层面上是不能并行的。</li><li>boosting在选择hyperspace的时候给样本加了一个权值，使得loss function尽量考虑那些分错类的样本（如分错类的样本weight大）。怎么做的呢？<ul><li>boosting重采样的不是样本，而是样本的分布，对于分类正确的样本权值低，分类错误的样本权值高(通常是边界附近的样本)，最后的分类器是很多弱分类器的线性叠加(加权组合)。</li></ul></li></ul></li><li>Adaboosting<ul><li>对一份数据，建立M个模型(比如分类)，而一般这种模型比较简单，称为弱分类器(weak learner)。每次分类都将上一次分错的数据权重提高一点，对分对的数据权重降低一点，再进行分类。这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的效果。</li><li>每次迭代的样本是一样的，即没有采样过程，不同的是不同的样本权重不一样。(当然也可以对样本/特征进行采样，这个不是adaboosting的原意)。</li><li>另外，每个分类器的步长由在训练该分类器时的误差来生成。</li></ul></li><li>Gradient boosting<ul><li>每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度 (Gradient)方向上建立一个新的模型。所以说在Gradient Boost中，每个新模型是为了使之前模型的残差往梯度方向减少，与传统Boost对正确，错误的样本进行加权有着很大的区别。</li></ul></li></ul></li><li>gbdt中哪些部分可以并行？<ul><li>A. 计算每个样本的负梯度</li><li>B. 分裂挑选最佳特征及其分割点时，对特征计算相应的误差及均值时</li><li>C. 更新每个样本的负梯度时</li><li>D. 最后预测过程中，每个样本将之前的所有树的结果累加的时候</li></ul></li><li>树生长成畸形树，会带来哪些危害，如何预防？<ul><li>在生成树的过程中，加入树不平衡的约束条件。这种约束条件可以是用户自定义的。例如对样本集中分到某个节点，而另一个节点的样本很少的情况进行惩罚。</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RF与GBDT之间的区别&quot;&gt;&lt;a href=&quot;#RF与GBDT之间的区别&quot; class=&quot;headerlink&quot; title=&quot;RF与GBDT之间的区别&quot;&gt;&lt;/a&gt;RF与GBDT之间的区别&lt;/h1&gt;&lt;h2 id=&quot;相同点&quot;&gt;&lt;a href=&quot;#相同点&quot; class
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="面试" scheme="https://chenzk1.github.io/tags/%E9%9D%A2%E8%AF%95/"/>
    
  </entry>
  
  <entry>
    <title>ROC AUC, Accuracy, Recall, F1</title>
    <link href="https://chenzk1.github.io/2019/07/02/%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/"/>
    <id>https://chenzk1.github.io/2019/07/02/树的遍历/</id>
    <published>2019-07-02T14:03:38.453Z</published>
    <updated>2019-07-02T14:17:33.083Z</updated>
    
    <content type="html"><![CDATA[<ul><li>树的遍历，分深度优先：前序，中序，后序；广度优先：层次遍历。</li><li>前序：根、左、右</li><li>中序：左、根、右</li><li>后序：左、右、根</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""节点类"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, elem=<span class="number">-1</span>, lchild=None, rchild=None)</span>:</span></span><br><span class="line">        self.elem = elem</span><br><span class="line">        self.lchild = lchild</span><br><span class="line">        self.rchild = rchild</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tree</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""树类"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.root = Node()</span><br><span class="line">        self.myQueue = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, elem)</span>:</span></span><br><span class="line">        <span class="string">"""为树添加节点"""</span></span><br><span class="line">        node = Node(elem)</span><br><span class="line">        <span class="keyword">if</span> self.root.elem == <span class="number">-1</span>:  <span class="comment"># 如果树是空的，则对根节点赋值</span></span><br><span class="line">            self.root = node</span><br><span class="line">            self.myQueue.append(self.root)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            treeNode = self.myQueue[<span class="number">0</span>]  <span class="comment"># 此结点的子树还没有齐。</span></span><br><span class="line">            <span class="keyword">if</span> treeNode.lchild == <span class="literal">None</span>:</span><br><span class="line">                treeNode.lchild = node</span><br><span class="line">                self.myQueue.append(treeNode.lchild)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                treeNode.rchild = node</span><br><span class="line">                self.myQueue.append(treeNode.rchild)</span><br><span class="line">                self.myQueue.pop(<span class="number">0</span>)  <span class="comment"># 如果该结点已经有了右子树，将此结点丢弃，从下一个节点插入。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">front_digui</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用递归实现树的先序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">print</span> root.elem,</span><br><span class="line">        self.front_digui(root.lchild)</span><br><span class="line">        self.front_digui(root.rchild)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle_digui</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用递归实现树的中序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.middle_digui(root.lchild)</span><br><span class="line">        <span class="keyword">print</span> root.elem,</span><br><span class="line">        self.middle_digui(root.rchild)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">later_digui</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用递归实现树的后序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.later_digui(root.lchild)</span><br><span class="line">        self.later_digui(root.rchild)</span><br><span class="line">        <span class="keyword">print</span> root.elem,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">front_stack</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用堆栈实现树的先序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        myStack = []</span><br><span class="line">        node = root</span><br><span class="line">        <span class="keyword">while</span> node <span class="keyword">or</span> myStack:</span><br><span class="line">            <span class="keyword">while</span> node:                     <span class="comment">#从根节点开始，一直找它的左子树</span></span><br><span class="line">                <span class="keyword">print</span> node.elem,</span><br><span class="line">                myStack.append(node)</span><br><span class="line">                node = node.lchild</span><br><span class="line">            node = myStack.pop()            <span class="comment">#while结束表示当前节点node为空，即前一个节点没有左子树了</span></span><br><span class="line">            node = node.rchild                  <span class="comment">#开始查看它的右子树</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle_stack</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用堆栈实现树的中序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        myStack = []</span><br><span class="line">        node = root</span><br><span class="line">        <span class="keyword">while</span> node <span class="keyword">or</span> myStack:</span><br><span class="line">            <span class="keyword">while</span> node:                     <span class="comment">#从根节点开始，一直找它的左子树</span></span><br><span class="line">                myStack.append(node)</span><br><span class="line">                node = node.lchild</span><br><span class="line">            node = myStack.pop()            <span class="comment">#while结束表示当前节点node为空，即前一个节点没有左子树了</span></span><br><span class="line">            <span class="keyword">print</span> node.elem,</span><br><span class="line">            node = node.rchild                  <span class="comment">#开始查看它的右子树</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">later_stack</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用堆栈实现树的后序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        myStack1 = []</span><br><span class="line">        myStack2 = []</span><br><span class="line">        node = root</span><br><span class="line">        myStack1.append(node)</span><br><span class="line">        <span class="keyword">while</span> myStack1:                   <span class="comment">#这个while循环的功能是找出后序遍历的逆序，存在myStack2里面</span></span><br><span class="line">            node = myStack1.pop()</span><br><span class="line">            <span class="keyword">if</span> node.lchild:</span><br><span class="line">                myStack1.append(node.lchild)</span><br><span class="line">            <span class="keyword">if</span> node.rchild:</span><br><span class="line">                myStack1.append(node.rchild)</span><br><span class="line">            myStack2.append(node)</span><br><span class="line">        <span class="keyword">while</span> myStack2:                         <span class="comment">#将myStack2中的元素出栈，即为后序遍历次序</span></span><br><span class="line">            <span class="keyword">print</span> myStack2.pop().elem,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">level_queue</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用队列实现树的层次遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        myQueue = []</span><br><span class="line">        node = root</span><br><span class="line">        myQueue.append(node)</span><br><span class="line">        <span class="keyword">while</span> myQueue:</span><br><span class="line">            node = myQueue.pop(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">print</span> node.elem,</span><br><span class="line">            <span class="keyword">if</span> node.lchild != <span class="literal">None</span>:</span><br><span class="line">                myQueue.append(node.lchild)</span><br><span class="line">            <span class="keyword">if</span> node.rchild != <span class="literal">None</span>:</span><br><span class="line">                myQueue.append(node.rchild)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="string">"""主函数"""</span></span><br><span class="line">    elems = range(<span class="number">10</span>)           <span class="comment">#生成十个数据作为树节点</span></span><br><span class="line">    tree = Tree()          <span class="comment">#新建一个树对象</span></span><br><span class="line">    <span class="keyword">for</span> elem <span class="keyword">in</span> elems:                  </span><br><span class="line">        tree.add(elem)           <span class="comment">#逐个添加树的节点</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'队列实现层次遍历:'</span></span><br><span class="line">    tree.level_queue(tree.root)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n\n递归实现先序遍历:'</span></span><br><span class="line">    tree.front_digui(tree.root)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n递归实现中序遍历:'</span> </span><br><span class="line">    tree.middle_digui(tree.root)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n递归实现后序遍历:'</span></span><br><span class="line">    tree.later_digui(tree.root)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n\n堆栈实现先序遍历:'</span></span><br><span class="line">    tree.front_stack(tree.root)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n堆栈实现中序遍历:'</span></span><br><span class="line">    tree.middle_stack(tree.root)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n堆栈实现后序遍历:'</span></span><br><span class="line">    tree.later_stack(tree.root)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;树的遍历，分深度优先：前序，中序，后序；广度优先：层次遍历。&lt;/li&gt;
&lt;li&gt;前序：根、左、右&lt;/li&gt;
&lt;li&gt;中序：左、根、右&lt;/li&gt;
&lt;li&gt;后序：左、右、根&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;t
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="DS" scheme="https://chenzk1.github.io/tags/DS/"/>
    
  </entry>
  
</feed>
