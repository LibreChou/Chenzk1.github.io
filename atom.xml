<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hero&#39;s notebooks</title>
  <icon>https://www.gravatar.com/avatar/ba13505eb0c4bc0e7771060ab0cb2b31</icon>
  <subtitle>Sometimes naive.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://chenzk1.github.io/"/>
  <updated>2019-06-06T03:12:58.821Z</updated>
  <id>https://chenzk1.github.io/</id>
  
  <author>
    <name>Hero</name>
    <email>chenzk666@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>信息熵、交叉熵、条件熵、互信息</title>
    <link href="https://chenzk1.github.io/2019/05/27/%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81%E6%9D%A1%E4%BB%B6%E7%86%B5%E3%80%81%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
    <id>https://chenzk1.github.io/2019/05/27/信息熵、交叉熵、条件熵、互信息/</id>
    <published>2019-05-27T00:17:35.958Z</published>
    <updated>2019-06-06T03:12:58.821Z</updated>
    
    <content type="html"><![CDATA[<ul><li>设随机变量X，有n个事件$x_i$ –&gt; $x_n$，概率分布为p(x)</li></ul><ol><li><p>信息</p><ul><li>某随机变量X取值为xi的信息为 $I(X=xi)=\log_2\frac{1}{p(x_i)}=-\log_2p(x_i)$<br>：某事件xi的信息代表这个事件能提供的信息，一个发生概率越小的事件能够提供的信息量越大。</li></ul></li><li><p>信息熵</p><ul><li>信息代表一个事件的不确定性，信息熵是整个随机变量X不确定性的度量：<strong>信息的期望</strong>。<br>$H(X)=\sum_0^np(x_i)*I(x_i)=-\sum_0^np(x_i)\log_2(p(x_i))$</li><li>信息熵只与变量X的分布有关，与其取值无关。例如二分类中，两取值的概率均为0.5时，其熵最大，也最难预测某时刻哪一类别会发生。</li><li>如何通俗的解释交叉熵与相对熵? - CyberRep的回答 - 知乎<br><a href="https://www.zhihu.com/question/41252833/answer/195901726" target="_blank" rel="noopener">https://www.zhihu.com/question/41252833/answer/195901726</a></li><li>对于一个系统而言，若获知其真实分布，则我们能够找到一个最优策略，以最小的代价来消除系统的不确定性，而这个最小的代价（猜题次数、编码长度等）就是信息熵。</li></ul></li><li><p>条件熵</p><ul><li>定义为：给定条件X下，Y的分布（Y|X）的熵对X的数学期望：$H(Y|X)=\sum_xp(x)H(Y|X=x)$</li><li>在ML中，即选定某个特征X(X有n类)后，label(Y)的条件概率熵求期望：<strong>给定X特征的条件下Y的信息熵</strong>。</li><li>条件熵越小，代表在这个特征下，label的信息熵越小，也就是说要解决问题的代价越小。</li></ul></li><li><p>信息增益 — ID3</p><ul><li>$IG(Y|X)=H(Y)-H(Y|X)$</li><li>在决策树中作为选择特征的指标，IG越大，这个特征的选择性越好，也可以理解为：待分类的集合的熵和选定某个特征的条件熵之差越大，这个特征对整个集合的影响越大。</li><li>对于条件熵来说，条件熵越小，分类后的纯度越高，但是问题是：X的取值越多，每个取值下Y的纯度越高，H(Y|X)越小，但此时并不有利于Y的区分。信息增益也是如此。–&gt; 信息增益率。</li></ul></li><li><p>信息增益率/信息增益比 — C4.5</p><ul><li>偏好取值少的特征。C4.5：先选择高于平均水平信息增益的特征，再在其中选择最高信息增益率的特征。</li><li>见<a href="https://chenzk1.github.io/2019/03/14/Decision%20Tree/">Decision Tree</a></li></ul></li><li><p>基尼系数 — CART</p><ul><li><p>表示数据的不纯度。既有分类也有回归，既要确定特征，也要确定特征的分叉值。</p></li><li><p>见<a href="https://chenzk1.github.io/2019/03/14/Decision%20Tree/">Decision Tree</a></p></li></ul></li><li><p>交叉熵</p><ul><li>前面提到：信息熵是最优策略下，消除系统不确定性的最小代价。这里的前提是：<strong>我们得到了系统的真实分布</strong>。</li><li>实际中，一般难以获知系统真实分布，所以要以假设分布去近似。<strong>交叉熵：用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小</strong>。$CEH(p,q)=\sum_{k=1}^np_k\log_2\frac{1}{q_k}$，注意这里log中是q，是基于非真实分布q的信息量对真实分布的期望。</li><li>当假设分布$q_k$与真实分布$p_k$相同时，交叉熵最低，等于信息熵，所以得到的策略为最优策略。<blockquote><p>在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。</p></blockquote></li></ul><blockquote><p>例如：在逻辑斯蒂回归或者神经网络中都有用到交叉熵作为评价指标，其中p即为真实分布的概率，而q为预测的分布，以此衡量两不同<strong>分布</strong>的相似性。 </p><ul><li>如何衡量不同<strong>策略</strong>的差异：相对熵</li></ul></blockquote></li><li><p>相对熵/K-L散度</p><ul><li>用来衡量两个取值为正的函数或概率分布之间的差异。两者相同相对熵为0</li><li>使用非真实分布q的交叉熵，与使用真实分布p的的信息熵的差值：相对熵，又称K-L散度。</li><li>$KL(p,q)=CEH(p,q)-H(p)=\sum_{i=1}^np(x_i)\log\frac{p(x_i)}{q(x_i)}$</li></ul></li><li><p>联合熵</p><ul><li>H(X,Y) 随机变量X,Y联合表示的信息熵</li></ul></li><li><p>互信息</p><ul><li>H（X；Y）俩变量交集，也记作I(X;Y)</li><li>H（X；Y) = H(X,Y)-H(Y|X)-H(X|Y)</li><li>I(X;Y)=KL(P(X,Y), P(X)P(Y))</li></ul></li></ol><ul><li>互信息越小，两变量独立性越强，P(X,Y)与P(X)P(Y)差异越小，P(X,Y)与P(X)P(Y)的相对熵越小  </li><li>相对熵(p,q) = 信息熵(p) - 交叉熵(p,q)</li><li>信息增益(Y|X) = 信息熵(Y) - 条件熵(Y|X)</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;设随机变量X，有n个事件$x_i$ –&amp;gt; $x_n$，概率分布为p(x)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;信息&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;某随机变量X取值为xi的信息为 $I(X=xi)=\log_2\frac{1}{p(x_i)}=-\lo
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>CTR_LR/Poly2/FM/FFM</title>
    <link href="https://chenzk1.github.io/2019/05/26/CTR/"/>
    <id>https://chenzk1.github.io/2019/05/26/CTR/</id>
    <published>2019-05-26T12:57:46.294Z</published>
    <updated>2019-06-06T00:31:45.509Z</updated>
    
    <content type="html"><![CDATA[<ol><li>LR</li></ol><ul><li>问题：特征之间无相关性</li></ul><ol start="2"><li>Ploy2</li></ol><ul><li>暴力加入两两特征组合（权重*两特征点积）</li><li>问题：大部分特征是稀疏的，得到的特征值都是0，所以梯度更新时，因为大部分feature为0所以梯度并不会更新</li></ul><ol start="3"><li>FM(Factorization Machine、因子机)</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;LR&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;问题：特征之间无相关性&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Ploy2&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;暴力加入两两特征组合（权重*两特征点积）&lt;/li&gt;
&lt;li&gt;问题：大部分特征是稀
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="CTR" scheme="https://chenzk1.github.io/tags/CTR/"/>
    
  </entry>
  
  <entry>
    <title>Tips in DS Competition</title>
    <link href="https://chenzk1.github.io/2019/04/11/Tips%20in%20DS%20Competition/"/>
    <id>https://chenzk1.github.io/2019/04/11/Tips in DS Competition/</id>
    <published>2019-04-11T12:30:34.956Z</published>
    <updated>2019-04-11T14:03:24.795Z</updated>
    
    <content type="html"><![CDATA[<h2 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h2><ul><li>删除</li><li>imputation: mean mode …</li><li>imputation + missing_flag</li><li>…</li></ul><h2 id="Categorial-Columns"><a href="#Categorial-Columns" class="headerlink" title="Categorial Columns"></a>Categorial Columns</h2><ul><li>对于种类不是很多的：onehot encoder<ul><li>sklearn.preprocessing.OneHotEncoder: 如果使用线性模型，存在一个问题就是生成的n列是线性相关的，因此要满足线性无关就要删除其中一列。该类提供了drop_first参数</li></ul></li><li>不用label encoder的原因：label encoder引入了大小顺序</li></ul><h2 id="XGBOOST"><a href="#XGBOOST" class="headerlink" title="XGBOOST"></a>XGBOOST</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;处理缺失数据&quot;&gt;&lt;a href=&quot;#处理缺失数据&quot; class=&quot;headerlink&quot; title=&quot;处理缺失数据&quot;&gt;&lt;/a&gt;处理缺失数据&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;删除&lt;/li&gt;
&lt;li&gt;imputation: mean mode …&lt;/li&gt;
&lt;li&gt;imp
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="DataScience" scheme="https://chenzk1.github.io/tags/DataScience/"/>
    
  </entry>
  
  <entry>
    <title>SVD&amp;PCA</title>
    <link href="https://chenzk1.github.io/2019/04/06/SVD&amp;PCA/"/>
    <id>https://chenzk1.github.io/2019/04/06/SVD&amp;PCA/</id>
    <published>2019-04-06T03:07:20.164Z</published>
    <updated>2019-04-06T08:09:32.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="特征值分解-Eigen-Value-Decomposition"><a href="#特征值分解-Eigen-Value-Decomposition" class="headerlink" title="特征值分解(Eigen Value Decomposition)"></a>特征值分解(Eigen Value Decomposition)</h3><p>Ax = λx</p><p>-&gt; A = WΣW^(-1)</p><p>其中Σ为对角阵，对角的值是A的特征值；W的列向量为对应的特征向量</p><p>对W标准化后，即Wi^(T).wi = 1</p><p>所以W^(T).W = I –&gt; W^(T) = W^(-1)</p><p><strong>W经标准化后为酉矩阵</strong></p><p>-&gt; A = WΣW^(T)</p><p><strong>!!A必须是方阵</strong></p><h3 id="SVD-Singular-Value-Decomposition"><a href="#SVD-Singular-Value-Decomposition" class="headerlink" title="SVD(Singular Value Decomposition)"></a>SVD(Singular Value Decomposition)</h3><ul><li>可以对<strong>非方阵</strong>分解</li><li>A = UΣV^(T), A: m x n, U: m x m, V: n x n, Σ: m x n; <strong>U和V都为酉矩阵</strong>，Σ主对角线上元素为奇异值</li><li>UVΣ的求解：<ul><li>U: AA^(T) = UΣ1U^(T)</li><li>V: A^(T)A = VΣ2V^(T)</li><li>Σ: A = UΣV^(T) =&gt; AV = UΣ =&gt; U^(T)AV = Σ =&gt; σi = Avi/ui</li></ul></li><li>性质：可以用几个最大的奇异值及其左右奇异向量近似原矩阵</li><li>应用：降维，数据压缩，去噪声；也可用于NLP，如LSA…</li></ul><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><h4 id="基"><a href="#基" class="headerlink" title="基"></a>基</h4><p>由线性不相关的向量组成，有时会取正交。</p><h4 id="坐标变换-amp-矩阵相乘"><a href="#坐标变换-amp-矩阵相乘" class="headerlink" title="坐标变换&amp;矩阵相乘"></a>坐标变换&amp;矩阵相乘</h4><p>AB = C，B矩阵的每一个列向量变换到以A矩阵的行向量为基表示的空间中，最终得到的向量的维度（C的行数）取决于基的个数 –&gt; 可用于降、升维</p><h4 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h4><ul><li>降维的目标：维数变低&amp;尽量保留更多的信息。</li><li>对于二维降到一维，要保留更多的信息，则原始向量在基向量上的投影应相隔距离尽量远 –&gt; 大方差</li><li><p>对于高维数据，如3维到2维，若只遵循大方差的原则，则两个基向量会相隔很近 –&gt; 信息不够分散 –&gt; 基向量之间的相关系数应尽量小</p></li><li><p>方差：单个随机变量之间的离散程度；协方差：多个随机变量之间的相似性</p></li></ul><p><strong>综上 –&gt; 协方差矩阵</strong></p><h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>协方差矩阵对角线上是原矩阵的方差，其他位置的元素是原矩阵两两向量之间的协方差 –&gt; 协方差矩阵是实对称矩阵 –&gt; 可逆</p><ul><li>原始问题即协方差矩阵的对角化</li></ul><h4 id="协方差矩阵对角化"><a href="#协方差矩阵对角化" class="headerlink" title="协方差矩阵对角化"></a>协方差矩阵对角化</h4><ul><li>原向量X，对应的协方差矩阵为C，P为基向量组成的变换矩阵；X经变换后为Y，Y=PX，Y的协方差矩阵为D，则：设X Y的期望为0，<br>D = YY^(T) / m = PX X^(T)P^(T) / m = PCP^(T)</li><li>PCA即寻找矩阵P使得 PCP^(T)是一个对角矩阵，且对角线上的值从大到小排列，取前k个值，以及对应P中的k个向量，即可将原n维矩阵降维至k维 –&gt; D对角线上的值即特征值，P为特征向量组成的矩阵</li></ul><h4 id="PCA-1"><a href="#PCA-1" class="headerlink" title="PCA"></a>PCA</h4><ul><li>总结：寻找实现协方差矩阵对角化的矩阵P，并应用P对原有数据进行变换</li><li>算法步骤<br>设有m条n维数据：</li></ul><ol><li>将原始数据按列组成n行m列矩阵X</li><li>将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li><li>求出协方差矩阵C = XX^(T) / m</li><li>求出协方差矩阵的特征值及对应的特征向量</li><li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</li><li>Y=PX即为降维到k维后的数据</li></ol><ul><li>优点：降低数据特征维度，减少数据存储量；加快运行速度</li><li>注意事项：<strong>量纲敏感性</strong>，最好进行量纲统一化；适用于大样本，小样本的话建议因子分析法</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;特征值分解-Eigen-Value-Decomposition&quot;&gt;&lt;a href=&quot;#特征值分解-Eigen-Value-Decomposition&quot; class=&quot;headerlink&quot; title=&quot;特征值分解(Eigen Value Decompositio
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Topic Model__TF-IDF/LSA/pLSA/NMF</title>
    <link href="https://chenzk1.github.io/2019/04/05/Topic%20Model__TF-IDF_LSA_pLSA_NMF/"/>
    <id>https://chenzk1.github.io/2019/04/05/Topic Model__TF-IDF_LSA_pLSA_NMF/</id>
    <published>2019-04-05T05:40:47.788Z</published>
    <updated>2019-04-08T12:50:47.316Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Topic-Model"><a href="#Topic-Model" class="headerlink" title="Topic Model"></a>Topic Model</h1><p><em><strong>主题模型即在大量文档中发现潜在主题的统计模型</strong></em></p><p>主题模型是一种<strong>生成式有向图</strong>模型，即文档以一定概率选择主题，而主题是单词的概率分布。</p><p>文档 –&gt; 主题 –&gt; 单词</p><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><ul><li>TF: term frequency —— (# occurrences of term t in document) / (# of words in documents)</li><li>IDF: inverse document frequency —— log(# of documents / # documents with term t in it)</li><li><p>TF * IDF</p></li><li><p>一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。一个词term的重要性与其在整个文件中出现的频率成正比；与它在语料库中的频率成反比。</p></li><li>可以用来提取关键词</li><li>有不足：比如某次在每个文件中都出现，这个词可能是is are之类的无用词，也可能是可以代表该文件库的关键词—-&gt;结合停用词处理？；没有考虑语义关联</li></ul><h2 id="LSA-LSI-Latent-Semantic-Analysis-Indexing"><a href="#LSA-LSI-Latent-Semantic-Analysis-Indexing" class="headerlink" title="LSA/LSI(Latent Semantic Analysis/Indexing)"></a>LSA/LSI(Latent Semantic Analysis/Indexing)</h2><p>潜在语义分析/检索</p><ul><li>假设有m个输入文档，每个文档有n个词项，则可以组成一个term-document的稀疏矩阵A∈Rm*n，它的行对应词项、列对应文档；Aij对应第i个文档的第j个词项，可以通过TF-IDF、词项在文档中出现的次数等方式确定矩阵每个元素的权重作为计算输入。经过SVD分解后将奇异值从大到小排列，取前k个最大的奇异值作为对原矩阵A的近似表示，Σ中的每个奇异值代表了潜在语义的重要度。</li><li>通过一次SVD分解就可以得到主题模型，同时解决语义的问题，但是计算得到的矩阵U、V中经常存在负数；可以通过计算词项（U的行）、文档（V的行or VT的列）之间的余弦相似度得到词项与词项、文档与文档之间的相似度；还可以对U、V中的词项和文档直接进行聚类，提取语义级别的近义词集合，便于搜索且减少数据存储量。</li><li>LSA适用于较小规模数据，可用于文档分类/聚类、同义词/多义词检索、跨语言搜索；SVD的计算很耗时，且潜在语义的数量k的选择对结果的影响非常大；LSA的原理简单但得到的不是概率模型，缺乏统计基础，矩阵中的负值难以解释，无法对应成现实中的概念。</li></ul><h2 id="pLSA-Potential-Latent-Semantic-Analysis-Indexing"><a href="#pLSA-Potential-Latent-Semantic-Analysis-Indexing" class="headerlink" title="pLSA(Potential Latent Semantic Analysis/Indexing)"></a>pLSA(Potential Latent Semantic Analysis/Indexing)</h2><p>引入了隐含变量，并使用了EM算法求解</p><h2 id="NMF-Non-negative-Matrix-Factorization"><a href="#NMF-Non-negative-Matrix-Factorization" class="headerlink" title="NMF(Non-negative Matrix Factorization)"></a>NMF(Non-negative Matrix Factorization)</h2><ul><li>V ≈ WH，其中V∈Rn<em>m为文档-单词矩阵，W∈Rn</em>r体现文档和主题的概率相关度，H∈Rr*m体现单词和主题的概率相关度。<br><img src="https://img-blog.csdn.net/20180713123152376?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXk5ODAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="V=WH"></li><li>目的：V与WH的误差最小化，度量方式可选择欧几里得距离、KL散度…</li><li>NMF的目标函数中共包含了n<em>r+r</em>m个参数，可以使用梯度下降法、拟牛顿法、坐标轴下降法等进行求解。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Topic-Model&quot;&gt;&lt;a href=&quot;#Topic-Model&quot; class=&quot;headerlink&quot; title=&quot;Topic Model&quot;&gt;&lt;/a&gt;Topic Model&lt;/h1&gt;&lt;p&gt;&lt;em&gt;&lt;strong&gt;主题模型即在大量文档中发现潜在主题的统计模型
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="NLP" scheme="https://chenzk1.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>RNN &amp; LSTM &amp; GRU</title>
    <link href="https://chenzk1.github.io/2019/04/04/LSTM/"/>
    <id>https://chenzk1.github.io/2019/04/04/LSTM/</id>
    <published>2019-04-04T00:05:19.299Z</published>
    <updated>2019-04-05T07:03:21.021Z</updated>
    
    <content type="html"><![CDATA[<ol><li>RNN</li></ol><p><img src="https://pic1.zhimg.com/v2-206db7ba9d32a80ff56b6cc988a62440_r.jpg" alt="示意图"></p><ul><li>动机：RNN之前，语言模型主要是N-gram，N可以变动，其作用是某位置的词取决于前N个词，所以该方法对任意序列的文本处理存在问题。</li><li>RNN：<em>理论上</em>可以看到往前看/往后看任意的长度。</li><li>最简单的RNN，即t时刻隐藏层的值既取决于t时刻的输入，也取决于t-1时刻的隐藏层的值。</li><li>双向循环网络，应用了下文的信息；深度循环网络，使用了多层隐藏层，更复杂了。</li></ul><p>一个教程：<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">RNN</a></p><ol start="2"><li>LSTM(Long Short Term)</li></ol><ul><li>动机：由RNN的机理可知，如果每层向下一个时刻传输的权重w大于1，则容易产生梯度爆炸问题，小于1，则会产生梯度消失<br>一个对比图<br><img src="https://pic4.zhimg.com/v2-e4f9851cad426dfe4ab1c76209546827_r.jpg" alt="RNN&amp;LSTM"><br>RNN只有一个传递状态ht，而LSTM有两个传递状态：cell state, hidden state。<br>其中cell state是上一序列的隐藏层输出加上一些东西（有点类似于RNN中的ht）；而ht变化会比较大。</li><li>过程：<ul><li>计算四个状态：<br><img src="https://pic4.zhimg.com/80/v2-15c5eb554f843ec492579c6d87e1497b_hd.jpg" alt="z&amp;zi"><br><img src="https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_hd.jpg" alt="zo&amp;zf"><br>z是输入，用了tanh，取值范围为-1，相当于归一化；zi&amp;zo&amp;zf为门控，用了sigmoid，输出为0~1，与某值相乘后是对该值的选择，其中i是information，f是forget，o是output；</li><li>四个状态在LSTM中的应用：<br><img src="https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_hd.jpg" alt="LSTM结构"><ul><li>忘记：zf点乘ct，即通过zf忘记不重要的</li><li>选择记忆：对于输入信息z，点乘zi，进行有选择的记忆</li><li>输出：对上一阶段的c通过tanh进行放缩，再通过zo控制输出</li></ul></li></ul></li></ul><ol start="3"><li>GRU(Gate Recurrent Unit)</li></ol><ul><li>同样为了解决梯度问题</li><li>比LSTM：运算量小</li><li>使用了同一个门控状态控制记忆和忘记</li></ul><p>具体：<br><a href="https://zhuanlan.zhihu.com/p/32481747" target="_blank" rel="noopener">原文</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;RNN&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&quot;https://pic1.zhimg.com/v2-206db7ba9d32a80ff56b6cc988a62440_r.jpg&quot; alt=&quot;示意图&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;动机：RNN之前，语言模型
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>FP Tree算法</title>
    <link href="https://chenzk1.github.io/2019/03/19/FP%20Tree/"/>
    <id>https://chenzk1.github.io/2019/03/19/FP Tree/</id>
    <published>2019-03-19T13:51:07.844Z</published>
    <updated>2019-03-19T14:24:41.333Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/pinard/p/6307064.html" target="_blank" rel="noopener">blog</a></p><ol><li><p>FP Tree是Aprior算法的优化</p><p>优化点在于：Aprior需要多次扫描数据以查询频繁项，FP Tree使用了树结构，提高了算法运行效率。</p></li><li><p>FP Tree数据结构</p><ul><li>项头表。格式为： | 项 | 该项出现的次数（降序排列） </li><li>FP Tree。将原始数据集映射为一个FP Tree。</li><li>节点链表。所有项头表里的1项频繁集都是一个节点链表的头，它依次指向FP树中该1项频繁集出现的位置。这样做主要是方便项头表和FP Tree之间的联系查找和更新，也好理解。</li></ul></li><li><p>项头表建立</p><ul><li>第一次扫描数据：得到频繁一项集的计数，删除支持度小于阈值的项，并将剩余频繁集放入项头表，降序排列。</li><li><p>第二次扫描数据：从<strong>原始数据</strong>中删除非频繁项集，并对<strong>每一条数据</strong>按支持度降序排列其中的非频繁项集。</p></li><li><p>e.g.<br><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119161846125-505903867.png" alt="e.g."></p></li></ul></li><li><p>FP Tree建立</p><ul><li>以null为根节点</li><li><p>按照项头表的顺序，从第一条数据开始插入频繁项集，每条数据中排序靠前的为祖先节点，反之为子孙节点，每个节点均置1<br><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119163935296-1386696266.png" alt="e.g."> </p></li><li><p>接下来进行下一条数据的插入。当已存在频繁项时，在原有数字上加一即可（注意在插入时，是每一层每一层插入，不能跳过某层直接在已有节点上加一）</p></li><li>…</li><li><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165427593-1237891371.png" alt="最终"></li></ul></li><li><p>节点链表</p><p>节点链表即项头表中每个频繁项集都有一个指针指向FP Tree的相应节点。</p></li><li><p>FP Tree挖掘</p><p>得到了FP树和项头表以及节点链表，我们首先要从项头表的底部项依次向上挖掘。对于项头表对应于FP树的每一项，我们要找到它的条件模式基。所谓条件模式基是以我们要挖掘的节点作为叶子节点所对应的FP子树。得到这个FP子树，我们将子树中每个节点的的计数设置为叶子节点的计数，并删除计数低于支持度的节点。从这个条件模式基，我们就可以递归挖掘得到频繁项集了。</p><ul><li>先从最底下的F节点开始，我们先来寻找F节点的条件模式基，由于F在FP树中只有一个节点，因此候选就只有下图左所示的一条路径，对应{A:8,C:8,E:6,B:2, F:2}。我们接着将所有的祖先节点计数设置为叶子节点的计数，即FP子树变成{A:2,C:2,E:2,B:2, F:2}。一般我们的条件模式基可以不写叶子节点，因此最终的F的条件模式基如下图右所示。 <img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119170723421-1812925376.png" alt="e.g."></li><li>通过它，我们很容易得到F的频繁2项集为{A:2,F:2}, {C:2,F:2}, {E:2,F:2}, {B:2,F:2}。递归合并二项集，得到频繁三项集为{A:2,C:2,F:2}，{A:2,E:2,F:2},…还有一些频繁三项集，就不写了。当然一直递归下去，最大的频繁项集为频繁5项集，为{A:2,C:2,E:2,B:2,F:2}</li><li>F挖掘完了，我们开始挖掘D节点。D节点比F节点复杂一些，因为它有两个叶子节点，因此首先得到的FP子树如下图左。我们接着将所有的祖先节点计数设置为叶子节点的计数，即变成{A:2, C:2,E:1 G:1,D:1, D:1}此时E节点和G节点由于在条件模式基里面的支持度低于阈值，被我们删除，最终在去除低支持度节点并不包括叶子节点后D的条件模式基为{A:2, C:2}。通过它，我们很容易得到D的频繁2项集为{A:2,D:2}, {C:2,D:2}。递归合并二项集，得到频繁三项集为{A:2,C:2,D:2}。D对应的最大的频繁项集为频繁3项集。 <img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119171924093-1331841220.png" alt="e.g."></li></ul></li><li><p>算法具体步骤</p><ul><li>扫描数据，得到所有频繁一项集的的计数。然后删除支持度低于阈值的项，将1项频繁集放入项头表，并按照支持度降序排列。</li><li>扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。</li><li>读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到FP树后，FP树的建立完成。</li><li>从项头表的底部项依次向上找到项头表项对应的条件模式基。从条件模式基递归挖掘得到项头表项项的频繁项集。</li><li>如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集。</li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.cnblogs.com/pinard/p/6307064.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;blog&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;FP Tree是Aprior算法的优化&lt;/p&gt;
&lt;
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>Apriori 关联规则挖掘</title>
    <link href="https://chenzk1.github.io/2019/03/18/Apriori%20%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98/"/>
    <id>https://chenzk1.github.io/2019/03/18/Apriori 关联规则挖掘/</id>
    <published>2019-03-18T01:43:37.509Z</published>
    <updated>2019-03-18T02:33:56.654Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>问题引入</p><ul><li>经常被同时购买的商品可以摆近一点，刺激购买欲望</li><li>通过附属产品优惠的方式，刺激主产品的销售</li></ul></li><li><p>总体思想</p><p>逐层搜索迭代，通过K-1项集迭代出K项集。<br>Aprior是用于压缩搜索空间。</p></li><li><p>概念</p><ul><li>支持度：<br>关联规则A-&gt;B的支持度support=P(AB)，同时发生的概率</li><li>置信度：<br>confidence=P(A|B)</li><li>k项集：<br>事件A中包含k个元素，成为k项集；若事件A满足最小支持度阈值，称其为频繁k项集</li><li>由频繁项集产生强关联规则<ul><li>K维数据项集LK是频繁项集的必要条件是它所有K-1维子项集也为频繁项集，记为LK-1　</li><li>如果K维数据项集LK的任意一个K-1维子集Lk-1，不是频繁项集，则K维数据项集LK本身也不是最大数据项集。</li><li>Lk是K维频繁项集，如果所有K-1维频繁项集合Lk-1中包含LK的K-1维子项集的个数小于K，则Lk不可能是K维最大频繁数据项集。</li><li>同时满足最小支持度阀值和最小置信度阀值的规则称为强规则。</li></ul></li><li>e.g.<br>顾客购买记录的数据库D，包含6个事务。项集I={网球拍,网球,运动鞋,羽毛球}。考虑关联规则：网球拍网球，事务1,2,3,4,6包含网球拍，事务1,2,6同时包含网球拍和网球，支持度，置信度。若给定最小支持度，最小置信度，关联规则网球拍网球是有趣的，认为购买网球拍和购买网球之间存在强关联。</li></ul></li><li><p>算法步骤</p><p>Apriori算法过程分为两个步骤：</p><ul><li>第一步通过迭代，检索出事务数据库中的所有频繁项集，即支持度不低于用户设定的阈值的项集；</li><li>第二步利用频繁项集构造出满足用户最小信任度的规则。</li></ul><p>具体做法就是：</p><p>首先找出频繁1-项集，记为L1；然后利用L1来产生候选项集C2，对C2中的项进行判定挖掘出L2，即频繁2-项集；不断如此循环下去直到无法发现更多的频繁k-项集为止。每挖掘一层Lk就需要扫描整个数据库一遍。算法利用了一个性质：</p><p>Apriori 性质：任一频繁项集的所有非空子集也必须是频繁的。意思就是说，生成一个k-itemset的候选项时，如果这个候选项有子集不在(k-1)-itemset(已经确定是frequent的)中时，那么这个候选项就不用拿去和支持度判断了，直接删除。具体而言：</p><ul><li><p>连接步:<br>为找出Lk（所有的频繁k项集的集合），通过将Lk-1（所有的频繁k-1项集的集合）与自身连接产生候选k项集的集合。候选集合记作Ck。设l1和l2是Lk-1中的成员。记li[j]表示li中的第j项。假设Apriori算法对事务或项集中的项按字典次序排序，即对于（k-1）项集li，li[1]&lt;li[2]&lt;……….&lt;li[k-1]。将Lk-1与自身连接，如果(l1[1]=l2[1])&amp;&amp;( l1[2]=l2[2])&amp;&amp;……..&amp;&amp; (l1[k-2]=l2[k-2])&amp;&amp;(l1[k-1]&lt;l2[k-1])，(这里的作用是为了保证不产生重复的k项集)，那认为l1和l2是可连接。连接l1和l2 产生的结果是{l1[1],l1[2],……,l1[k-1],l2[k-1]}。</p></li><li><p>剪枝步:<br>CK是LK的超集，也就是说，CK的成员可能是也可能不是频繁的。通过扫描所有的事务（交易），确定CK中每个候选的计数，判断是否小于最小支持度计数，如果不是，则认为该候选是频繁的。为了压缩Ck,可以利用Apriori性质：任一频繁项集的所有非空子集也必须是频繁的，反之，如果某个候选的非空子集不是频繁的，那么该候选肯定不是频繁的，从而可以将其从CK中删除。</p></li></ul></li><li><p>e.g.</p><p><a href="https://blog.csdn.net/u011067360/article/details/24810415" target="_blank" rel="noopener">例子</a></p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;p&gt;问题引入&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;经常被同时购买的商品可以摆近一点，刺激购买欲望&lt;/li&gt;
&lt;li&gt;通过附属产品优惠的方式，刺激主产品的销售&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;总体思想&lt;/p&gt;
&lt;p&gt;逐层搜索迭代，通过K-1项集迭代出K项集
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>KNN</title>
    <link href="https://chenzk1.github.io/2019/03/14/knn/"/>
    <id>https://chenzk1.github.io/2019/03/14/knn/</id>
    <published>2019-03-14T07:03:48.611Z</published>
    <updated>2019-03-14T07:07:55.198Z</updated>
    
    <content type="html"><![CDATA[<ol><li><p>算法步骤</p><ul><li>给定已经分好类的训练集</li><li>对给定的测试数据，计算其与训练集中每一个样本的距离</li><li>取距离最小的前K个，按多数表决的方法决定属于哪一类</li></ul></li><li><p>注意事项</p><ul><li>数据要fair</li><li>可以通过对距离近的样本点加更大的权重优化</li><li>计算量较大</li></ul></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;&lt;p&gt;算法步骤&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给定已经分好类的训练集&lt;/li&gt;
&lt;li&gt;对给定的测试数据，计算其与训练集中每一个样本的距离&lt;/li&gt;
&lt;li&gt;取距离最小的前K个，按多数表决的方法决定属于哪一类&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;注意事项
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="KNN" scheme="https://chenzk1.github.io/tags/KNN/"/>
    
  </entry>
  
  <entry>
    <title>自然辩证法#6</title>
    <link href="https://chenzk1.github.io/2019/03/14/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95#6/"/>
    <id>https://chenzk1.github.io/2019/03/14/自然辩证法#6/</id>
    <published>2019-03-14T01:54:35.330Z</published>
    <updated>2018-12-25T06:25:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="科技与社会（一）"><a href="#科技与社会（一）" class="headerlink" title="科技与社会（一）"></a>科技与社会（一）</h1><ul><li>经典科学 -&gt; 现代科学</li><li>学者型科学 -&gt; 产业型科学<br>经典科学：作为纯粹理性的科学，非职业化的科学，追求一般规律的科学，简单和谐的科学。经典科学，经典科学家，与哲学、艺术靠近，应用是附带的次要的，成就及其大小主要由政治经济系统评价。</li><li>产业科学或科学的产业化：例如19世纪的重工业</li><li>科学的质变：<ul><li>科学目标：纯粹理性 -&gt; 产品性能（垄断手段）和劳动生产率</li><li>科学主体：个体 -&gt; 集体</li><li>科学家：自由学者 -&gt; 雇员</li><li>科学资源：数学、哲学 -&gt; 市场</li><li>科学知识：简谐 -&gt; 爆炸</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;科技与社会（一）&quot;&gt;&lt;a href=&quot;#科技与社会（一）&quot; class=&quot;headerlink&quot; title=&quot;科技与社会（一）&quot;&gt;&lt;/a&gt;科技与社会（一）&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;经典科学 -&amp;gt; 现代科学&lt;/li&gt;
&lt;li&gt;学者型科学 -&amp;gt; 产业型
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="自然辩证法" scheme="https://chenzk1.github.io/tags/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>自然辩证法#5</title>
    <link href="https://chenzk1.github.io/2019/03/14/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95#5/"/>
    <id>https://chenzk1.github.io/2019/03/14/自然辩证法#5/</id>
    <published>2019-03-14T01:54:35.314Z</published>
    <updated>2018-12-18T06:40:00.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="当代西方科学论"><a href="#当代西方科学论" class="headerlink" title="当代西方科学论"></a>当代西方科学论</h1><p>科学的地位、作用和权力剧增，科学的合理性被关注</p><p>英，波普尔，证伪主义</p><p>美，库恩，历史主义</p><p>科学划界和科学动力学</p><h2 id="“可证实性”与“可证伪性”"><a href="#“可证实性”与“可证伪性”" class="headerlink" title="“可证实性”与“可证伪性”"></a>“可证实性”与“可证伪性”</h2><p>波普尔对“可证实性”的反思：</p><ul><li>有无意义不取决于属于本身，而取决于理论（语境）；</li><li>只有单称陈述可证实，而科学理论是全称陈述；</li><li>单称陈述也未必可证实，观察中渗透着理论；</li><li>“可证实性”使科学划界混乱。</li></ul><p>可证伪性：任何科学命题在逻辑上都可以被经验证明为假。</p><p>什么命题是无法被证明为假的</p><p>可证伪度：普遍性、精确性、简单性。这正好是科学理论命题的性质。</p><p>可证伪：证伪主义的科学划界标准。</p><h2 id="证伪主义的科学动力学"><a href="#证伪主义的科学动力学" class="headerlink" title="证伪主义的科学动力学"></a>证伪主义的科学动力学</h2><p>对归纳逻辑的反思：归纳法几乎总是受预设的干扰；归纳法非常有可能把重要的因素排除在外；归纳法有可能建立假的因果关系。总之，归纳法不是科学合理性的基础。</p><p>科学始于问题</p><p>P1 -&gt; TT -&gt; EE -&gt; P2</p><ul><li>P: Problem</li><li>TT: Tentative Theory</li><li>EE: Elimation of Error</li></ul><p>库恩对证伪主义的批判：科学史表明，自然现象喊“不”，科学家喊得更响。</p><p>观察渗透理论的问题。</p><p>证伪主义的启发：“正确”和有效。</p><p>至此，真理问题换为合理性问题，再换为有效性问题。</p><h2 id="范式和科学共同体"><a href="#范式和科学共同体" class="headerlink" title="范式和科学共同体"></a>范式和科学共同体</h2><p>库恩的历史主义科学哲学试图回答：一致性来自何处？科学是如何进步的？</p><p>范式：信念、信仰、习惯、原则、原理…</p><p>科学共同体：掌握、遵守和使用范式的群体。职业的、紧密的、有话语权的专家。</p><h2 id="历史主义的科学动力学"><a href="#历史主义的科学动力学" class="headerlink" title="历史主义的科学动力学"></a>历史主义的科学动力学</h2><p>前科学 无范式 -&gt; 常规科学 范式 -&gt; 反思 -&gt; 危机 -&gt; 科学革命新的范式</p><p>库恩列举几大科学革命：哥白尼日心说、氧化理论、相对论和量子力学</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;当代西方科学论&quot;&gt;&lt;a href=&quot;#当代西方科学论&quot; class=&quot;headerlink&quot; title=&quot;当代西方科学论&quot;&gt;&lt;/a&gt;当代西方科学论&lt;/h1&gt;&lt;p&gt;科学的地位、作用和权力剧增，科学的合理性被关注&lt;/p&gt;
&lt;p&gt;英，波普尔，证伪主义&lt;/p&gt;
&lt;p&gt;美
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="自然辩证法" scheme="https://chenzk1.github.io/tags/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>自然辩证法#4</title>
    <link href="https://chenzk1.github.io/2019/03/14/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95#4/"/>
    <id>https://chenzk1.github.io/2019/03/14/自然辩证法#4/</id>
    <published>2019-03-14T01:54:35.299Z</published>
    <updated>2018-12-18T05:18:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="历史观点"><a href="#历史观点" class="headerlink" title="历史观点"></a>历史观点</h1><p>科学：本质。应用于自然事物。<br>技术：应用。人造事物。</p><h1 id="现代哲学的观点"><a href="#现代哲学的观点" class="headerlink" title="现代哲学的观点"></a>现代哲学的观点</h1><p>海德格尔</p><ul><li>物Physita：作为自身存在并持续存在的东西</li><li>物mathemata：可学的，可描述的，可传授的东西。</li></ul><p>Mathemata是使事物成为对象的测量方案，即技术，所以海德格尔不谈科学。</p><p>比较：</p><ul><li>阳明格竹</li><li>赫尔蒙特的柳树生长实验</li></ul><h1 id="科学的真正进步取决于技术化程度"><a href="#科学的真正进步取决于技术化程度" class="headerlink" title="科学的真正进步取决于技术化程度"></a>科学的真正进步取决于技术化程度</h1><p>科学的历史使用测量技术取代形而上学的过程</p><p>用几何量取代本性</p><p>即，测量技术使物成为可知可控可用的对象</p><p>可知可控可用是一回事。重视前者表现为科学，重视后者表现为技术。</p><p>所以科学和技术是同一回事：用测量方案和数学模型将对象描述为可控的。两者本无区别，区别在有人想利用这体现哲学目的（宇宙图景）还是经济目的（生产力）。</p><h1 id="认为科学和技术有本质区别的习惯从何而来"><a href="#认为科学和技术有本质区别的习惯从何而来" class="headerlink" title="认为科学和技术有本质区别的习惯从何而来"></a>认为科学和技术有本质区别的习惯从何而来</h1><p>西方，古希腊文化，中世纪神学，近代科学</p><p>开普勒追求的元物理学 metaphysics</p><ul><li>古代和中世纪：经验几何学实用技术</li><li>近代两百年的欧洲：元物理学，劳动生产率</li><li>19世纪以后：现代科技一体化</li></ul><p>结论：</p><ol><li>古代和中世纪，无科学和技术之分。在人类漫长历史中的一小段时间，西方一些地区，即在以牛顿力学为代表的经典科学体系出现之后，在欧洲的几个国家，学者们开始重视演绎性的理论和积累性的技术之间的某种分离状况，并一度将前者的地位放在后者之前。</li><li>近代欧洲的文化目标——宇宙的先在秩序。即上帝将数学放进了宇宙，无论人类是否研究宇宙都是简单和谐的。研究只是使人类理智知道这个和谐。这导致对所谓基础理论，纯科学的偏爱。<br>强调科学和技术的区别，只是把近代欧洲的科学理念当成了一般的东西。科学不是超越于具体文化差异的。</li><li>欧洲也并不迷信“纯科学”。</li><li>现代科技观<br>近代欧洲的理想追求虽然与工业的目标并不矛盾，但是在西方社会的步伐进入工业化之后，仅仅因为那样的理想不被急需，纯科学的意义模糊了。<br>二战期间，科学服务于军事使得科学与技术的区别无关紧要。战争这种重大的社会事件无需借助哲学说明就轻易地改变了近代欧洲的科学理念。<br>科学和技术咋新的用途中成为了同一个东西，这是因为他们本来就是同一种东西。</li><li>旧科技观的弊病<br>强调科学和技术的区别只是获得定义上的齐整，但不是一种有效的科学观。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;历史观点&quot;&gt;&lt;a href=&quot;#历史观点&quot; class=&quot;headerlink&quot; title=&quot;历史观点&quot;&gt;&lt;/a&gt;历史观点&lt;/h1&gt;&lt;p&gt;科学：本质。应用于自然事物。&lt;br&gt;技术：应用。人造事物。&lt;/p&gt;
&lt;h1 id=&quot;现代哲学的观点&quot;&gt;&lt;a href=&quot;#现
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="自然辩证法" scheme="https://chenzk1.github.io/tags/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>自然辩证法#3</title>
    <link href="https://chenzk1.github.io/2019/03/14/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95#3/"/>
    <id>https://chenzk1.github.io/2019/03/14/自然辩证法#3/</id>
    <published>2019-03-14T01:54:35.267Z</published>
    <updated>2018-12-04T07:13:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>科学观——科学是一种公共知识</p><h1 id="从另一个角度看近代科学为什么发生在欧洲"><a href="#从另一个角度看近代科学为什么发生在欧洲" class="headerlink" title="从另一个角度看近代科学为什么发生在欧洲"></a>从另一个角度看近代科学为什么发生在欧洲</h1><p>近代欧洲的社会变革和知识观</p><p>古代和中世纪：知识就是权力（权力就是知识）</p><p>资产阶级革命 新兴资产阶级和自由知识分子</p><p>知识就是力量</p><p>首先是对知识一词的含义的重建</p><p>离开了经院哲学，知识的起点可以有：<br>经验主义，实验主义，数学精神，科学理性</p><p>阿格里科拉和伽利略的共同点：效率，公共性，实证</p><p>知识世俗化、知识公共化是大工业、垄断资本和现代金融、现代市场、现代城市需要科学并成就的</p><h1 id="影响世界历史进程的书"><a href="#影响世界历史进程的书" class="headerlink" title="影响世界历史进程的书"></a>影响世界历史进程的书</h1><ul><li><p>1532 马基雅维利 君主论<br><strong>文艺复兴后的第一个里程碑</strong><br>政治是一种独立的活动，自身具有区别于道德与宗教的原则和规律。批判 “手段（政治）为目的（为政以德）服务”。权术学。伟大的骗子教程，君主需兼具狮子的凶残和狐狸的狡诈。目的为手段服务，可以不择手段。全套术治理论。</p><p>开启了一种新的学术规则——方法有限而非观点优先；以效率为合理性（说服力）的核心；若无这样的规则，任何观点都无法被证明为对的或错的；这与稍后的科学革命不谋而合，且科学革命发扬得更彻底</p></li><li>1543 哥白尼 天体运行论</li><li>1628 哈维 心血运动论</li><li>1687 牛顿 自然哲学的数学原理</li><li><p>1689 洛克 政府论<br>上篇批判君权神授和世袭制</p><p>无个性的基本单元：个人主义者的自然权利（生命、健康、自由、财产），人人平等，互相不得侵犯。</p><p>自然状态：人应当自由实践其自我利益，人人具有自然法执行权。但按照牛顿力学，此为布朗运动，需如日星辰城之有序。故人们为了避免冲突，维护公正，需要形成政治共同体，建立斌遵循政治秩序，但需给予政治秩序以理性解释。</p></li><li>1762 卢梭 社会契约论<br>自然状态，自然人（粒子）<br>公民形成文明社会，以众人之力，保护每个个体。<br>为何需要保护？反原罪说。</li><li>1832 克劳塞维兹 战争论<br>改革贵族文化中的旧军事体制</li><li>1859 达尔文 物种起源<br>进攻目的论，神创论的最后堡垒</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;科学观——科学是一种公共知识&lt;/p&gt;
&lt;h1 id=&quot;从另一个角度看近代科学为什么发生在欧洲&quot;&gt;&lt;a href=&quot;#从另一个角度看近代科学为什么发生在欧洲&quot; class=&quot;headerlink&quot; title=&quot;从另一个角度看近代科学为什么发生在欧洲&quot;&gt;&lt;/a&gt;从另一个角度看
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="自然辩证法" scheme="https://chenzk1.github.io/tags/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>自然辩证法#2</title>
    <link href="https://chenzk1.github.io/2019/03/14/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95#2/"/>
    <id>https://chenzk1.github.io/2019/03/14/自然辩证法#2/</id>
    <published>2019-03-14T01:54:35.252Z</published>
    <updated>2018-11-27T09:25:06.000Z</updated>
    
    <content type="html"><![CDATA[<p>现代自然观</p><h1 id="现代自然观的来源"><a href="#现代自然观的来源" class="headerlink" title="现代自然观的来源"></a>现代自然观的来源</h1><ul><li>起源于古希腊，兴于近代欧洲，完成于牛顿。</li><li>粒子论 机械论 还原论</li><li>时空观：时间、空间相互独立；空间均匀、平直、三维、各向同性；时间均匀、平直、一维、不可逆；时间、空间与物质各自独立。</li><li>物质观：物质只具有几何属性（形状、大小、位置）和机械属性（质量、弹性）。</li><li>从古希腊哲学到现代自然观<br>古希腊知识分子的民间性：<br>（非官方，正式由于其哲学本身上的纯粹性，才获得了很大的进步）<ul><li>世界是物质的（麦乐斯“水是万物的本源”）</li><li>物质是质料的（原子论者，亚里士多德etc）</li><li>质料是形式的（毕达哥拉斯学派，希罗多德历史）</li><li>形式是几何的（欧几里得，阿基米德，天球）</li><li>运动是机械的（哥白尼，开普勒，伽利略，牛顿）<br>中世纪奔溃，资产阶级重新选择文化的时候，选择了古希腊。</li></ul></li></ul><h1 id="科学进步与自然观的更新"><a href="#科学进步与自然观的更新" class="headerlink" title="科学进步与自然观的更新"></a>科学进步与自然观的更新</h1><ul><li>牛顿力学自然观本身就是一次科学大进步的产物</li><li>现代科学对机械论自然观的几次有限突破，都伴随着科学的重大突破</li><li>相对论自然观</li><li>量子力学自然观</li><li>复杂系统自然观</li><li><strong>但语境还是机械论，自然观的反思和革命还将继续。</strong></li></ul><h1 id="人与自然关系的反思"><a href="#人与自然关系的反思" class="headerlink" title="人与自然关系的反思"></a>人与自然关系的反思</h1><ol><li>悲观主义</li><li>乐观主义</li><li>现实主义</li></ol><p>三个流派的共同出发点：</p><ol><li>人与自然的关系恶化</li><li>恶化的本质：现代人类对自然界的影响超过了自然界对人类活动的承载力。</li></ol><h2 id="悲观主义"><a href="#悲观主义" class="headerlink" title="悲观主义"></a>悲观主义</h2><p>代表：罗马俱乐部</p><h2 id="乐观主义"><a href="#乐观主义" class="headerlink" title="乐观主义"></a>乐观主义</h2><p>《今后二百年——美国和世界的一幅远景》</p><ul><li>在过去到未来的前后各二百年中，人类将从困境中摆脱出来，自然因素并不构成限制，科学技术的发展将会是自然环境和社会环境充满活力。</li><li>自然资源供应无限，人类的资源短缺等问题可以通过技术进步来解决。</li><li>当穷国富起来时，人口增长就会自动停止。</li></ul><p>各有缺点：</p><ul><li>悲观论者：线性外推，但忽略现有的社会生产方式、社会制度、社会意识形态都是可以改变的。</li><li>乐观论者：以自然资源潜力无限和科学技术能力无限为前提，但忽略了自然资源在理论上的潜在存在和…</li></ul><h2 id="现实主义"><a href="#现实主义" class="headerlink" title="现实主义"></a>现实主义</h2><p>《纵观世界全局》（美国世界观察研究所）</p><p>世界明天的好坏不是命运决定的，也不是科学技术的本性决定的，它取决于人类今后20年左右做出的决策是否明智。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;现代自然观&lt;/p&gt;
&lt;h1 id=&quot;现代自然观的来源&quot;&gt;&lt;a href=&quot;#现代自然观的来源&quot; class=&quot;headerlink&quot; title=&quot;现代自然观的来源&quot;&gt;&lt;/a&gt;现代自然观的来源&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;起源于古希腊，兴于近代欧洲，完成于牛顿。&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="自然辩证法" scheme="https://chenzk1.github.io/tags/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>自然辩证法#1</title>
    <link href="https://chenzk1.github.io/2019/03/14/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95#1/"/>
    <id>https://chenzk1.github.io/2019/03/14/自然辩证法#1/</id>
    <published>2019-03-14T01:54:35.252Z</published>
    <updated>2018-11-27T05:37:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>次数不定随堂测验 | 80</p><p>平时成绩(课堂汇报) | 20</p><h1 id="西方哲学认识论"><a href="#西方哲学认识论" class="headerlink" title="西方哲学认识论"></a>西方哲学认识论</h1><ul><li>内省型文化主客体关系</li><li>主体凸现和客体廓清<br>主体凸现：反思，内省<br>客体廓清：形式，语言，理论（模型、线索、规范、logos）<br>语言和语言产品成为主客体中介<br>从古希腊到近代欧洲，逻辑学、几何学、力学成为主客体中介</li></ul><h1 id="经典科学与哲学"><a href="#经典科学与哲学" class="headerlink" title="经典科学与哲学"></a>经典科学与哲学</h1><p>哲学是关于反思的<br>人如何获知真理：经验论与唯理论<br>经验论：洛克、休谟、贝克莱<br>孔洞生成定律<br>唯理论：笛卡尔、莱布尼兹、斯宾诺莎<br>抽象思维，形式化概念体系<br>恐惧：对恶的期待</p><h1 id="（空）"><a href="#（空）" class="headerlink" title="（空）"></a>（空）</h1><h1 id="现代科学哲学"><a href="#现代科学哲学" class="headerlink" title="现代科学哲学"></a>现代科学哲学</h1><h1 id="当代科学技术哲学与自然辩证法"><a href="#当代科学技术哲学与自然辩证法" class="headerlink" title="当代科学技术哲学与自然辩证法"></a>当代科学技术哲学与自然辩证法</h1><p>《反杜林论》和《自然辩证法手稿》</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;次数不定随堂测验 | 80&lt;/p&gt;
&lt;p&gt;平时成绩(课堂汇报) | 20&lt;/p&gt;
&lt;h1 id=&quot;西方哲学认识论&quot;&gt;&lt;a href=&quot;#西方哲学认识论&quot; class=&quot;headerlink&quot; title=&quot;西方哲学认识论&quot;&gt;&lt;/a&gt;西方哲学认识论&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="自然辩证法" scheme="https://chenzk1.github.io/tags/%E8%87%AA%E7%84%B6%E8%BE%A9%E8%AF%81%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>中国古代典籍文化#1</title>
    <link href="https://chenzk1.github.io/2019/03/14/%E4%B8%AD%E5%9B%BD%E5%8F%A4%E4%BB%A3%E5%85%B8%E7%B1%8D%E6%96%87%E5%8C%96#1/"/>
    <id>https://chenzk1.github.io/2019/03/14/中国古代典籍文化#1/</id>
    <published>2019-03-14T01:54:35.252Z</published>
    <updated>2019-02-27T13:31:46.000Z</updated>
    
    <content type="html"><![CDATA[<p><em>考核：第四五周布置论文题目，第八周交论文</em></p><h1 id="古代典籍的载体和装帧"><a href="#古代典籍的载体和装帧" class="headerlink" title="古代典籍的载体和装帧"></a>古代典籍的载体和装帧</h1><h2 id="甲骨"><a href="#甲骨" class="headerlink" title="甲骨"></a>甲骨</h2><ul><li>龟甲和兽骨的合称<ul><li>甲：龟甲的腹板</li><li>骨：牛的肩胛骨和胫骨，也有猪羊的肩胛骨</li></ul></li></ul><h2 id="金"><a href="#金" class="headerlink" title="金"></a>金</h2><ul><li>铜，指青铜器（铜和锡的合金，呈青灰色）上的铭文。</li><li>金文铸刻的器具以钟鼎居多，又称钟鼎文</li><li>西周是钟鼎文最盛行的时代</li></ul><h2 id="石"><a href="#石" class="headerlink" title="石"></a>石</h2><h2 id="木牍"><a href="#木牍" class="headerlink" title="木牍"></a>木牍</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;em&gt;考核：第四五周布置论文题目，第八周交论文&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&quot;古代典籍的载体和装帧&quot;&gt;&lt;a href=&quot;#古代典籍的载体和装帧&quot; class=&quot;headerlink&quot; title=&quot;古代典籍的载体和装帧&quot;&gt;&lt;/a&gt;古代典籍的载体和装帧&lt;/h1&gt;&lt;h2 
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="中国古代典籍文化" scheme="https://chenzk1.github.io/tags/%E4%B8%AD%E5%9B%BD%E5%8F%A4%E4%BB%A3%E5%85%B8%E7%B1%8D%E6%96%87%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>ROC AUC, Accuracy, Recall, F1</title>
    <link href="https://chenzk1.github.io/2019/03/14/ROC%20%20AUC/"/>
    <id>https://chenzk1.github.io/2019/03/14/ROC  AUC/</id>
    <published>2019-03-14T01:54:35.236Z</published>
    <updated>2019-03-16T02:58:16.880Z</updated>
    
    <content type="html"><![CDATA[<p>类不平衡（class imbalance）现象：即负样本比正样本多很多（或者相反）</p><ol><li>ACCURACY/RECALL/F1</li></ol><ul><li><p>Accuracy：TP/(TP+TN) 即预测对的数据个数与总数据个数的比</p><p>  <strong><em>A和R的问题在于如果样本不平衡，则参考意义很小</em></strong></p></li><li><p>准确率（Precision）：P=TP/(TP+FP)。通俗地讲，就是预测正确的正例数据占预测为正例数据的比例。</p><p>  <strong>针对判别结果 查准率</strong></p></li><li><p>召回率（Recall）：R=TP/(TP+FN)。通俗地讲，就是预测为正例的数据占实际为正例数据的比例。</p><p>  <strong>针对样本 查全率</strong></p></li><li>F1 = 2*P*R/(P+R)：既有P又有R</li><li><p>PRC， precision recall curve，与下面的ROC一样，先看是否光滑，光滑的话说明过拟合不大。越往右上越好。</p><p>  <strong><em>A和R的问题在于如果样本不平衡，则参考意义很小</em></strong></p></li></ul><ol start="2"><li>ROC AUC（receiver operating characteristic curve）</li></ol><p>TPR=TP/(TP+FN)=TP/actual positives 也就是Recall<br>FPR=FP/(FP+TN)=FP/actual negatives<br>ROC是由点（TPR,FPR）组成的曲线，AUC就是ROC的面积。AUC越大越好。</p><ul><li><p>画法：ROC曲线其实是多个混淆矩阵的结果组合，如果在上述模型中我们没有定好阈值，而是将模型预测结果从高到低排序，将每个概率值依次作为阈值，那么就有多个混淆矩阵。对于每个混淆矩阵，我们计算两个指标TPR（True positive rate）和FPR（False positive rate），TPR=TP/(TP+FN)=Recall，TPR就是召回率。FPR=FP/(FP+TN)，FPR即为实际为好人的人中，预测为坏人的人占比。我们以FPR为x轴，TPR为y轴画图，就得到了ROC曲线。</p><p><strong><em>也就是说，ROC曲线的阈值不是多次运行模型得到的，是同一个模型中通过对所得结果按照概率的排序得到了一个threshold</em></strong></p></li><li>原理：<ul><li>在画图描点过程中，每取一个样本，会以此样本被预测为1的概率作为阈值，概率排序在此之上的样本认为是1，之下的样本会被认为是0（解释了ROC为何与样本预测值的排序有关）。</li><li>AUC的值代表了：取真实label为1和为0的两个样本，其中真样本被预测为1的概率大于假样本被预测为1的概率这一事件的概率。假设有M个真样本，N个假样本，M个真样本的预测概率升序排序，其值从大到小分别为rank_1…rank_m,例如6个真4个假，假设真样本概率最高的排序为10，则比它低的假样本有10-6=4个，下一个排序为8，则比它低的假样本有8-(6-1)=3个，此概率为：（rank_1 - m + rank_2 - (m-1) + rank_m - 1）/ M*N</li><li>AUC=0.5时，任意一个样本被判断为真和判断为假的概率相等</li><li>e.g 当threshold取为0.5时：<br><a href="https://cdn-images-1.medium.com/max/1600/1*Uu-t4pOotRQFoyrfqEvIEg.png" target="_blank" rel="noopener">!ex0</a><br>此时AUC=1：<br><a href="https://cdn-images-1.medium.com/max/800/1*HmVIhSKznoW8tFsCLeQjRw.png" target="_blank" rel="noopener">!ex00</a><br>AUC=0.7:<br><a href="https://cdn-images-1.medium.com/max/1600/1*yF8hvKR9eNfqqej2JnVKzg.png" target="_blank" rel="noopener">!</a><br><a href="https://cdn-images-1.medium.com/max/800/1*-tPXUvvNIZDbqXP0qqYNuQ.png" target="_blank" rel="noopener">!</a><br>AUC=0.5:<br><a href="https://cdn-images-1.medium.com/max/1600/1*iLW_BrJZRI0UZSflfMrmZQ.png" target="_blank" rel="noopener">!</a><br><a href="https://cdn-images-1.medium.com/max/800/1*k_MPO2Q9bLNH9k4Wlk6v_g.png" target="_blank" rel="noopener">!</a><br>AUC=0:<br><a href="https://cdn-images-1.medium.com/max/1600/1*aUZ7H-Lw74KSucoLlj1pgw.png" target="_blank" rel="noopener">!</a><br><a href="https://cdn-images-1.medium.com/max/800/1*H7JGQbaa06BUab6tvGNZKg.png" target="_blank" rel="noopener">!</a></li></ul></li><li>一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting。</li><li><p>越往左上越好。</p><p>  所以使用ROC的话，它会先对预测到的结果进行排序，然后再根据排序的结果画图，所以他的曲线形状不会因为数据不平衡而发生大的改变。<br>  <strong>但是当数据极度不平衡时，ROC仍然有问题，下面的PRC表现更好。</strong></p></li></ul><p><a href="https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5" target="_blank" rel="noopener">参考1</a><br><a href="https://www.zhihu.com/question/39840928" target="_blank" rel="noopener">参考2</a></p><ol><li>PRC， precision recall curve，与上面的ROC一样，先看是否光滑，光滑的话说明过拟合不大。越往右上越好。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;类不平衡（class imbalance）现象：即负样本比正样本多很多（或者相反）&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;ACCURACY/RECALL/F1&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Accuracy：TP/(TP+TN) 即预测对的数据个数与总数据个数的比&lt;/p
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="ML" scheme="https://chenzk1.github.io/tags/ML/"/>
    
      <category term="Metrics" scheme="https://chenzk1.github.io/tags/Metrics/"/>
    
  </entry>
  
  <entry>
    <title>WNSP#2</title>
    <link href="https://chenzk1.github.io/2019/03/14/WNSP#2/"/>
    <id>https://chenzk1.github.io/2019/03/14/WNSP#2/</id>
    <published>2019-03-14T01:54:35.236Z</published>
    <updated>2018-09-29T05:17:10.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>50GHz<ul><li>丰富的频谱资源，即频带宽</li><li>速度快</li><li>波长短，更适合短距传输。穿透性差。</li><li>高度方向性</li><li>安全性</li></ul></li><li>Li-Fi(Light Fidelity)<ul><li>安全性和低穿透性：灯光具有可遮挡性</li><li>频谱资源丰富</li><li>无电磁辐射和电磁干扰</li></ul></li><li>LP-WAN<ul><li>NB-IoT</li><li>数据安全及隐私：公网传输导致数据泄露</li><li>通信安全：通信网络面临攻击</li><li>设备安全</li></ul></li></ul><h1 id="密码学"><a href="#密码学" class="headerlink" title="密码学"></a>密码学</h1><ul><li>唯密文、唯明文、选择性密文、选择性明文</li><li>保密性 完整性 可用性 真实性 可追溯性 不可抵赖性 匿名性</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;50GHz&lt;ul&gt;
&lt;li&gt;丰富的频谱资源，即频带宽&lt;/li&gt;
&lt;li&gt;速度快&lt;/li&gt;
&lt;li&gt;波长短，更适合短距传输。穿透性差。&lt;/li&gt;
&lt;li&gt;高度方向性&lt;/li&gt;
&lt;li&gt;安全性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Li-Fi(Light Fid
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>python面向对象</title>
    <link href="https://chenzk1.github.io/2019/03/14/python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"/>
    <id>https://chenzk1.github.io/2019/03/14/python面向对象/</id>
    <published>2019-03-14T01:54:35.221Z</published>
    <updated>2018-11-27T05:26:20.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="属性命名"><a href="#属性命名" class="headerlink" title="属性命名"></a>属性命名</h3><ul><li>属性以双下划线开头，类内变量，实例无法访问。但可以通过某些方式访问，例如Student例中定义了__name变量，可以用_Student_name来实现访问，但不建议，因为不同的解释器的转化方式不一样。</li><li>单下划线可以打开，但需要注意不能随意更改。</li><li>双下划线结尾与开头，特殊变量，类内可以访问，实例不知。</li><li><h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3>开闭原则：定义一个类Animal及其多个之类Dog/Cat/…，当定义一个函数或操作时：</li></ul><ul><li>对扩展开放：允许新增Animal的子类；</li><li>对修改封闭：不需要修改依赖Animal类型的run_twice()等函数，仍然可以传入Dog/Cat等类。<br>事实上，不需要继承也可以实现多态————鸭子类型。</li></ul><h3 id="若干方法"><a href="#若干方法" class="headerlink" title="若干方法"></a>若干方法</h3><ul><li>isinstance(object,class) 判断是否属于某个类</li><li>dir() 列举出一个对象的属性和方法</li><li>getattr()、setattr()、hasattr()可以获得、添加、查询是否需要某个属性<ul><li>__slots__ 限制可以添加的属性，__slots__ = (‘name’, ‘age’) # 用tuple定义允许绑定的属性名称</li></ul></li><li>装饰器</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;属性命名&quot;&gt;&lt;a href=&quot;#属性命名&quot; class=&quot;headerlink&quot; title=&quot;属性命名&quot;&gt;&lt;/a&gt;属性命名&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;属性以双下划线开头，类内变量，实例无法访问。但可以通过某些方式访问，例如Student例中定义了__name变量
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="python" scheme="https://chenzk1.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Presentation of WNSP and IS</title>
    <link href="https://chenzk1.github.io/2019/03/14/Presentation%20of%20WNSP%20and%20IS/"/>
    <id>https://chenzk1.github.io/2019/03/14/Presentation of WNSP and IS/</id>
    <published>2019-03-14T01:54:35.221Z</published>
    <updated>2018-10-04T14:08:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><ul><li>Behavioral biomertics is the study of individual patterns(hand-writing, typing, mouse movements).</li><li>最早：二战中的电报员keying pattern</li><li>password hardening(secondary authentication)、password-less logins<ul><li>Banks use typing information as an additional layer of security.</li><li>Google is developing methods to authenticate users on mobile devices without passwords.</li></ul></li><li>human chosen passwords are far from safe -&gt; additional authentication -&gt; explicit methods are usually disruptive to the user -&gt; use behavioral biometrics</li></ul><h1 id="goal-design-a-adversarial-algorithms"><a href="#goal-design-a-adversarial-algorithms" class="headerlink" title="goal: design a adversarial algorithms"></a>goal: design a adversarial algorithms</h1><ul><li>前提：the attacker has access to the user’s password, but needs to overcome a keystroke dynamics based authentication layer</li></ul><h1 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h1><h2 id="behavioral-biometrics"><a href="#behavioral-biometrics" class="headerlink" title="behavioral biometrics"></a>behavioral biometrics</h2><blockquote><p>hand-writing, typing, mouse movements, touchscreen swipes, gait analysis</p></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><h3 id="Protocols"><a href="#Protocols" class="headerlink" title="Protocols"></a>Protocols</h3><p><em>for collecting new data, and selecting new samples for training and testing</em></p><ol><li><p>collectign MTurk dataset</p><ul><li>pre-processing: drop any malformed samples(due to a combination of reasons that include: different behavior of browsers, differences in internet speed, or other noise as the subjects took the study simultaneously)</li><li>describe the protocol for selecting samples for training and testing, and creating adversarial samples across all datasets.</li><li></li></ul><p>生物行为学有两种分类器：一类分类器和两类分类器。前者只用正确样本，后者还会用到假样本。一般用一类分类器：1. because it is very impractical to expect negative samples for an arbitrary password.2. both the two class classifiers, and one class classifiers appear to give similar EER scores</p></li><li><p>Genuine User Samples真实样本<br>use the first half of the samples for training, the second half of the samples for testing</p></li><li>imposter training samples虚假训练样本<br>随机选择，与真实样本同数量</li><li>imposter testing samples虚假测试样本<br>DSN: first four samples of every user besides the genuine user<br>MYurk and touchscreen swipes dataset: randomly sampled the same number of impostor samples as the genuine user’s test samples</li><li>adversary对抗样本<br>The <strong>Targeted K-means++ adversary</strong> used all the samples from the data set excluding the ones from the target user and the ones used for training and testing the user’s classifier. For <strong>the Indiscriminate K-means++ </strong>adversary, we conducted a new MTurk study, as described before, a few months after the original study. We used all the samples from this new study. In Algorithm 2, we set the parameter “SAMPLE-SIZE” to 20000.</li></ol><h3 id="Detection-Algorithms-检测算法"><a href="#Detection-Algorithms-检测算法" class="headerlink" title="Detection Algorithms 检测算法"></a>Detection Algorithms 检测算法</h3><h4 id="One-class-classifiers"><a href="#One-class-classifiers" class="headerlink" title="One class classifiers"></a>One class classifiers</h4><ul><li>Manhattan distance<br>$$ \sum_{i=1}^m\frac{\left|{x_i-y_i}\right|}m $$</li><li>Gaussian高斯<br>training samples are modeled as a Gaussian distribution based on their mean and standard deviation</li><li><p>Gaussian mixture高斯混合模型</p><blockquote><p><a href="https://blog.csdn.net/jinping_shi/article/details/59613054" target="_blank" rel="noopener">https://blog.csdn.net/jinping_shi/article/details/59613054</a> </p></blockquote><blockquote><p>高斯混合模型（Gaussian Mixed Model）指的是多个高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布的情况（或者是同一类分布但参数不一样，或者是不同类型的分布，比如正态分布和伯努利分布）。<br>$$ \sum_{k=1}^Kπ_kN\left(x|μ_k,\sum_k\right) $$<br>$$ \sum_{k=1}^Kπ_k=1 $$<br>$$ 0 ≤ Kπ_k ≤ 1 $$<br>即$π_k$相当于每个分量的权重<br>GMM常用于聚类。如果要从 GMM 的分布中随机地取一个点的话，实际上可以分为两步：首先随机地在这 K 个 Component 之中选一个，每个 Component 被选中的概率实际上就是它的系数πkπk \pi_k ，选中 Component 之后，再单独地考虑从这个 Component 的分布中选取一个点就可以了──这里已经回到了普通的 Gaussian 分布，转化为已知的问题。将GMM用于聚类时，假设数据服从混合高斯分布（Mixture Gaussian Distribution），那么只要根据数据推出 GMM 的概率分布来就可以了；然后 GMM 的 K 个 Component 实际上对应KKK个 cluster 。根据数据来推算概率密度通常被称作 density estimation 。</p></blockquote></li><li>one class SVM<br>used the Support Vector Machine(SVM) implementation in sklearn, with radial basis function (RBF) kernel, and kernel parameter 0.9.</li><li>Autoencoder and Contractive Autoencoder自动编码和收缩性自动编码<br>With the advent of deep learning, researchers have started using variants of neural networks in the domain of cybersecurity. One of the key structures used in the past are autoencoders and contractive autoencoders<br>随着深度学习的到来，研究人员开始在网络安全领域使用神经网络的变体。过去使用的关键结构之一是自动编码器和压缩自动编码器</li></ul><h4 id="Two-class-classifiers"><a href="#Two-class-classifiers" class="headerlink" title="Two class classifiers"></a>Two class classifiers</h4><ul><li>Random Forest<br>used a model similar to the one described by Antal et al [4]. Random Forests with 100 trees was their best-performing classifier on the touchscreen swipes dataset. We used the Random Forest implementation in sklearn<br>我们使用了一个类似于Antal et al[4]所描述的模型。随机森林与100棵树是他们在触摸屏滑动数据集上表现最好的分类器。我们在sklearn中使用了Random Forest实现</li><li>Nearest Neighbor<br>Here we classify a test sample based on the majority label among a fixed number of its nearest neighbors in the training set. The neighbours are determined using Euclidean distance. We used the implementation in [32]<br>在测试样本中用最近邻</li><li>Fully Connected Neural Net全连接NN<br>We experimented with multiple variants of multi layer perceptron by using different hyper parameters. The network that performed the best had two hidden layers with 15 neurons each computing scores for genuine and impostor classes. There was no significant improvement in the performance of the network by increasing the number of layers or neurons per layer in the architecture of the neural network.<br>我们使用不同的超参数对多层感知器的多个变体进行了实验。表现最好的网络有两个隐藏层，每个层有15个神经元，计算真实和冒名顶替类的分数。在神经网络体系结构中，每层增加层数或神经元数量，网络性能没有显著改善。</li></ul><h4 id="Monaco’s-Normalization-Technique-标准化"><a href="#Monaco’s-Normalization-Technique-标准化" class="headerlink" title="Monaco’s Normalization Technique 标准化"></a>Monaco’s Normalization Technique 标准化</h4><p>The <strong>key insight of this technique</strong> was that a user’s classifier could normalize future input samples based only on the genuine user’s data given to it at the start. Essentially, this acts like a filtering step - and features that are too far from the mean of the genuine user’s fitting data get filtered out.<br>后续样本基于刚开始给定的真实输入样本来做标准化<br>这个标准化很重要，没这个就无法得出结果we do not even mention our results without this<br>normalization.</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Equal-error-rate"><a href="#Equal-error-rate" class="headerlink" title="Equal error rate"></a>Equal error rate</h3><table><thead><tr><th>Name of Classifier</th><th>DSN EER</th><th>MTurk EER</th></tr></thead><tbody><tr><td>Manhattan</td><td>0.091</td><td>0.097</td></tr><tr><td>SVM</td><td>0.087</td><td>0.097</td></tr><tr><td>Gaussian</td><td>0.121</td><td>0.109</td></tr><tr><td>Gaussian Mixture</td><td>0.137</td><td>0.135</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table><hr><p>(具体见表2)</p><p>注：没有标准化的EER都在0.15左右</p><h3 id="Keystroke-Results"><a href="#Keystroke-Results" class="headerlink" title="Keystroke Results"></a>Keystroke Results</h3><p>In this section we discuss the results of testing our adversaries on the DSN and MTurk datasets, which are summarized in Tables III, IV. We conducted the tests independently on each of the five passwords in the MTurk dataset, but for a more compact presentation, we average the results of all passwords. A few interesting highlights based on these results are given below<br>在本节中,我们讨论的结果,测试DSN和MTurk数据集,总结在表III、IV。我们进行独立测试在MTurk数据集中的密码。为了更紧凑的表示,我们把所有的结果平均之后显示出来。</p><h4 id="MasterKey-VS-K-means"><a href="#MasterKey-VS-K-means" class="headerlink" title="MasterKey VS K-means++"></a>MasterKey VS K-means++</h4><p>K-means++ performs better than MasterKey.<br>Figure 2 展示了最好的一类分类器和二类分类器下，Target K-means++和Indiscriminate K-means++以及MasterKey的性能对比<br>Targeted K-means++ seems to essentially <strong>be able<br>to compromise the security of all the users</strong> in the limit.</p><p>Table3展示了K-means++强于asterKey</p><p>本文中用到的样本量更大，选择train sample和test sample的protocol也不一样，但是EER与原文差不多。如图5所示。</p><p>As can be seen by Table V, and Figure 4, the results on this dataset show the same trends as seen in the keystroke dynamics datasets before. The first try which hits the mean of the impostor samples is not very successful here. This is particularly bad for an adversary like MasterKey which stays around the mean of the distribution, and is reflected in the results in Table V. But the K-means++ adversary is quickly able to explore the sample space to find more challenging queries and in 10 tries itself, breaks into a sizeable proportion of the  classifiers as in the keystrokes dataset. And in the limit, essentially all the user’s classifiers are compromised.由表V和图4可以看出，该数据集上的结果显示了与之前击键动力学数据集相同的趋势。第一次尝试就击中了冒名顶替样本的均值，但并不是很成功。这是特别糟糕的敌人像万能钥匙保持周围分布的均值,并反映在结果表诉。但k - means + +对手很快就能够探索样本空间中找到更有挑战性的查询和10次尝试本身,闯进了一相当大的比例的数据集分类器的按键。在极限情况下，基本上所有用户的分类器都被破坏了。</p><h1 id="Conclusion-and-future-work"><a href="#Conclusion-and-future-work" class="headerlink" title="Conclusion and future work"></a>Conclusion and future work</h1><p>Behavioral biometrics is a promising field of research, but it is not a reliable solution for authentication in its current state. <strong>行为生物识别技术是一个很有前途的研究领域，但在目前的状态下，它并不是一个可靠的认证解决方案。</strong>We proposed two adversarial agents that require a different amount of effort from the adversary. <strong>Both attack methods performed clearly better than the previously studied attack methods</strong> in the literature and show that current state of the art classifiers add little protection against such adversaries. In the case of Indiscriminate K-means++, more than its success rate, it is worrying for the keystroke dynamics systems that such an adversary could conduct its attack without any additional cost incurred to collect samples. Past research has focused much more on improving the classifiers against naive adversaries, but this work shows that a lot more research from the adversarialperspective is required before such authentication systems can be adopted in sensitive contexts.<br>The design of our K-means++ adversaries utilizes a <strong>common intuition about human behavior, which is that a person’s behavioral data belongs to a “cluster”, rather than being absolutely unique</strong>. Thus it is natural to expect such techniques to generalize to other types of behavioral data. The results on the touchscreen touchscreen swipes dataset also supports this claim.<br>我们提出了两种敌对代理人，它们需要不同于对手的努力。这两种攻击方法的性能明显优于文献中先前研究的攻击方法，表明当前的艺术分类器对这类敌人的保护很少。在不加区别的K-means++的情况下，对于击键动力学系统来说，这样的对手可以进行攻击而不需要额外的成本来收集样本，这比其成功率更令人担忧。过去的研究更多地关注于改进针对天真的对手的分类器，但这项工作表明，在这种身份验证系统可以在敏感的上下文中采用之前，需要从adversarialperspective的角度进行更多的研究。我们的k -means++敌人的设计利用了一种关于人类行为的共同直觉，即一个人的行为数据属于一个“集群”，而不是绝对独一无二的。因此，很自然地期望这些技术可以推广到其他类型的行为数据。触屏触摸屏上的结果也支持这一说法。<br>Of course, from a practical perspective, it is much harder to simulate an attack on a touchscreen based system, as opposed to a keystroke dynamics system, because of the diversity of the touchscreen features like pressure, finger size and so on. Unlike keystrokes - we can’t just write an easily automated script to carry out such an attack. This implies that a swipes based classifier is more secure for now. But given enough motivation, it is possible that methods could be devised to bypass such limitations. For instance, such attacks could be carried out by feeding false information to the android sensors, or in an extreme example, by building a robotic arm.<br>当然，从实际角度来看，由于触摸屏的压力、手指大小等特性的多样性，模拟攻击基于触摸屏的系统要比模拟击键动力学系统困难得多。与击键不同的是，我们不能仅仅编写一个易于自动化的脚本来执行这样的攻击。这意味着基于滑动的分类器现在更安全。但只要有足够的动力，就有可能设计出绕过这些限制的方法。例如，这种攻击可以通过向android传感器提供虚假信息来实施，或者在一个极端的例子中，通过制造机械手臂来实施。<br>Previous research has relied exclusively on the average Equal Error Rate scores across all subjects to measure the robustness of classifiers. To develop more robust behavioral biometric classifiers, it would be useful to <strong>benchmark against the adversarial agents proposed in this paper instead.</strong> For instance, one class classifiers have been the dominant method researched in the keystroke dynamics literature as they perform as well as the two class classifiers in terms of EER, while the two class classifiers are not practical because one can not expect impostor samples for arbitrary passwords. Yet, against both the adversarial algorithms, the two class classifiers performed clearly better than the one class classifiers. This suggests that a future direction of research would be to bridge the gap between the idealized and practical versions of such two class classifiers as explained in section IV A.<br>以往的研究完全依赖于所有科目的平均等错误率分数来衡量分类器的鲁棒性。为了开发出更健壮的行为生物特征分类器，我们将对本文提出的抗辩剂进行基准测试。例如，在击键力学文献中，一类分类器是主要的研究方法，因为它们的性能和EER的两个类分类器一样好，而这两个类分类器是不实用的，因为人们不能指望冒名顶替者样本来处理任意的密码。然而，与两种对抗性算法相比，这两个类分类器的性能明显优于一个类分类器。这表明，今后的研究方向将是弥补第四节a所解释的这两类分类器的理想化版本和实际版本之间的差距。<br>From the adversarial perspective, one possibility for future work would be to extend these methods to free text based classifiers. Free text classifiers utilize a continuous stream of input text, as opposed to fixed text passwords, in order to classify keystroke patterns. This leads to differences in the features and algorithms that are utilized for these classifiers. But conceptually, the Indiscriminate K-means++ adversary should be well suited to generate adversarial samples against free text classifiers as well.从敌对的角度来看，未来工作的一种可能是将这些方法扩展到基于自由文本的分类器。自由文本分类器使用连续的输入文本流(与固定文本密码相反)来分类击键模式。这导致了这些分类器所使用的特性和算法的差异。但从概念上讲，不加区分的K-means++对手也应该非常适合针对自由文本分类器生成对抗性样本。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;动机&quot;&gt;&lt;a href=&quot;#动机&quot; class=&quot;headerlink&quot; title=&quot;动机&quot;&gt;&lt;/a&gt;动机&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;Behavioral biomertics is the study of individual patterns(hand-w
      
    
    </summary>
    
      <category term="Learning" scheme="https://chenzk1.github.io/categories/Learning/"/>
    
    
      <category term="WNSP" scheme="https://chenzk1.github.io/tags/WNSP/"/>
    
  </entry>
  
</feed>
