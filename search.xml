<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>稀疏编码</title>
      <link href="/2019/11/19/%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81/"/>
      <url>/2019/11/19/%E7%A8%80%E7%96%8F%E7%BC%96%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<ol><li><p>稀疏表示</p><ul><li><p>稀疏：p的维度较大，且尽可能使α中的系数多为0（只有少数的non-zero elements）</p></li><li><p>作用：</p><p>1）拥有更强大表达能力(Representation Power）（去掉冗余的信息）</p><p>2）拥有识别和约束能力（Discriminative, or Regularization Power）（加速运算，减小存储（可以通过一些表达方式来表达稀疏矩阵））</p></li><li><p>用途：</p><p>1）信号处理领域：自然界中的信号低频居多，高频基本都是噪声。因此在做基矩阵时，表达系数只在少数低频基上较大，高频基的系数基本都接近于0.所以在一些问题，例如逆问题中，从一些损坏或者噪声中提取某个信号，相当于解不定方程，不加约束的话会有很多可行解。于是在解决这类问题的时候，会加上约束，即稀疏性约束</p><p>2）推荐系统：用户行为，例如用户评价是一个低秩矩阵</p><p>3）深度学习：l1正则抗拟合</p></li><li><p>表示：如下图，原来的特征为x，编码后为α。相当于某向量在基下的坐标。</p></li></ul><p><img src="https://img-blog.csdn.net/20151223214504471" alt></p></li></ol><ul><li><p>实现：</p><p>使得表示前后误差尽可能小：<br>$$<br>J(D, α)=|D α-x|<em>{2}<br>$$<br>加入限制条件——稀疏：<br>$$<br>J(D, α)=|D α-x|</em>{2}+\lambda|α|_{1}<br>$$</p></li><li><p>理解：</p><p>1）有点像l1正则化</p><p>2）降维是原向量空间的子集，稀疏是原向量空间子集的集合</p></li></ul><ol start="2"><li><p>非NN中特征稀疏的解决</p><p>1）推荐系统FM模型解决</p><p>2）</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度下降及其优化算法</title>
      <link href="/2019/11/19/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/"/>
      <url>/2019/11/19/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8F%8A%E5%85%B6%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="GD及其变体"><a href="#GD及其变体" class="headerlink" title="GD及其变体"></a>GD及其变体</h1><h2 id="Batch-GD-Vanilla-GD"><a href="#Batch-GD-Vanilla-GD" class="headerlink" title="Batch GD / Vanilla GD"></a>Batch GD / Vanilla GD</h2><p><img src="https://www.zhihu.com/equation?tex=%5Ctheta_%7Bi%2B1%7D%3D+%5Ctheta_t+-+%5Ceta+g_t" alt></p><p>ALL SAMPLES</p><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>求梯度的时候，要经过所有样本，计算量会很大，随机梯度下降是<strong>每次迭代随机选取</strong>一个样本计算梯度。</p><p>每次用到的梯度方差大，收敛过程不稳定</p><p>问题是可能到不了局部最优。</p><h2 id="mini-batch-gd"><a href="#mini-batch-gd" class="headerlink" title="mini-batch gd"></a>mini-batch gd</h2><p>降低了更新参数的方差，收敛过程更稳定</p><h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><h3 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h3><p>$$<br>v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}<br>$$<br>$$<br>\begin{array}{l}{v_{0}=0} \ {v_{1}=\beta v_{0}+(1-\beta) \theta_{1}} \ {v_{2}=\beta v_{1}+(1-\beta) \theta_{2}=\beta\left(\beta v_{0}+\theta_{1}\right)+(1-\beta) \theta_{2}} \ {\vdots} \ {v_{t}=\beta v_{t-1}+(1-\beta) \theta_{t}=\sum_{i=1}^{t} \beta^{t-i}(1-\beta) \theta_{t}}\end{array}<br>$$</p><ul><li>使用之前的观测值以及当前观测值的平均来作为所需要的值，以达到平滑的目的。距离当前时刻越近的观测值对求得移动平均值的影响越大。</li><li>指数的意思是：按照公式归纳，之前观测值的权值为设定常数的次方。</li></ul><h3 id="动量梯度下降"><a href="#动量梯度下降" class="headerlink" title="动量梯度下降"></a>动量梯度下降</h3><p>$$<br>\begin{aligned} v_{t} &amp;=\beta v_{t-1}+(1-\beta) \nabla J(\theta)<em>{t} \ \theta &amp;=\theta-\alpha v</em>{t} \end{aligned}<br>$$<br>也即：<br>$$<br>\begin{aligned} v_{t} &amp;=\gamma v_{t-1}+\eta \nabla J(\theta)<em>{t} \ \theta &amp;=\theta-v</em>{t} \end{aligned}<br>$$</p><ul><li>动量梯度下降即对梯度进行指数加权平均，以达到平滑的作用，利用到了历史梯度值。</li><li>平滑：例如对于有些参数梯度正负不断震荡，导致loss也在这些参数对应的方向上不断震荡，那加入历史值则会在一定程度上抵消震荡；而如果某些参数的变化是一致的，平均后不会产生太大影响，只是使得具体值变化变小了而已。</li></ul><h2 id="Nesterov-Accelerated-Gradient-NAG"><a href="#Nesterov-Accelerated-Gradient-NAG" class="headerlink" title="Nesterov Accelerated Gradient,NAG"></a>Nesterov Accelerated Gradient,NAG</h2><ul><li>之前的方法是利用梯度方向来进行参数更新，Nesterov对梯度加入了预测功能：即预测参数未来的近似位置，并利用此位置处的参数梯度来进行更新当前参数</li></ul><p>$$<br>\begin{aligned} v_{t} &amp;=\gamma v_{t-1}+\eta \nabla J\left(\theta-\gamma v_{t-1}\right) \ \theta &amp;=\theta-v_{t} \end{aligned}<br>$$<br><img src="https://img2018.cnblogs.com/blog/439761/201903/439761-20190313101736137-755682989.jpg" alt></p><ul><li>动量下降为蓝色，短蓝为当前梯度，常蓝为动量；NAG为，先在之前的动量项（棕色）前进一步，并计算此时的梯度，然后用这个梯度与之前的动量项指数平均。</li><li>相当于先判断下一阶段可能要去的地方，然后使用那个地方的梯度，相当于提前预测，做个修正，少走冤枉路。</li></ul><h2 id="Adagrad-Adaptive-Gradient"><a href="#Adagrad-Adaptive-Gradient" class="headerlink" title="Adagrad(Adaptive Gradient)"></a>Adagrad(Adaptive Gradient)</h2><ul><li>之前的参数更新时，不同的参数使用相同的步长，为了对不同的参数使用不同的步长，引入Adagrad，当梯度大时，步长小，梯度小时，步长大。</li><li>AdaGrad对每个变量更新时，利用该变量历史积累的梯度来修正其学习速率。这样，已经下降的很多的变量则会有小的学习率，而下降较少的变量则仍然保持较大的学习率。<br>$$<br>\theta_{(t+1, i)}=\theta_{(t, i)}-\frac{\eta}{\sqrt{\sum_{\tau=1}^{t} \nabla J\left(\theta_{i}\right)<em>{\tau}+\epsilon}} \cdot \nabla J\left(\theta</em>{i}\right)_{t+1}<br>$$</li><li>缺点：<ul><li>仍然需要设初始学习率；</li><li>学习率不断衰减，到后期会很小，导致训练过早停止。</li></ul></li></ul><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>解决Adagrad学习率衰减的问题</p><ul><li>学习率衰减：引入动量（指数加权平均），引入超参γ，在累积梯度的<strong>平方</strong>项近似衰减：</li></ul><p>$$<br>\begin{array}{l}{s_{(t, i)}=\gamma s_{(t-1, i)}+(1-\gamma) \nabla J\left(\theta_{i}\right)<em>{t} \odot \nabla J\left(\theta</em>{i}\right)<em>{t}} \ {\theta</em>{(t, i)}=\theta_{(t-1, i)}-\frac{\eta}{\sqrt{s_{(t, i)}+\epsilon}} \odot \nabla J\left(\theta_{i}\right)_{t}}\end{array}<br>$$</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p><strong>RMSprop+Momentum</strong></p><p>即梯度的指数平均和梯度平方的指数平均的结合：<br>$$<br>\begin{aligned} m_{t} &amp;=\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \ v_{t} &amp;=\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \end{aligned}<br>$$<br>mt和vt的初始值为0，训练初期可能较小，因此需要对其放大：<br>$$<br>\begin{aligned} \hat{m}<em>{t} &amp;=\frac{m</em>{t}}{1-\beta_{1}^{t}} \ \hat{v}<em>{t} &amp;=\frac{v</em>{t}}{1-\beta_{2}^{t}} \end{aligned}<br>$$<br>即：<br>$$<br>\theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{\hat{v}<em>{t}}+\epsilon} \hat{m}</em>{t}<br>$$<br>作者建议β1设置为0.9,β2设置为0.999，取ϵ=10−8。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><ul><li>从Vanilla GD到动量项，解决了梯度震荡的问题；<strong>Momentum</strong></li><li>加入历史梯度累计作为修正项，解决了每个参数的步长一样的问题；<strong>Adagrad</strong></li><li>一阶梯度累计项变二阶梯度累计项的指数加权平均，解决了Adagrad训练后期学习率小的问题；<strong>RMSprop</strong></li><li><strong>Momentum+RMSprop=Adam</strong></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统FM/FMM</title>
      <link href="/2019/11/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9FFM%E3%80%81FFM/"/>
      <url>/2019/11/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9FFM%E3%80%81FFM/</url>
      
        <content type="html"><![CDATA[<p>CTR预估时，除了单特征，还要对特征进行组合。组合方法：FM系列和Tree系列。</p><ol><li><p>FM：为了解决特征稀疏</p><p>one-hot编码后，特征空间非常稀疏，进而造成一些问题。</p></li><li><p>线性模型：<br>$$<br>y=\omega_{0}+\sum_{i=1}^{n} \omega_{i} x_{i}<br>$$</p></li><li><p>FM：加入线性组合，只加入两个特征的组合：</p></li></ol><p>$$<br>y=\omega_{0}+\sum_{i=1}^{n} \omega_{i} x_{i}+\sum_{i=1}^{n-1} \sum_{j=i+1}^{n} \omega_{i j} x_{i} x_{j}<br>$$</p><pre><code>- 问题：特征稀疏，xi, xj都不为0的情况比较少，所以wij无法训练得出</code></pre><ul><li><p>解法：</p><ul><li>共n个特征，对每个特征分量xi引入辅助向量Vi，Vi的长度为k<br>$$<br>\mathbf{V}=\left(\begin{array}{cccc}{v_{11}} &amp; {v_{12}} &amp; {\cdots} &amp; {v_{1 k}} \ {v_{21}} &amp; {v_{22}} &amp; {\cdots} &amp; {v_{2 k}} \ {\vdots} &amp; {\vdots} &amp; {} &amp; {\vdots} \ {v_{n 1}} &amp; {v_{n 2}} &amp; {\cdots} &amp; {v_{n k}}\end{array}\right)<em>{n \times k}=\left(\begin{array}{c}{\mathbf{v}</em>{1}} \ {\mathbf{v}<em>{2}} \ {\vdots} \ {\mathbf{v}</em>{n}}\end{array}\right)<br>$$<br>有：<br>$$<br>\hat{\mathbf{W}}=\mathbf{V} \mathbf{V}^{T}=\left(\begin{array}{c}{\mathbf{v}<em>{1}} \ {\mathbf{v}</em>{2}} \ {\vdots} \ {\mathbf{v}<em>{n}}\end{array}\right)\left(\begin{array}{cccc}{\mathbf{v}</em>{1}} \ {\mathbf{v}<em>{1}^{T}} &amp; {\mathbf{v}</em>{2}^{T}} &amp; {\cdots} &amp; {\left.\mathbf{v}<em>{n}^{T}\right)} \ {\mathbf{v}</em>{n}} &amp; {} &amp; {} &amp; {}\end{array}\right.<br>$$<br>从求解w变为求解v：<br>$$<br>\begin{aligned} &amp; \sum_{i=1}^{n-1} \sum_{j=i+1}^{n}\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{j}\right\rangle x_{i} x_{j} \=&amp; \frac{1}{2} \sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{j}\right\rangle x_{i} x_{j}-\frac{1}{2} \sum_{i=1}^{n}\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{i}\right\rangle x_{i} x_{i} \=&amp; \frac{1}{2}\left(\sum_{i=1}^{n} \sum_{j=1}^{n}\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{j, f}, x_{i} x_{j}-\sum_{i=1}^{n} \sum_{f=1}^{k} v_{i, f} v_{i, f} x_{i} x_{i}\right)\right.\=&amp; \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \=&amp; \frac{1}{2} \sum_{f=1}^{k}\left(\left(\sum_{i=1}^{n} v_{i, f} x_{i}\right)^{2}-\sum_{i=1}^{n} v_{i, f}^{2} x_{i}^{2}\right) \end{aligned}<br>$$<br>即：从求ab变为求解<br>$$<br>\left((a+b+c)^{2}-a^{2}-b^{2}-c^{2}\right.<br>$$</li></ul></li></ul><ol start="4"><li><p>FFM</p><ul><li><p>加入field的概念：FM中，v针对特征是经过one-hot后处理后的，有些特征是从同一个类别特征one-hot得来的</p></li><li><p>隐向量不仅与特征有关，也与field有关</p></li><li><p>n个特征属于f个field，FM相当于把所有特征归属于一个field时的FFM。</p></li><li><p>FFM有nfk个参数，FM有nk个</p></li><li><p>$$<br>y(\mathbf{x})=w_{0}+\sum_{i=1}^{n} w_{i} x_{i}+\sum_{i=1}^{n} \sum_{j=i+1}^{n}\left\langle\mathbf{v}_{i ,f_j}, \mathbf{v}<em>{j ,f</em>{i}}\right\rangle x_{i} x_{j}<br>$$</p></li><li><p>一种实现：将问题定义为分类问题，使用加入正则项的logloss（1，-1的logloss）<br>$$<br>\min <em>{\mathbf{w}} \sum</em>{i=1}^{L} \log \left(1+\exp \left{-y_{i} \phi\left(\mathbf{w}, \mathbf{x}_{i}\right)\right}\right)+\frac{\lambda}{2}|\mathbf{w}|^{2}<br>$$<br>利用SGD进行训练。</p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统实验方法与评测</title>
      <link href="/2019/11/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95%E4%B8%8E%E8%AF%84%E6%B5%8B/"/>
      <url>/2019/11/19/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%AE%9E%E9%AA%8C%E6%96%B9%E6%B3%95%E4%B8%8E%E8%AF%84%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="实验方法"><a href="#实验方法" class="headerlink" title="实验方法"></a>实验方法</h1><h2 id="离线"><a href="#离线" class="headerlink" title="离线"></a>离线</h2><p>训练集、测试集</p><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>不需要对实际系统的控制</td><td>无法计算商业上关心的指标</td></tr><tr><td>无需用户参与实验</td><td>离线与在线的差距</td></tr><tr><td>速度快，可以测试大量算法</td></tr></tbody></table><h2 id="用户调查"><a href="#用户调查" class="headerlink" title="用户调查"></a>用户调查</h2><table><thead><tr><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>可以获得体现主观感受的指标</td><td>招募测试用户代价大</td></tr><tr><td>相对在线实验风险低</td><td>难以组织大量用户</td></tr></tbody></table><h2 id="在线"><a href="#在线" class="headerlink" title="在线"></a>在线</h2><p>ABtest</p><h1 id="评测指标"><a href="#评测指标" class="headerlink" title="评测指标"></a>评测指标</h1><ul><li><p>用户满意度</p><p>用户调查 / 在线实验</p></li><li><p>准确度</p><ul><li><p>评分预测: rmse, mae</p></li><li><p>Top N: recall, precision<br>$$<br>\begin{array}{l}{\quad \text { Recall }=\frac{\sum_{u v}|R(u) \cap T(u)|}{\sum_{u=U}|T(u)|}} \ {\text { Precision }=\frac{\sum_{u v}|R(u) \cap T(u)|}{\sum_{u=v}|R(u)|}}\end{array}<br>$$</p></li></ul></li><li><p>覆盖率：需体现推荐系统挖掘长尾的能力</p><ul><li><p>信息熵 H = - sigma p(i)logp(i)</p></li><li><p>基尼系数<br>$$<br>G=\frac{1}{n-1} \sum_{j=1}^{n}(2 j-n-1) p\left(i_{j}\right)<br>$$<br>这里ij是按照物品流行度p()从小到大排序的物品列表中第j个物品</p><p><img src="/Users/didi/Library/Application Support/typora-user-images/image-20190924155235973.png" alt="image-20190924155235973"></p></li></ul></li><li><p>多样性：用户兴趣是广泛的</p></li><li>新颖性：给用户推荐其之前没有听过或看过的物品</li><li>精细度：推荐结果和用户的历史兴趣不相似，但却让用户觉得满意，那么就可以说推荐结果的惊喜度很高，而推荐的新颖性仅仅取决于用户是否听说过这个 推荐结果。</li><li>信任度<ul><li>提高推荐系统透明度</li><li>考虑用户社交网络信息，利用好友信息给用户作推荐，并用好友进行推荐解释</li></ul></li><li>实时性：物品的时效性、用户兴趣的时效性</li><li>健壮性（鲁棒性）：反作弊</li><li>商业目标</li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 推荐系统 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>滴滴实习总结</title>
      <link href="/2019/11/19/%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/"/>
      <url>/2019/11/19/%E5%AE%9E%E4%B9%A0%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h1 id="信用分"><a href="#信用分" class="headerlink" title="信用分"></a>信用分</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>平台上每天对乘客有10+w差评投诉，专快<strong>坏账</strong>每月3千万，<strong>司机nps -17%</strong>，<strong>乘客诈骗</strong>引起pr事件，专车乘客封禁但依然能活跃在其他业务线等，乘客管控一片空白，业务迫切希望有一个抓手来解决这些问题。在这个背景下，乘客行为分应运而生。</p><p>乘客行为分的本质是<strong>按照乘客在平台上的行为对其进行分层</strong>。乘客行为分是构建良好司机／乘客生态的一部分，<strong>能为平台提供更多GMV／运力，为司机提供更好体验（NPS）</strong>，为乘客提供更好体验（NPS）。</p><ul><li>平台资损：补券、有责取消（取消费用、取消次数、取消率）</li><li>平台体验：司机投诉（司机nps）、乘客投诉（虚假投诉）</li></ul><p>定位：<strong>乘客行为分是连接B端和C端的工具。其中，C端是乘客，公司内的B端主要包括客服部门 （乘客管控方向）／乘客补贴部门（补贴方向）和乘客运营部门（拉新方向）。</strong></p><p>价值：<strong>乘客行为分的核心价值在于为B端提供用户分层服务，为C端提供用户权益（差异化服务）。</strong></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><ul><li>准入（乘客运营）：<ul><li>乘客行为分替代芝麻分作为海棠湾（出行单车事业部）和黑马（电单车）的准入条件，分别提高<strong>31%</strong>、<strong>30%</strong>的转化率</li></ul></li><li>免押（乘客运营）：<ul><li>粤港车减低未播率 提高乘客体验</li></ul></li><li>降资损（乘客管控）：<ul><li>恶意补券</li><li>低信用乘客不给预付</li></ul></li><li>乘客管控与乘客教育：<ul><li>海棠湾和黑马 行为分露出，坏行为下降，好行为上升</li><li>深圳地区乘客教育与乘客管控</li></ul></li><li><strong>司机体验优化（低信用乘客豁免）</strong></li><li>分单倾斜</li><li>用作其他业务的参考依据：为滴滴金融等借贷业务提供决策的依据</li></ul><h2 id="评估"><a href="#评估" class="headerlink" title="评估"></a>评估</h2><p><img src="C:\Users\Chenzk\Documents\Learning\滴滴\cmd\image\image2019-8-2_11-47-48.png" alt="image2019-8-2_11-47-48"></p><p>收益评估：正向收益与负向收益</p><h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>履约表现、行为健康度、净贡献值、身份特征</p><h3 id="履约表现"><a href="#履约表现" class="headerlink" title="履约表现"></a>履约表现</h3><ul><li>标签：表现期内是否违约（网约车业务逾期30天（平台对坏账的定义）以上，滴水贷违约）</li><li>特征：履约能力（固定资产、流动资产、平台流水（发单次数、gmv、完单次数））和信用历史（个人借贷还款、逾期（逾期支付次数、逾期支付金额、滴水贷还款总金额、当前逾期未还本金、逾期次数、历史最大逾期天数）等情况的统计信息）</li><li>baseline：网约车规则、粤港车规则</li><li>样本<ul><li>观察期内有行为的乘客，没有行为发生给默认分</li><li>抽样：全量粤港车（1万）+全量滴水贷（45万）+随机抽样网约车（ 200万）</li></ul></li><li>分箱<ul><li>等频、等距、卡方</li></ul></li><li>模型：见卡方分布及卡方检验、评分卡</li><li>评估：（大盘抽样或整个大盘，例如ks、gini等可以直接用整个大盘数据）（如果不合常理问题一般是woc值的分布规律不正常）<ul><li>准召（固定一个调整另一个，注重于精确率，粤港车注重于召回率（因为未付订单较少，则降资损更重要，所以查全））、f1、ks（0.42）、auc（0.82）</li><li>分布：平滑 正态 有无异常值，如突出的毛刺（对应了分箱或者数据中的异常值，例如企业支付、代叫号，如果企业支付是按乘客算的，那就算看起来是异常数据也要加入）</li><li>分数与GMV、坏账、未播率、逾期单数、渗透率、滴水贷逾期金额、滴水贷逾期次数的分布</li><li>应用于业务时的影响面、未播率、资损delta</li></ul></li><li>思考：<ul><li>label选择的方案：逾期31天和逾期29天在特征方面差别很小，如何设计？考虑去除灰色部分，即训练时只选取逾期25天以内的作为label=0，35天以上作为label=1，但测试的时候全部加入，特征和测试的label不变，依然为30天，即只是为了提高模型的表现</li></ul></li></ul><h3 id="行为健康度"><a href="#行为健康度" class="headerlink" title="行为健康度"></a>行为健康度</h3><ul><li><p>Label：根据乘客在历史X个月的行为表现预测在未来X个月的行为表现。好：label=0，坏：label=1.（其中历史x选取6，未来x选取1）</p><p>其中坏行为是有责投诉(费用类有责投诉和服务类有责投诉)、有责取消和迟到。</p><p><strong>label选取方式：</strong></p><ol><li>人群范围：历史六个月发单&gt;=10单 &amp; 未来一个月发单&gt;=5单</li><li>label1：未来一个月的坏行为发生率&gt;=0.4，其中坏行为发生率=（有责投诉数+有责取消数+迟到）/ 发单数。</li><li>label0：未来一个月的坏行为发生次数&lt;=1</li></ol></li><li><p>数据选取：</p><ol><li>label1：选取满足条件的全部数据量。</li><li>label0：从满足条件的数据中随机选取14w。正：负=1：1。</li><li>数据划分：训练集：测试集=8:2</li></ol></li><li><p>特征：[快专出豪顺]完单数、投诉数、费用有责投诉、服务类有责投诉（不包括取消和费用投诉）、有责取消次数、无责取消次数、应答后取消次数、对司机好评、差评、迟到…</p></li><li><p>评估：</p><ul><li>虚假投诉乘客管控：高投诉高补偿（发单&gt;=10 &amp; 投诉工单&gt;=5 &amp;（投诉率&gt;=0.3 or 投诉数&gt;= 15） &amp; (补偿率&gt;= 0.3 or 补偿订单数&gt;= 10) &amp; 用户价值&lt;0 ）乘客分布集中在低分段</li></ul></li><li><p>应用：</p><ul><li>区域乘客教育：取消、迟到、投诉、费用投诉、费用有责投诉的分布、与分数的分布、率的分布。最终使用规则+分数的方式做乘客教育。其中规则和分数的阈值选取：考虑影响面（8万左右）。取消次数大于等于3次，分数小于等于620分，对应人数80934，作为教育的对象；迟到次数大于等于3次，分数小于等于569，影响人数19239，作为教育的对象。<strong>高取消/高迟到乘客次月的重犯次数依旧很高，说明教育必要性</strong></li><li>乘客管控：石锤虚假投诉封禁</li></ul></li></ul><h3 id="分数更新"><a href="#分数更新" class="headerlink" title="分数更新"></a>分数更新</h3><ol><li>更新方式<ol><li>全量更新：选取固定时间周期，滑动窗口，重新训练模型。<ol><li>优势：模型是用最新的数据产生的模型，产生的分数在固定维度内具有区分度。</li><li>劣势：每次生成新的分数，分数变动不稳定。</li></ol></li><li>增量更新：在初始分数的基础上，根据增量的数据做分数的增量变动。<ol><li>优势：分数变动波动性小。</li><li>劣势：增量分数的累积会使区分度变差。</li></ol></li></ol></li><li>更新周期<ol><li>参考选取label的周期。label选取是未来一个月，更新周期是一个月更新一次。</li><li>更新周期太频繁，分数变动太快，不够稳定。更新周期太长，分数不够准确体现乘客当前的行为。</li></ol></li></ol><p>为了使模型向后兼容&amp;&amp;打通流程，在更新时选取增量更新的方式，初试分数的计算和增量更新时的分数需要具有可解释性。</p><h2 id="问题与挑战"><a href="#问题与挑战" class="headerlink" title="问题与挑战"></a>问题与挑战</h2><h3 id="多个业务场景"><a href="#多个业务场景" class="headerlink" title="多个业务场景"></a>多个业务场景</h3><p>分多维度、维度下分子维度，不同维度与子维度都会生成相应的分数，对不同维度设置不同阈值，从而建立个性化的门槛，就可以赋能于不同的业务场景（<strong>用户权益／乘客管控／司机体验</strong>）。</p><h3 id="可解释性"><a href="#可解释性" class="headerlink" title="可解释性"></a>可解释性</h3><p>乘客行为分在最终理想态下需要透传。这意味着，需要考虑行为分（加分和扣分）的可解释性。对用户透传分数时有以下几种不同的力度和方案。</p><ol><li>直接透传当前的总分（类似芝麻分）。这种形式可控性强，但用户对于自己的分数怎么计算的会有疑惑，透传的话会有大量的用户进线。</li><li>透传每种行为项的分数。<ul><li>这种形式可解释性强，用户对于分数的构成很清晰，但是会造成用户恶意刷分的情况。同时不在计算范围的行为，用户可能不会care，从而缺乏管控的能力。</li><li>这种形式可控性很差。如果后期每项行为的分值发生变化，分数的稳定性会受到用户的质疑。</li></ul></li></ol><ol><li>两种形式的折衷（类似内测的橙信值）。对于坏行为给用户完全透传出来，使用户意识到哪些形式要明令禁止的；对于好行为给出加分的总和。</li><li>两种形式的折衷（类似内测的橙信值）。对于坏行为给用户完全透传出来，使用户意识到哪些形式要明令禁止的；对于好行为给出加分的总和。</li></ol><p>增强行为分可解释性，意味着牺牲策略空间，并且增加了刷分的风险。所以在一些维度（例如乘客身份特征：如实名认证等）上需要尽可能提供高可解释性，而在另一些维度（例如行为健康度：如被司机投诉等）上则需要尽可能保持高度灵活。</p><p><strong><em>xgboost更具解释性？存疑 每个叶子都有权重</em></strong></p><h3 id="数据稀疏"><a href="#数据稀疏" class="headerlink" title="数据稀疏"></a>数据稀疏</h3><ul><li>采样方式，解决有些特征难以用到的问题</li></ul><p>调研了推荐系统中的数据稀疏解决方式，见wiki</p><p>两篇论文：</p><p><a href="http://link.zhihu.com/?target=https%3A//github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BAirbnb%20Embedding%5D%20Real-time%20Personalization%20using%20Embeddings%20for%20Search%20Ranking%20at%20Airbnb%20%28Airbnb%202018%29.pdf" target="_blank" rel="noopener">Real-time Personalization using Embeddings for Search Ranking at Airbnb (Airbnb 2018)</a></p><p><a href="http://link.zhihu.com/?target=https%3A//github.com/wzhe06/Reco-papers/blob/master/Embedding/%5BAlibaba%20Embedding%5D%20Billion-scale%20Commodity%20Embedding%20for%20E-commerce%20Recommendation%20in%20Alibaba%20%28Alibaba%202018%29.pdf" target="_blank" rel="noopener">[Alibaba Embedding] Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba (Alibaba 2018)</a></p><h2 id="体会"><a href="#体会" class="headerlink" title="体会"></a>体会</h2><ul><li><p>数据：自己取；大规模；</p></li><li><p>建模目标、label：自定义</p></li><li><p>问题：采样方法、数据稀疏</p></li><li><p>Q：在一个具体项目中，前期通过简单的规则作为baseline的必要性在哪？</p><p>A：1）前期实现时，模型缺乏对照，规则是最简单的可以提供此对照的实现；</p><p>2）公司中不仅有算法/策略部门，还有产品、运营、开发部门，为什么产品、运营、开发给一份规则不能解决问题？算法相对规则可以带来多少delta收益？这也是方案评估的时候会被考虑的几个标准，换言之，baseline作为对照也是对算法/策略部门存在必要性的一个说明。</p><p>Q：机器学习模型中，有些标签本身就是离散的，例如男/女，有房/无房等，而有些标签是连续特征离散化的，例如青年/中年，再例如判责中有有责/无责，也有判不清，这些标签是人为从某些连续标签经过离散化得到的。在拟合连续标签离散化得到的标签的时候，就存在一个灰色样本的问题：人为设定的界线附近的标签区分度不高（例如长得很像狗的猫和长得很像猫的狗就很难给出区分的定义），导致在训练集中打label的时候很难打；就算打好了label，预测时灰色样本的置信度也会比较低，这该如何解决？</p><p>A：1）二分类问题模型输出是一个概率值，再通过设置阈值的方式可以规避掉这些灰色样本的存在，但这种方法是回避了问题，并没有真正判清样本；2）有些分类模型是通过设置阈值来分类的，也有些是通过排序来分类，应用排序的方式可以避免概率值接近这种衡量方式上的判不清；3）有些连续标签的离散化相当于模糊集合的应用，因此在模型中恰当地引入模糊集合理论也可以实现对问题的解决。</p><p>A2：工业界中的数据是自己产生的，label有时是真实数据，有时是要自己打的，信用分三个子维度的问题，是通过已有数据打label，这个打的过程本身就存在打不清楚的问题；如果在训练集中剔除灰色样本，则—-→训练集测试集分布不一致，用不存在预测存在，出现很多问题。</p><p>泓州提到，分布要一致这玩意是理论，我们现在在实践，实践以实际业务需求以及效果为衡量王道，其他不care，能否满足业务需求，能否达到更好的效果才是所关心的，所以分布一致这条原则可以舍去。</p><p>感想：没那么多条条框框，思维要开阔，限制不应该是理论前提的不满足，而是业务、效果。？业务、效果也别限制了吧，怎么爽怎么来。</p><p><img src="C:\Users\Chenzk\Documents\Learning\滴滴\cmd\image\微信图片_20191115101713.png" alt="微信图片_20191115101713"></p></li><li><p>业务方：模型的本质是用历史行为来判责当次发生的问题，说服力不强。</p><p>业务方要求定制化分数?</p><p>可解释性？</p><p>Q：为什么要有可解释性，对这个分数负责不就好了，用户真的有申诉信用分的必要吗？</p><p>A：权益激励/限制权益等不需要，类似于芝麻分信用分不够不能免费骑车也不会有人申诉。但是一些更大的处置动作，例如准出、处罚等需要有可解释性。不care前85%的好人好在哪里，关注的是尾部分布的15%有问题的乘客问题在哪里，以及问题会导致什么 ——&gt; 管控</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 实习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贪心算法</title>
      <link href="/2019/11/19/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/"/>
      <url>/2019/11/19/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h4 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h4><p>顾名思义，贪心算法总是作出在当前看来最好的选择。也就是说贪心算法并不从整体最优考虑，它所作出的选择只是在某种意义上的局部最优选择。在一些情况下，即使贪心算法不能得到整体最优解，其最终结果却是最优解的很好近似。</p><ul><li><p>贪心选择性质。所谓贪心选择性质是指所求问题的整体最优解可以通过一系列局部最优的选择，即贪心选择来达到。这是贪心算法可行的第一个基本要素，也是贪心算法与动态规划算法的主要区别。</p><p>动态规划算法通常以自底向上的方式解各子问题，而贪心算法则通常以自顶向下的方式进行，以迭代的方式作出相继的贪心选择，每作一次贪心选择就将所求问题简化为规模更小的子问题。</p><p>对于一个具体问题，要确定它是否具有贪心选择性质，必须证明每一步所作的贪心选择最终导致问题的整体最优解。</p></li><li><p>当一个问题的最优解包含其子问题的最优解时，称此问题具有最优子结构性质。问题的最优子结构性质是该问题可用动态规划算法或贪心算法求解的关键特征。</p></li></ul><h4 id="活动时间安排"><a href="#活动时间安排" class="headerlink" title="活动时间安排"></a>活动时间安排</h4><p>设有N个活动时间集合，每个活动都要使用同一个资源，比如说会议场，而且同一时间内只能有一个活动使用，每个活动都有一个使用活动的开始si和结束时间fi，即他的使用区间为（si,fi）,现在要求你分配活动占用时间表，即哪些活动占用该会议室，哪些不占用，使得他们不冲突，要求是尽可能多的使参加的活动最大化，即所占时间区间最大化。</p><p><img src="https://img-my.csdn.net/uploads/201303/29/1364554174_8902.jpg" alt="img"></p><ul><li>重点：<strong>结束时间按顺序排列</strong>。时间从前往后推移，若以当时时间为开始时间的活动的开始时间≥前一个活动的结束时间，选择此活动即可 –&gt; 原因是<strong>结束时间按顺序排列</strong>。</li></ul><figure class="highlight plain"><figcaption><span>main(int argc, char* argv[])</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">int main(int argc,char* argv[])</span><br><span class="line">&#123;</span><br><span class="line">int s[11] =&#123;1,3,0,5,3,5,6,8,8,2,12&#125;;</span><br><span class="line">int f[11] =&#123;4,5,6,7,8,9,10,11,12,13,14&#125;;</span><br><span class="line"> </span><br><span class="line">bool mark[11] = &#123;0&#125;;</span><br><span class="line"> </span><br><span class="line">GreedyChoose(11,s,f,mark);</span><br><span class="line">for(int i=0;i&lt;11;i++)</span><br><span class="line">if(mark[i])</span><br><span class="line">cout&lt;&lt;i&lt;&lt;&quot; &quot;;</span><br><span class="line">system(&quot;pause&quot;);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">void GreedyChoose(int len,int *s,int *f,bool *flag)</span><br><span class="line">&#123;</span><br><span class="line">flag[0] = true;</span><br><span class="line">int j = 0;</span><br><span class="line">for(int i=1;i&lt;len;++i)</span><br><span class="line">if(s[i] &gt;= f[j])</span><br><span class="line">&#123;</span><br><span class="line">flag[i] = true;</span><br><span class="line">j = i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="线段覆盖"><a href="#线段覆盖" class="headerlink" title="线段覆盖"></a>线段覆盖</h4><p>在一维空间中告诉你N条线段的起始坐标与终止坐标，要求求出这些线段一共覆盖了多大的长度。</p><p><img src="https://img-my.csdn.net/uploads/201303/30/1364630415_9404.jpg" alt="img"></p><p>重点：<strong>将线段按起始坐标排序</strong>。</p><figure class="highlight plain"><figcaption><span>main(int argc, char* argv[])</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">int s[10] = &#123;2,3,4,5,6,7,8,9,10,11&#125;;</span><br><span class="line">int f[10] = &#123;3,5,7,6,9,8,12,10,13,15&#125;;</span><br><span class="line">int TotalLength = (3-2);                 </span><br><span class="line"> </span><br><span class="line">for(int i=1,int j=0; i&lt;10 ; ++i)</span><br><span class="line">&#123;</span><br><span class="line">if(s[i] &gt;= f[j])</span><br><span class="line">&#123;</span><br><span class="line">TotalLength += (f[i]-s[i]);</span><br><span class="line">j = i;</span><br><span class="line">&#125;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">if(f[i] &lt;= f[j])</span><br><span class="line">continue;</span><br><span class="line">else</span><br><span class="line">&#123;</span><br><span class="line">TotalLength += f[i] - f[j];</span><br><span class="line">j = i;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">cout&lt;&lt;TotalLength&lt;&lt;endl;</span><br><span class="line">system(&quot;pause&quot;);</span><br><span class="line">return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="找零钱问题"><a href="#找零钱问题" class="headerlink" title="找零钱问题"></a>找零钱问题</h4><p>贪心：每次选择可选的最大面值的钱。</p><p>本质上还是<strong>排序</strong>。</p><h4 id="Leetcode"><a href="#Leetcode" class="headerlink" title="Leetcode:"></a>Leetcode:</h4><p>给定一个数组，它的第 <em>i</em> 个元素是一支给定股票第 <em>i</em> 天的价格。</p><p>设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。</p><p><strong>注意：</strong>你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。</p><p><a href="https://leetcode-cn.com/explore/interview/card/top-interview-questions-easy/1/array/22/" target="_blank" rel="noopener">https://leetcode-cn.com/explore/interview/card/top-interview-questions-easy/1/array/22/</a></p><p>使用贪心算法的一个假设：同一天可以卖出又可以买入</p><figure class="highlight plain"><figcaption><span>Solution</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def maxProfit(self, prices: List[int]) -&gt; int:</span><br><span class="line">    ans = 0</span><br><span class="line">    for i in range(len(prices)-1):</span><br><span class="line">        if prices[i+1]&gt;prices[i]:</span><br><span class="line">            ans += prices[i+1]-prices[i]</span><br><span class="line">    return ans</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> DS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>评分卡</title>
      <link href="/2019/11/19/%E8%AF%84%E5%88%86%E5%8D%A1/"/>
      <url>/2019/11/19/%E8%AF%84%E5%88%86%E5%8D%A1/</url>
      
        <content type="html"><![CDATA[<ul><li><p>模型流程</p><ul><li><p>分箱：等频、等距、卡方（掌握卡方分箱的原理和操作方式：卡方值最小的相邻区间合并（卡方越小说明俩区间区别越小，卡方衡量都是区间中label的分布））</p></li><li><p>根据分箱求得woe=ln(# good占比/ # bad占比)</p></li><li><p>LR模型获得W</p></li><li><p>score = A - B*ln(odds) </p><ul><li>直观理解：对一条数据来说，其属于好样本的概率与属于坏样本的概率的比值越大，其分数越大。只不过用了LR来间接表示这个概率，且未直接用到特征值，用到的是对label有可解释性的woe值。</li><li>A, B: 设置比率为θ0（也就是odds）的特定点分值为P0，比率为2θ0的点的分值为P0+PDO。带入上面公式可得到：</li></ul><p>$$<br>\left{\begin{array}{ll}{P_{0}} &amp; {=A-B \ln \left(\theta_{0}\right)} \ {P_{0}+P D O} &amp; {=A-\operatorname{Bln}\left(2 \theta_{0}\right)}\end{array}\right.<br>$$</p><p>即：<br>$$<br>\left{\begin{array}{l}{B=\frac{P D O}{l n 2}} \ {A=P_{0}+B \ln \left(\theta_{0}\right)}\end{array}\right.<br>$$</p></li></ul></li></ul><pre><code>（可以设置P0=600，PDO=20, θ0=1/30）</code></pre><ul><li><p>实际应用时</p><ul><li>LR模型中：</li></ul><p>$$<br>p=\frac{1}{1+e^{-\theta^{T} x}}<br>$$</p><p>$$<br>\begin{array}{c}{\ln \left(\frac{p}{1-p}\right)=\theta^{T} x} \ {\ln \left(\frac{p}{1-p}\right)=\ln (\text {odds})} \ {\ln (\text {odds})=\theta^{T} x=w_{0}+w_{1} x_{1}+\cdots+w_{n} x_{n}}\end{array}<br>$$</p><p>$$<br>\begin{aligned} \text {score}<em>{\mathbb{Z}} &amp;=A+B <em>\left(\theta^{T} x\right)=A+B </em>\left(w</em>{0}+w_{1} x_{1}+\cdots+w_{n} x_{n}\right) \ &amp;=\left(A+B <em> w_{0}\right)+B </em> w_{1} x_{1}+\cdots+B * w_{n} x_{n} \end{aligned}<br>$$</p></li></ul><pre><code>先通过lr模型对(woe, label)训练得到权重矩阵，并获得初始分A+B*w0；每个变量的分箱对应的分数为，权重\*每个分箱对应的woe- 新加入的用户，根据之前得到的分箱dict得到其特征的分箱区间对其特征进行分箱，在根据上一步得到的每个分箱的分数来获得每个特征的分数，并逐个相加，得到总分数</code></pre><ul><li><p>特征选择与分箱调参</p><ul><li><p><strong>并不是维度越多越好。</strong>一个评分卡中，一般不超过15个维度。</p><ol><li>可根据Logistic Regression模型系数来确定每个变量的权重，保留权重高的变量。通过协方差计算的相关性大于0.7的变量一般只保留IV值最高的那一个。</li><li>IV可以衡量特征的预测能力：</li></ol><p>$$<br>I V=\sum_{i=1}^{N}\left(g o o d_{占比} -bad_{占比} \right) * W O E_{i}<br>$$</p></li></ul><p>| IV        | 预测能力   |<br>| ——— | ———- |<br>| &lt;0.03     | 无预测能力 |<br>| 0.03~0.09 | 低         |<br>| 0.1~0.29  | 中         |<br>| 0.3~0.49  | 高         |<br>| &gt;=0.5     | 极高       |</p><p>预测能力低则需重新调整分箱。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 评分卡 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>评分卡优化</title>
      <link href="/2019/11/19/%E8%AF%84%E5%88%86%E5%8D%A1%E4%BC%98%E5%8C%96/"/>
      <url>/2019/11/19/%E8%AF%84%E5%88%86%E5%8D%A1%E4%BC%98%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<ul><li>A-Bln(odds)：应有weight&gt;0</li></ul><h1 id="优化1"><a href="#优化1" class="headerlink" title="优化1"></a>优化1</h1><ul><li>加入更多的网约车数据</li><li>等宽</li><li>删特征 –&gt; weight&lt;0的特征减少了 –&gt;平滑 </li><li>异常值处理：做异常值截断</li><li>长尾分布取对数</li></ul><ul><li>只用ygc数据，且进行上采样，precision不超过5% –&gt; 无效果</li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 评分卡 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>卡方分布及卡方检验</title>
      <link href="/2019/11/19/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83%E5%8F%8A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/"/>
      <url>/2019/11/19/%E5%8D%A1%E6%96%B9%E5%88%86%E5%B8%83%E5%8F%8A%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/cuitzjd/article/details/80755310" target="_blank" rel="noopener">Doc1</a><br><a href="https://cosx.org/2010/11/hypotheses-testing/" target="_blank" rel="noopener">Doc2</a></p><h3 id="统计检验"><a href="#统计检验" class="headerlink" title="统计检验"></a>统计检验</h3><ul><li>出发点：小概率事件发生则假设不成立。可以检验一个样本与总体的差异，也可以检验一个样本集与另一个样本集的差异。</li><li>e.g. 观察到某个班级男女生身高的均值有差距，要证明这一点，先假设其”没有差距“，然后做相关的检验，如果检验出来有差距这个事件的概率小于某个值，则说明假设不成立。即一个具有足够小概率的事件发生了，这不是偶然。</li><li>第一类错误：原假设为真，检验的结果劝你放弃原假设。其概率为α。显著性水平。</li><li>第二类错误：原假设为假，检验的结果劝你接受原假设。概率为1-α。</li><li>显著性检验，即只限定第一类错误。当经过检验发现p&gt;α，则说明检验、总体这两样本之间不存在显著性差异，因此接受原假设；否则，说明一个很小概率的事件发生了，不接受原假设。</li><li>不同的检验方式有不同的前提。</li></ul><h3 id="T"><a href="#T" class="headerlink" title="T"></a>T</h3><ul><li><p><strong>用于样本量较小，总体标准差σ未知的正态分布。</strong></p></li><li><p>单总体检验：一个样本的平均数与已知的总体平均数的差异是否显著。</p></li><li><p>双总体样本检验：一个样本与另一个样本的差异。</p></li></ul><h3 id="F"><a href="#F" class="headerlink" title="F"></a>F</h3><ul><li>两个样本的总体方差是否相等。</li></ul><h3 id="卡方"><a href="#卡方" class="headerlink" title="卡方"></a>卡方</h3><h4 id="卡方检验"><a href="#卡方检验" class="headerlink" title="卡方检验"></a>卡方检验</h4><p><img src="https://ss0.bdstatic.com/94oJfD_bAAcT8t7mm9GUKT-xh_/timg?image&amp;quality=100&amp;size=b4000_4000&amp;sec=1512961717&amp;di=6133a04e10dc72b22bfb76fca025156d&amp;src=http://pic.baike.soso.com/p/20130619/20130619123748-1653885327.jpg" alt></p><ul><li><p>原假设为两者分布无差异、不相关等。</p></li><li><p>卡方值越大，实际（假设）与理论（根据原假设求得的概率值/统计量）差异越大，即假设的错误程度越大，超过阈值，则推翻假设。</p><p>若求得的卡方值落在阈值之上，说明小概率事件(e.g. 5%)发生了，则推翻原假设，说明两者差异的程度确实够大，即两者有差异；否则有95%的置信程度说明两者无差异。</p></li><li><p>参数有：自由度、置信水平。</p><p>例如，看喝牛奶与感冒有没有关系。给定置信度β，确定卡方值阈值x。</p><p>原假设：无关。</p><p>卡方值：先在原假设的基础上求出理论分布，然后看理论分布与实际分布的差异性（卡方值）。</p><p>查表，看有没有超过阈值，若超过，说明小概率（显著性水平：1-β）事件发生，推翻原假设，两者有关。</p></li></ul><h4 id="卡方分布"><a href="#卡方分布" class="headerlink" title="卡方分布"></a>卡方分布</h4><p>n个相互独立且均服从标准正态分布的随机变量，其平方和服从卡方分布。</p><p><img src="https://img-blog.csdn.net/20180109180130184?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc25vd2Ryb3B0dWxpcA==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt></p><h4 id="卡方分箱"><a href="#卡方分箱" class="headerlink" title="卡方分箱"></a>卡方分箱</h4><ul><li><p>算法步骤：</p><ul><li><p>初始化：排序，每个实例属于一个一个区间</p></li><li><p>合并区间：</p><ul><li><p>计算每一对相邻区间的卡方值</p></li><li><p>将卡方值最小的一对区间合并</p><p>$$  \mathrm{X}^{2}=\sum_{i=1}^{2} \sum_{j=1}^{2} \frac{\left(A_{i j}-E_{i j}\right)^{2}}{E_{i j}}  $$</p><p>其中，Ai,j为第i区间第j类的实例的数量，Ei,j为期望频率（整体分布，整体分布计算的时候还是用的这俩区间的数值）=Ni*(Cj/N)，Cj为j类在整体分布中的比例，Ni为i区间样本数</p><p>例如label有0和1，有两个区间，即[[3,4], [5,7]]</p><p>A11=3,A12=4… E11=7*8/19</p></li></ul></li></ul></li><li><p>原理：假设这俩区间没差异，其卡方值代表了该假设（由实际分布得出的统计量）与理论（整体样本[特征]分布）的差异程度，越小则说明俩越接近，假设越成立。</p><p>评分卡模型中，第i区间第j类的实例：类是指区间中有多少类特征，实例指其对应的label</p></li><li><p>阈值：显著性水平+自由度（比类别数量少1）确定。</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LR-logloss</title>
      <link href="/2019/11/19/%E4%B8%A4%E7%A7%8Dlogloss/"/>
      <url>/2019/11/19/%E4%B8%A4%E7%A7%8Dlogloss/</url>
      
        <content type="html"><![CDATA[<ol><li>LR中，若0，1分类：<br>$$<br>\begin{array}{l}{\mathbb{P}(y=1 | z)=\sigma(z)=\frac{1}{1+e^{-z}}} \ {\mathbb{P}(y=0 | z)=1-\sigma(z)=\frac{1}{1+e^{z}}}\end{array}<br>$$<br>即：<br>$$<br>\mathbb{P}(y | z)=\sigma(z)^{y}(1-\sigma(z))^{1-y}<br>$$<br>极大似然：<br>$$<br>L(z)=\log \left(\prod_{i}^{m} \mathbb{P}\left(y_{i} | z_{i}\right)\right)=-\sum_{i}^{m} \log \left(\mathbb{P}\left(y_{i} | z_{i}\right)\right)=\sum_{i}^{m}-y_{i} z_{i}+\log \left(1+e^{z_{i}}\right)<br>$$<br>损失函数：<br>$$<br>l(z)=-\log \left(\prod_{i}^{m} \mathbb{P}\left(y_{i} | z_{i}\right)\right)=-\sum_{i}^{m} \log \left(\mathbb{P}\left(y_{i} | z_{i}\right)\right)=\sum_{i}^{m}-y_{i} z_{i}+\log \left(1+e^{z_{i}}\right)<br>$$</li></ol><ol start="2"><li><p>1，-1分类：<br>$$<br>\begin{aligned} \mathbb{P}(y | z) &amp;=\sigma(y z) \ &amp;=\frac{1}{1+e^{-y z}} \end{aligned}<br>$$<br>分开表示跟0，1分类是一样的</p><p>损失函数：<br>$$<br>L(z)=-\log \left(\prod_{j}^{m} \mathbb{P}\left(y_{j} | z_{j}\right)\right)=-\sum_{j}^{m} \log \left(\mathbb{P}\left(y_{j} | z_{j}\right)\right)=\sum_{j}^{m} \log \left(1+e^{-y_{j} z_{j}}\right)<br>$$</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表-反转链表</title>
      <link href="/2019/11/19/%E9%93%BE%E8%A1%A8-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/"/>
      <url>/2019/11/19/%E9%93%BE%E8%A1%A8-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="反转整个链表"><a href="#反转整个链表" class="headerlink" title="反转整个链表"></a>反转整个链表</h1><h2 id="递归"><a href="#递归" class="headerlink" title="递归"></a>递归</h2><ul><li>递归往上走的时候，每进一轮迭代，当前节点与之前节点的链接未变化，且head.next节点总是在已反转的链表末端，因此可以用head.next.next = head。</li><li>递归需要badcase</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseList(self, head: ListNode) -&gt; ListNode:</span><br><span class="line">        if head.next==None:</span><br><span class="line">            return head</span><br><span class="line">        last = reverseList(head.next)</span><br><span class="line">        head.next.next = head</span><br><span class="line">        head.next = null</span><br><span class="line">        return last</span><br></pre></td></tr></table></figure><h2 id="迭代"><a href="#迭代" class="headerlink" title="迭代"></a>迭代</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseList(self, head: ListNode) -&gt; ListNode:</span><br><span class="line">        curr = head</span><br><span class="line">        prev = None</span><br><span class="line">        while curr != None:</span><br><span class="line">            temp = curr.next</span><br><span class="line">            prev = curr</span><br><span class="line">            curr.next = prev</span><br><span class="line">            curr = temp</span><br><span class="line">        return prev</span><br></pre></td></tr></table></figure><h1 id="反转前n个链表"><a href="#反转前n个链表" class="headerlink" title="反转前n个链表"></a>反转前n个链表</h1><h2 id="递归-1"><a href="#递归-1" class="headerlink" title="递归"></a>递归</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseN(self, head: ListNode, n: int) -&gt; ListNode:</span><br><span class="line">        if n==1:</span><br><span class="line">            successor = head.next</span><br><span class="line">            return head</span><br><span class="line"></span><br><span class="line">        last = self.reverseN(head.next, n-1)</span><br><span class="line">        head.next.next = head</span><br><span class="line">        head.next = successor</span><br><span class="line">        </span><br><span class="line">        return last</span><br></pre></td></tr></table></figure><h1 id="反转一部分"><a href="#反转一部分" class="headerlink" title="反转一部分"></a>反转一部分</h1><h2 id="递归-使用前有函数"><a href="#递归-使用前有函数" class="headerlink" title="递归 使用前有函数"></a>递归 使用前有函数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseBetween(self, head: ListNode, m: int, n: int) -&gt; ListNode:</span><br><span class="line">        if m==1:</span><br><span class="line">            return self.reverseN(head, n)</span><br><span class="line">        head.next = self.reverseBetween(head.next, m-1, n-1)</span><br><span class="line">        return head</span><br></pre></td></tr></table></figure><h2 id="迭代-1"><a href="#迭代-1" class="headerlink" title="迭代"></a>迭代</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseBetween(self, head: ListNode, m: int, n: int) -&gt; ListNode:</span><br><span class="line">        dummy = ListNode(-1)</span><br><span class="line">        dummy.next = head</span><br><span class="line">        prev = dummy</span><br><span class="line">        for _ in range(m-1):</span><br><span class="line">            prev = prev.next</span><br><span class="line">        curr = prev.next</span><br><span class="line">        node = None</span><br><span class="line">        for _ in range(n-m+1):</span><br><span class="line">            temp = curr.next</span><br><span class="line">            curr.next = node</span><br><span class="line">            node = curr</span><br><span class="line">            curr = temp</span><br><span class="line"></span><br><span class="line">        prev.next.next = curr</span><br><span class="line">        prev.next = node</span><br><span class="line">        return dummy.next</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">class Solution:</span><br><span class="line">    def reverseBetween(self, head: ListNode, m: int, n: int) -&gt; ListNode:</span><br><span class="line">        dummy = ListNode(-1)</span><br><span class="line">        dummy.next = head</span><br><span class="line">        prev = dummy</span><br><span class="line">        for _ in range(m-1):</span><br><span class="line">            prev = prev.next</span><br><span class="line">        start = prev.next</span><br><span class="line">        tail = start.next</span><br><span class="line">        for _ in range(n-m):</span><br><span class="line">            start.next = tail.next</span><br><span class="line">            tail.next = start</span><br><span class="line">            prev.next = tail</span><br><span class="line">            tail = start.next</span><br><span class="line">        return dummy.next</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DS </tag>
            
            <tag> 链表 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN-手写字的识别</title>
      <link href="/2019/11/19/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E6%89%8B%E5%86%99%E5%AD%97%E7%9A%84%E8%AF%86%E5%88%AB)/"/>
      <url>/2019/11/19/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C(%E6%89%8B%E5%86%99%E5%AD%97%E7%9A%84%E8%AF%86%E5%88%AB)/</url>
      
        <content type="html"><![CDATA[<h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数据集：MNIST手写数字集</span><br><span class="line">训练集：<span class="number">42</span>,<span class="number">000</span>个<span class="number">0</span><span class="number">-9</span>手写数字的图像</span><br><span class="line">测试集：有<span class="number">28</span>,<span class="number">000</span>个无label样本</span><br><span class="line">每个图像的大小是<span class="number">28</span>×<span class="number">28</span>=<span class="number">784</span>个像素</span><br><span class="line">目标：使用卷积神经网络识别图像是什么数字</span><br></pre></td></tr></table></figure><h3 id="导入相关包"><a href="#导入相关包" class="headerlink" title="导入相关包"></a>导入相关包</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python的内置垃圾收集。用来删除一些变量，并收集必要的空间来保存RAM。</span></span><br><span class="line"><span class="keyword">import</span> gc </span><br><span class="line"><span class="comment"># 用来生成随机数。</span></span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd </span><br><span class="line"><span class="comment">#用来检查运行时间。</span></span><br><span class="line"><span class="keyword">import</span> time </span><br><span class="line"><span class="comment"># 在数据增强部分，我们使用圆周率旋转图像。</span></span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> pi </span><br><span class="line"><span class="comment"># 用Keras来构建我们的CNN模型。它使用TensorFlow作为后端。</span></span><br><span class="line"><span class="keyword">import</span> keras </span><br><span class="line"><span class="comment"># 绘制手写的数字图像。</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt </span><br><span class="line"><span class="comment"># 矩阵操作。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment"># 操作数据，比如加载和输出</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 用TensorFlow作为数据增强部分</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 用来建立学习速率衰减的模型</span></span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ReduceLROnPlateau, EarlyStopping</span><br><span class="line"><span class="comment"># 构建CNN所需要的一些基本构件。 </span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> (BatchNormalization, Conv2D, Dense, Dropout, Flatten,</span><br><span class="line">                          MaxPool2D, ReLU)</span><br><span class="line"><span class="comment"># 图像显示。</span></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="comment"># 将数据分解为训练和验证两部分。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure><pre><code>Using TensorFlow backend.</code></pre><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p><strong>导入数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Loading..."</span>)</span><br><span class="line">path = <span class="string">"E:/机器学习/Tensorflow学习/data/"</span></span><br><span class="line">data_train = pd.read_csv(path + <span class="string">"train.csv"</span>,engine=<span class="string">"python"</span>)</span><br><span class="line">data_test = pd.read_csv(path + <span class="string">"test.csv"</span>,engine=<span class="string">"python"</span>)</span><br><span class="line">print(<span class="string">"Done!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Loading...Done!</code></pre><p><strong>查看数据集的大小</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Training data: &#123;&#125; rows, &#123;&#125; columns."</span>.format(data_train.shape[<span class="number">0</span>], data_train.shape[<span class="number">1</span>]))</span><br><span class="line">print(<span class="string">"Test data: &#123;&#125; rows, &#123;&#125; columns."</span>.format(data_test.shape[<span class="number">0</span>], data_test.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>Training data: 42000 rows, 785 columns.Test data: 28000 rows, 784 columns.</code></pre><p>训练集有42000行，785列，其中包括784个像素和一个标签，标注了这张图片是什么数字。</p><p>测试数据有28000行，没有标签。</p><p><strong>数据集拆分成x（图像数据）和y（标签）</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = data_train.values[:, <span class="number">1</span>:]</span><br><span class="line">y_train = data_train.values[:, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_2d</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d numpy array. m*n data image.</span></span><br><span class="line"><span class="string">       return a 3d image data. m * height * width * channel."""</span></span><br><span class="line">    <span class="keyword">if</span> len(x.shape) == <span class="number">1</span>:</span><br><span class="line">        m = <span class="number">1</span></span><br><span class="line">        height = width = int(np.sqrt(x.shape[<span class="number">0</span>]))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        m = x.shape[<span class="number">0</span>]</span><br><span class="line">        height = width = int(np.sqrt(x.shape[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    x_2d = np.reshape(x, (m, height, width, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> x_2d</span><br></pre></td></tr></table></figure><p><strong>查看图像</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_display = convert_2d(data_train.values[<span class="number">0</span>, <span class="number">1</span>:])</span><br><span class="line">plt.imshow(x_display.squeeze())</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.image.AxesImage at 0x22b013e7780&gt;</code></pre><p><img src="output_14_1.png" alt="png"></p><h3 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h3><p>在这里，我们直接研究数据增强。<br>当您没有足够的数据或想要扩展数据以提高性能时，数据增强是一种非常有用的技术。<br>在这场比赛中，数据增强基本上是指在不损害图像可识别性的前提下，对图像进行切割、旋转和缩放。<br>这里我使用了缩放、平移、白噪声和旋转。<br>随着数据的增加，您可以预期1-2%的准确性提高。</p><p><strong>放大</strong></p><p>使用crop_image函数来裁剪围绕中心的图像的一部分，调整其大小并将其保存为增强数据。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crop_image</span><span class="params">(x, y, min_scale)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d(m*n) numpy array. 1-dimension image data;</span></span><br><span class="line"><span class="string">       y: 1d numpy array. The ground truth label;</span></span><br><span class="line"><span class="string">       min_scale: float. The minimum scale for cropping.</span></span><br><span class="line"><span class="string">       return zoomed images.</span></span><br><span class="line"><span class="string">    # 该函数对图像进行裁剪，放大裁剪后的部分，并将其作为增强数据"""</span></span><br><span class="line">    <span class="comment"># 将数据转换为二维图像。图像应该是一个m*h*w*c数字数组。</span></span><br><span class="line">    images = convert_2d(x)</span><br><span class="line">    <span class="comment"># m是图像的个数。由于这是从0到255的灰度图像，所以它只有一个通道。</span></span><br><span class="line">    m, height, width, channel = images.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 原始图像的tf张量</span></span><br><span class="line">    img_tensor = tf.placeholder(tf.int32, [<span class="number">1</span>, height, width, channel])</span><br><span class="line">    <span class="comment"># tf tensor for 4 coordinates for corners of the cropped image</span></span><br><span class="line">    box_tensor = tf.placeholder(tf.float32, [<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line">    box_idx = [<span class="number">0</span>]</span><br><span class="line">    crop_size = np.array([height, width])</span><br><span class="line">    <span class="comment"># 裁剪并调整图像张量</span></span><br><span class="line">    cropped_img_tensor = tf.image.crop_and_resize(img_tensor, box_tensor, box_idx, crop_size)</span><br><span class="line">    <span class="comment"># numpy array for the cropped image</span></span><br><span class="line">    cropped_img = np.zeros((m, height, width, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># randomly select a scale between [min_scale, min(min_scale + 0.05, 1)]</span></span><br><span class="line">            rand_scale = np.random.randint(min_scale * <span class="number">100</span>, np.minimum(min_scale * <span class="number">100</span> + <span class="number">5</span>, <span class="number">100</span>)) / <span class="number">100</span></span><br><span class="line">            <span class="comment"># calculate the 4 coordinates</span></span><br><span class="line">            x1 = y1 = <span class="number">0.5</span> - <span class="number">0.5</span> * rand_scale</span><br><span class="line">            x2 = y2 = <span class="number">0.5</span> + <span class="number">0.5</span> * rand_scale</span><br><span class="line">            <span class="comment"># lay down the cropping area</span></span><br><span class="line">            box = np.reshape(np.array([y1, x1, y2, x2]), (<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">            <span class="comment"># save the cropped image</span></span><br><span class="line">            cropped_img[i:i + <span class="number">1</span>, :, :, :] = sess.run(cropped_img_tensor, feed_dict=&#123;img_tensor: images[i:i + <span class="number">1</span>], box_tensor: box&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># flat the 2d image</span></span><br><span class="line">    cropped_img = np.reshape(cropped_img, (m, <span class="number">-1</span>))</span><br><span class="line">    cropped_img = np.concatenate((y.reshape((<span class="number">-1</span>, <span class="number">1</span>)), cropped_img), axis=<span class="number">1</span>).astype(int)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> cropped_img</span><br></pre></td></tr></table></figure><p><strong>平移</strong> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(x, y, dist)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d(m*n) numpy array. 1-dimension image data;</span></span><br><span class="line"><span class="string">       y: 1d numpy array. The ground truth label;</span></span><br><span class="line"><span class="string">       dist: float. Percentage of height/width to shift.</span></span><br><span class="line"><span class="string">       return translated images.</span></span><br><span class="line"><span class="string">       这个函数将图像移动到4个不同的方向。</span></span><br><span class="line"><span class="string">       裁剪图像的一部分，移动，用0填充左边的部分"""</span></span><br><span class="line">    <span class="comment"># 将一维图像数据转换为m*h*w*c数组</span></span><br><span class="line">    images = convert_2d(x)</span><br><span class="line">    m, height, width, channel = images.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># set 4 groups of anchors. The first 4 int in a certain group lay down the area we crop.</span></span><br><span class="line">    <span class="comment"># The last 4 sets the area to be moved to. E.g.,</span></span><br><span class="line">    <span class="comment"># new_img[new_top:new_bottom, new_left:new_right] = img[top:bottom, left:right]</span></span><br><span class="line">    anchors = []</span><br><span class="line">    anchors.append((<span class="number">0</span>, height, int(dist * width), width, <span class="number">0</span>, height, <span class="number">0</span>, width - int(dist * width)))</span><br><span class="line">    anchors.append((<span class="number">0</span>, height, <span class="number">0</span>, width - int(dist * width), <span class="number">0</span>, height, int(dist * width), width))</span><br><span class="line">    anchors.append((int(dist * height), height, <span class="number">0</span>, width, <span class="number">0</span>, height - int(dist * height), <span class="number">0</span>, width))</span><br><span class="line">    anchors.append((<span class="number">0</span>, height - int(dist * height), <span class="number">0</span>, width, int(dist * height), height, <span class="number">0</span>, width))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># new_images: d*m*h*w*c array. The first dimension is the 4 directions.</span></span><br><span class="line">    new_images = np.zeros((<span class="number">4</span>, m, height, width, channel))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        <span class="comment"># shift the image</span></span><br><span class="line">        top, bottom, left, right, new_top, new_bottom, new_left, new_right = anchors[i]</span><br><span class="line">        new_images[i, :, new_top:new_bottom, new_left:new_right, :] = images[:, top:bottom, left:right, :]</span><br><span class="line">    </span><br><span class="line">    new_images = np.reshape(new_images, (<span class="number">4</span> * m, <span class="number">-1</span>))</span><br><span class="line">    y = np.tile(y, (<span class="number">4</span>, <span class="number">1</span>)).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">    new_images = np.concatenate((y, new_images), axis=<span class="number">1</span>).astype(int)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> new_images</span><br></pre></td></tr></table></figure><p><strong>添加白噪声</strong></p><p>现在我们给图像添加一些白噪声。我们随机选取一些像素，用均匀分布的噪声代替它们。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_noise</span><span class="params">(x, y, noise_lvl)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d(m*n) numpy array. 1-dimension image data;</span></span><br><span class="line"><span class="string">       y: 1d numpy array. The ground truth label;</span></span><br><span class="line"><span class="string">       noise_lvl: float. Percentage of pixels to add noise in.</span></span><br><span class="line"><span class="string">       return images with white noise.</span></span><br><span class="line"><span class="string">       This function randomly picks some pixels and replace them with noise."""</span></span><br><span class="line">    m, n = x.shape</span><br><span class="line">    <span class="comment"># calculate the # of pixels to add noise in</span></span><br><span class="line">    noise_num = int(noise_lvl * n)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># generate n random numbers, sort it and choose the first noise_num indices</span></span><br><span class="line">        <span class="comment"># which equals to generate random numbers w/o replacement</span></span><br><span class="line">        noise_idx = np.random.randint(<span class="number">0</span>, n, n).argsort()[:noise_num]</span><br><span class="line">        <span class="comment"># replace the chosen pixels with noise from 0 to 255</span></span><br><span class="line">        x[i, noise_idx] = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, noise_num)</span><br><span class="line"></span><br><span class="line">    noisy_data = np.concatenate((y.reshape((<span class="number">-1</span>, <span class="number">1</span>)), x), axis=<span class="number">1</span>).astype(<span class="string">"int"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> noisy_data</span><br></pre></td></tr></table></figure><p><strong>旋转</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rotate_image</span><span class="params">(x, y, max_angle)</span>:</span></span><br><span class="line">    <span class="string">"""x: 2d(m*n) numpy array. 1-dimension image data;</span></span><br><span class="line"><span class="string">       y: 1d numpy array. The ground truth label;</span></span><br><span class="line"><span class="string">       max_angle: int. The maximum degree for rotation.</span></span><br><span class="line"><span class="string">       return rotated images.</span></span><br><span class="line"><span class="string">       This function rotates the image for some random degrees(0.5 to 1 * max_angle degree)."""</span></span><br><span class="line">    images = convert_2d(x)</span><br><span class="line">    m, height, width, channel = images.shape</span><br><span class="line">    </span><br><span class="line">    img_tensor = tf.placeholder(tf.float32, [m, height, width, channel])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># half of the images are rotated clockwise. The other half counter-clockwise</span></span><br><span class="line">    <span class="comment"># positive angle: [max/2, max]</span></span><br><span class="line">    <span class="comment"># negative angle: [360-max/2, 360-max]</span></span><br><span class="line">    rand_angle_pos = np.random.randint(max_angle / <span class="number">2</span>, max_angle, int(m / <span class="number">2</span>))</span><br><span class="line">    rand_angle_neg = np.random.randint(-max_angle, -max_angle / <span class="number">2</span>, m - int(m / <span class="number">2</span>)) + <span class="number">360</span></span><br><span class="line">    rand_angle = np.transpose(np.hstack((rand_angle_pos, rand_angle_neg)))</span><br><span class="line">    np.random.shuffle(rand_angle)</span><br><span class="line">    <span class="comment"># convert the degree to radian</span></span><br><span class="line">    rand_angle = rand_angle / <span class="number">180</span> * pi</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rotate the images</span></span><br><span class="line">    rotated_img_tensor = tf.contrib.image.rotate(img_tensor, rand_angle)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        rotated_imgs = sess.run(rotated_img_tensor, feed_dict=&#123;img_tensor: images&#125;)</span><br><span class="line">    </span><br><span class="line">    rotated_imgs = np.reshape(rotated_imgs, (m, <span class="number">-1</span>))</span><br><span class="line">    rotated_imgs = np.concatenate((y.reshape((<span class="number">-1</span>, <span class="number">1</span>)), rotated_imgs), axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> rotated_imgs</span><br></pre></td></tr></table></figure><p><strong>合并</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">start = time.clock()</span><br><span class="line">print(<span class="string">"Augment the data..."</span>)</span><br><span class="line">cropped_imgs = crop_image(x_train, y_train, <span class="number">0.9</span>)</span><br><span class="line">translated_imgs = translate(x_train, y_train, <span class="number">0.1</span>)</span><br><span class="line">noisy_imgs = add_noise(x_train, y_train, <span class="number">0.1</span>)</span><br><span class="line">rotated_imgs = rotate_image(x_train, y_train, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">data_train = np.vstack((data_train, cropped_imgs, translated_imgs, noisy_imgs, rotated_imgs))</span><br><span class="line">np.random.shuffle(data_train)</span><br><span class="line">print(<span class="string">"Done!"</span>)</span><br><span class="line">time_used = int(time.clock() - start)</span><br><span class="line">print(<span class="string">"Time used: &#123;&#125;s."</span>.format(time_used))</span><br></pre></td></tr></table></figure><pre><code>G:\Anaconda\lib\site-packages\ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead  &quot;&quot;&quot;Entry point for launching an IPython kernel.Augment the data...WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.For more information, please see:  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md  * https://github.com/tensorflow/addonsIf you depend on functionality not listed there, please file an issue.Done!Time used: 26s.G:\Anaconda\lib\site-packages\ipykernel_launcher.py:11: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead  # This is added back by InteractiveShellApp.init_path()</code></pre><h3 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h3><p><strong>检查数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_train = data_train[:, <span class="number">1</span>:]</span><br><span class="line">y_train = data_train[:, <span class="number">0</span>]</span><br><span class="line">x_test = data_test.values</span><br><span class="line">print(<span class="string">"Augmented training data: &#123;&#125; rows, &#123;&#125; columns."</span>.format(data_train.shape[<span class="number">0</span>], data_train.shape[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><pre><code>Augmented training data: 336000 rows, 785 columns.</code></pre><p>使用数据增强之后的训练数据总共有33万6千行，是原来的8倍。</p><p><strong>向量转化为一个矩阵</strong></p><p>因为CNN接受的是输入是二维的图像，我们需要将向量转化为一个矩阵<br>格式：$m(图像数量)×h(图像高度)×w(图像宽度)×c(图像通道数量)$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = convert_2d(x_train)</span><br><span class="line">x_test = convert_2d(x_test)</span><br></pre></td></tr></table></figure><p><strong>将类别型数据转换成哑变量</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_classes = <span class="number">10</span></span><br><span class="line">y_train = keras.utils.to_categorical(y_train, num_classes)</span><br></pre></td></tr></table></figure><p>为了加快CNN优化速度，缩小像素值的范围。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_train = x_train / <span class="number">255</span></span><br><span class="line">x_test = x_test / <span class="number">255</span></span><br></pre></td></tr></table></figure><p><strong>划分训练集，验证集</strong></p><p>为了验证模型的好坏，用sklearn提供的一个函数来将数据按照9:1进行分割，90%为训练集，10%为验证集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># generate a random seed for train-test-split</span></span><br><span class="line">seed = np.random.randint(<span class="number">1</span>, <span class="number">100</span>)</span><br><span class="line">x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=<span class="number">0.1</span>, random_state=seed)</span><br></pre></td></tr></table></figure><p><strong>清理内存</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">del</span> data_train</span><br><span class="line"><span class="keyword">del</span> data_test</span><br><span class="line">gc.collect()</span><br></pre></td></tr></table></figure><pre><code>69</code></pre><h3 id="搭建CNN模型"><a href="#搭建CNN模型" class="headerlink" title="搭建CNN模型"></a>搭建CNN模型</h3><p>一个普通的CNN通常包括三种类型的层，卷积层，池化层和全连接层。<br>我还在模型中添加了标准化层和dropout层。</p><ul><li><p>这里使用了5×5的卷积核，而不是3×3的。5×5的卷积核感受野更大，效果更好。</p></li><li><p>这里的批量归一化放在了ReLU激活函数之后，当然也可以放在激活函数之前。</p></li><li><p>Dropout使用了0.2的drop概率，意味着在Dropout层的输入中20%的像素点会被重置为0。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个卷积层的信道数。 </span></span><br><span class="line">filters = (<span class="number">32</span>, <span class="number">32</span>, <span class="number">64</span>, <span class="number">64</span>)</span><br><span class="line"><span class="comment"># 每个conv层使用一个5x5内核</span></span><br><span class="line">kernel = (<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 在Dropout层的输入中20%的像素点会被重置为0。</span></span><br><span class="line">drop_prob = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">model = keras.models.Sequential()</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters[<span class="number">0</span>], kernel, padding=<span class="string">"same"</span>, input_shape=(<span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>),</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(Conv2D(filters[<span class="number">0</span>], kernel, padding=<span class="string">"same"</span>,</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters[<span class="number">1</span>], kernel, padding=<span class="string">"same"</span>,</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters[<span class="number">2</span>], kernel, padding=<span class="string">"same"</span>,</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line"></span><br><span class="line">model.add(Conv2D(filters[<span class="number">3</span>], kernel, padding=<span class="string">"same"</span>,</span><br><span class="line">                 kernel_initializer=keras.initializers.he_normal()))</span><br><span class="line">model.add(BatchNormalization())</span><br><span class="line">model.add(ReLU())</span><br><span class="line">model.add(MaxPool2D())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line"></span><br><span class="line"><span class="comment"># several fully-connected layers after the conv layers</span></span><br><span class="line">model.add(Flatten())</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">"relu"</span>))</span><br><span class="line">model.add(Dropout(drop_prob))</span><br><span class="line">model.add(Dense(num_classes, activation=<span class="string">"softmax"</span>))</span><br><span class="line"><span class="comment"># use the Adam optimizer to accelerate convergence</span></span><br><span class="line">model.compile(keras.optimizers.Adam(), <span class="string">"categorical_crossentropy"</span>, metrics=[<span class="string">"accuracy"</span>])</span><br></pre></td></tr></table></figure><pre><code>WARNING:tensorflow:From G:\Anaconda\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.Instructions for updating:Colocations handled automatically by placer.WARNING:tensorflow:From G:\Anaconda\lib\site-packages\keras\backend\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.Instructions for updating:Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.</code></pre><p><strong>查看模型架构</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure><pre><code>_________________________________________________________________Layer (type)                 Output Shape              Param #   =================================================================conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       _________________________________________________________________batch_normalization_1 (Batch (None, 28, 28, 32)        128       _________________________________________________________________re_lu_1 (ReLU)               (None, 28, 28, 32)        0         _________________________________________________________________conv2d_2 (Conv2D)            (None, 28, 28, 32)        25632     _________________________________________________________________batch_normalization_2 (Batch (None, 28, 28, 32)        128       _________________________________________________________________re_lu_2 (ReLU)               (None, 28, 28, 32)        0         _________________________________________________________________max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         _________________________________________________________________dropout_1 (Dropout)          (None, 14, 14, 32)        0         _________________________________________________________________conv2d_3 (Conv2D)            (None, 14, 14, 32)        25632     _________________________________________________________________batch_normalization_3 (Batch (None, 14, 14, 32)        128       _________________________________________________________________re_lu_3 (ReLU)               (None, 14, 14, 32)        0         _________________________________________________________________max_pooling2d_2 (MaxPooling2 (None, 7, 7, 32)          0         _________________________________________________________________dropout_2 (Dropout)          (None, 7, 7, 32)          0         _________________________________________________________________conv2d_4 (Conv2D)            (None, 7, 7, 64)          51264     _________________________________________________________________batch_normalization_4 (Batch (None, 7, 7, 64)          256       _________________________________________________________________re_lu_4 (ReLU)               (None, 7, 7, 64)          0         _________________________________________________________________max_pooling2d_3 (MaxPooling2 (None, 3, 3, 64)          0         _________________________________________________________________dropout_3 (Dropout)          (None, 3, 3, 64)          0         _________________________________________________________________conv2d_5 (Conv2D)            (None, 3, 3, 64)          102464    _________________________________________________________________batch_normalization_5 (Batch (None, 3, 3, 64)          256       _________________________________________________________________re_lu_5 (ReLU)               (None, 3, 3, 64)          0         _________________________________________________________________max_pooling2d_4 (MaxPooling2 (None, 1, 1, 64)          0         _________________________________________________________________dropout_4 (Dropout)          (None, 1, 1, 64)          0         _________________________________________________________________flatten_1 (Flatten)          (None, 64)                0         _________________________________________________________________dropout_5 (Dropout)          (None, 64)                0         _________________________________________________________________dense_1 (Dense)              (None, 128)               8320      _________________________________________________________________dropout_6 (Dropout)          (None, 128)               0         _________________________________________________________________dense_2 (Dense)              (None, 10)                1290      =================================================================Total params: 216,330Trainable params: 215,882Non-trainable params: 448_________________________________________________________________</code></pre><p>The list above is the structure of my CNN model. It goes:</p><ul><li>(Conv-ReLU-BatchNormalization-MaxPooling-Dropout) x 4;</li><li><p>3 fully-connected(dense) layers with 1 dropout layer. Dense(64)-Dense(128)-Dropout-Dense(with softmax activation).</p></li><li><p>In CNN people often use 3x3 or 5x5 kernel. I found that with a 5x5 kernel, the model’s accuracy improved about 0.125%, which is quite a lot when you pass 99% threshold.</p></li><li>Convolutional layers and max pooling layers can extract some high-level traits from the pixels. With the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks" target="_blank" rel="noopener">ReLU</a>) unit the and max pooling, we also add non-linearity into the network;</li><li>Batch normalization helps the network converge faster since it keeps the input of every layer at the same scale;</li><li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#Dropout" target="_blank" rel="noopener">Dropout</a> layers help us prevent overfitting by randomly drop some of the input units. With dropout our model won’t overfit to some specific extreme data or some noisy pixels;</li><li>The <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam" target="_blank" rel="noopener">Adam optimizer</a> also accelerates the optimization. Usually when the dataset is too large, we use mini-batch gradient descent or stochastic gradient descent to save some training time. The randomness in MBGD or SGD means that the steps towards the optimum are zig-zag rather than straight forward. Adam, or Adaptive Moment Estimation, uses exponential moving average on the gradients and the secend moment of gradients to make the steps straight and in turn accelerate the optimization.</li></ul><h3 id="训练CNN"><a href="#训练CNN" class="headerlink" title="训练CNN"></a>训练CNN</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># number of epochs we run</span></span><br><span class="line">iters = <span class="number">100</span></span><br><span class="line"><span class="comment"># batch size. Number of images we train before we take one step in MBGD.</span></span><br><span class="line">batch_size = <span class="number">1024</span></span><br></pre></td></tr></table></figure><p>当我们接近最佳状态时，我们需要降低学习速度以防止过度学习。高学习率会使我们远离最佳状态。因此，当验证数据的准确性不再提高时，我将这个学习率衰减设置为降低它。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># monitor: :要监视的数量。当它不再显著改善时，我们就降低了学习速度</span></span><br><span class="line"><span class="comment"># factor: 新学习率=旧学习率 * factor</span></span><br><span class="line"><span class="comment"># patience:在降低学习速度之前，我们要等待的时间</span></span><br><span class="line"><span class="comment"># verbose: 是否显示信息</span></span><br><span class="line"><span class="comment"># min_lr: 最小的学习率</span></span><br><span class="line"></span><br><span class="line">lr_decay = ReduceLROnPlateau(monitor=<span class="string">"val_acc"</span>, factor=<span class="number">0.5</span>, patience=<span class="number">3</span>, verbose=<span class="number">1</span>, min_lr=<span class="number">1e-5</span>)</span><br><span class="line"><span class="comment"># 如果模型在验证数据上没有得到任何改善，可以设置早期停止，以防止过度拟合，并节省一些时间。当监控量没有提高时，提前停止训练。</span></span><br><span class="line">early_stopping = EarlyStopping(monitor=<span class="string">"val_acc"</span>, patience=<span class="number">7</span>, verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>训练模型</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Training model..."</span>)</span><br><span class="line">fit_params = &#123;</span><br><span class="line">    <span class="string">"batch_size"</span>: batch_size,</span><br><span class="line">    <span class="string">"epochs"</span>: iters,</span><br><span class="line">    <span class="string">"verbose"</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">"callbacks"</span>: [lr_decay, early_stopping],</span><br><span class="line">    <span class="string">"validation_data"</span>: (x_dev, y_dev)     <span class="comment"># data for monitoring the model accuracy</span></span><br><span class="line">&#125;</span><br><span class="line">model.fit(x_train, y_train, **fit_params)</span><br><span class="line">print(<span class="string">"Done!"</span>)</span><br></pre></td></tr></table></figure><pre><code>Training model...WARNING:tensorflow:From G:\Anaconda\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.Instructions for updating:Use tf.cast instead.Train on 302400 samples, validate on 33600 samplesEpoch 1/100  3072/302400 [..............................] - ETA: 32:43 - loss: 2.6548 - acc: 0.1156</code></pre><h3 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.evaluate(x_dev, y_dev)</span><br></pre></td></tr></table></figure><pre><code>33600/33600 [==============================] - 3s 75us/step[0.0018058670724439621, 0.9994047619047619]</code></pre><p>evaluate这个方法会输出两个值，第一个是当期的损失函数值，第二个是模型的准确率。我们可以看到，模型的准确率在验证集上达到了99.84%！</p><h3 id="输出预测"><a href="#输出预测" class="headerlink" title="输出预测"></a>输出预测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(x_test, batch_size=batch_size)</span><br><span class="line">y_pred = np.argmax(y_pred, axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">idx = np.reshape(np.arange(<span class="number">1</span>, len(y_pred) + <span class="number">1</span>), (len(y_pred), <span class="number">-1</span>))</span><br><span class="line">y_pred = np.hstack((idx, y_pred))</span><br><span class="line">y_pred = pd.DataFrame(y_pred, columns=[<span class="string">'ImageId'</span>, <span class="string">'Label'</span>])</span><br><span class="line">y_pred.to_csv(<span class="string">'y_pred.csv'</span>, index=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基向量与坐标变换</title>
      <link href="/2019/11/19/%E5%9F%BA%E5%90%91%E9%87%8F%E4%B8%8E%E5%9D%90%E6%A0%87%E5%8F%98%E6%8D%A2/"/>
      <url>/2019/11/19/%E5%9F%BA%E5%90%91%E9%87%8F%E4%B8%8E%E5%9D%90%E6%A0%87%E5%8F%98%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<ol><li><p>基与维数</p><ul><li>F：一个数集</li><li>V：F上的非空子集，相当于一个向量空间。{α1，α2…αm}是V中的一个有序向量组。</li></ul><p>基：{α1，α2…αm}线性无关且V中的向量都可以用{α1，α2…αm}，{α1，α2…αm}为V的一组基。V = L(α1，α2…αm)</p><p>维数：基中向量的个数。</p></li><li><p>坐标：向量关于基的坐标<br>$$<br>\alpha=x_{1} \alpha 1+x_{2} \alpha 2+\cdots, x_{m} \alpha m=\left(\alpha_{1}, \alpha_{2}, \cdots, \alpha_{m}\right)\left(\begin{array}{c}{x_{1}} \ {x_{2}} \ {\vdots} \ {x_{m}}\end{array}\right)<br>$$<br>(X1…xm)是向量α在基下的坐标</p></li><li><p>基变换与坐标变换</p><ul><li><p>基变换</p><p>(β1… βm) = A(α1… αm) </p><p>A为基α到基β的过渡矩阵</p></li><li><p>坐标变换</p><p>同一向量关于一个基的坐标x到关于另一个基的坐标y的变换</p></li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 线性代数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>隐马尔科夫</title>
      <link href="/2019/11/19/HMM%EF%BC%88%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%EF%BC%89/"/>
      <url>/2019/11/19/HMM%EF%BC%88%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h2 id="最大熵模型"><a href="#最大熵模型" class="headerlink" title="最大熵模型"></a>最大熵模型</h2><ul><li><p>出发点：保留所有不确定性，将风险降到最小。</p></li><li><p>条件熵：$ H(Y|X) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(y_i|x_i) = \sum\limits_{j=1}^{n}p(x_j)H(Y|x_j) $</p></li><li><p>最大熵模型：假设分类模型为一个条件概率分布P(Y|X)，X为特征，Y为输出，给定(X,Y)，用最大熵模型选择一个最好的分类模型。</p><p><a href="https://www.cnblogs.com/pinard/p/6093948.html" target="_blank" rel="noopener">Doc</a></p></li></ul><h2 id="隐马尔可夫"><a href="#隐马尔可夫" class="headerlink" title="隐马尔可夫"></a>隐马尔可夫</h2><ul><li>要解决的问题：1）基于序列；2）问题中有两类数据，一类是可以观测到的（观测序列），另一类是不能观测的（状态序列）。</li></ul><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul><li><p>假设Q={q1,q2…qN}是<strong>所有可能</strong>的隐藏状态的集合，V={v1,v2…vM}是<strong>所有可能</strong>的观测状态的集合。</p></li><li><p>对于一个长度为T的序列，I={i1,i2…iT}为状态序列，O={o1,o2…oT}为观察序列。其中i∈q, o∈v</p></li><li><p>假设：</p><ul><li><p>齐次马尔科夫链假设。即任意时刻的隐藏状态只依赖于它前一个隐藏状态。如果在时刻tt的隐藏状态是it=qi,在时刻t+1t+1的隐藏状态是it+1=qj, 则从时刻t到时刻t+1的HMM状态转移概率aij可以表示为：</p><p>$ a_{ij} = P(i_{t+1} = q_j | i_t= q_i) $</p><p>aij的状态转移矩阵A=「aij」NxN</p></li><li><p>观察独立性假设。任意时刻的观察状态只依赖于当前时刻的隐藏状态。如果在时刻t的隐藏状态是it=qj, 而对应的观察状态为ot=vk, 则该时刻观察状态vk在隐藏状态qj下生成的概率为bj(k),满足：</p><p>$ b_j(k) = P(o_t = v_k | i_t= q_j) $</p><p>这样bj(k)可以组成观测状态生成的概率矩阵B:</p><p>$ B = \Big [b_j(k) \Big ]_{N \times M} $</p></li><li><p>此外，需要t=1时的隐藏状态概率分布II：</p><p>$ B = \Big [b_j(k) \Big ]_{N \times M} $</p></li></ul></li><li><p>一个HMM模型，可以由隐藏状态初始概率分布II，A、B状态转移矩阵和状态概率矩阵决定：</p><p>λ=(A,B,II)</p></li></ul><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol><li>评估观察序列概率。即给定模型λ=(A,B,II)和观测序列O={o1,o2,…oT}，计算在模型λ下观测序列O出现的概率P(O|λ)。这个问题的求解需要用到前向后向算法。</li><li>模型参数学习问题。即给定观测序列O={o1,o2,…oT}，估计模型λ=(A,B,II)的参数，使该模型下观测序列的条件概率P(O|λ)最大。这个问题的求解需要用到基于EM算法的鲍姆-韦尔奇算法， 我们在这个系列的第三篇会详细讲解。这个问题是HMM模型三个问题中最复杂的。</li><li>预测问题，也称为解码问题。即给定模型λ=(A,B,II)和观测序列O={o1,o2,…oT}，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的维特比算法，我们在这个系列的第四篇会详细讲解。这个问题是HMM模型三个问题中复杂度居中的算法。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KS-GINI</title>
      <link href="/2019/11/19/K-S%E3%80%81GINI/"/>
      <url>/2019/11/19/K-S%E3%80%81GINI/</url>
      
        <content type="html"><![CDATA[<p>##<strong>都是用来衡量模型区分度的</strong></p><h4 id="K-S"><a href="#K-S" class="headerlink" title="K-S"></a>K-S</h4><ul><li><p>KS(Kolmogorov-Smirnov)：KS用于模型风险区分能力进行评估，<br>指标衡量的是好坏样本累计分部之间的差值。<br>好坏样本累计差异越大，KS指标越大，那么模型的风险区分能力越强。</p></li><li><p>KS的计算步骤如下： </p><ol><li><p>计算每个评分区间的好坏账户数。 </p></li><li><p>计算每个评分区间的累计好账户数占总好账户数比率(good%)和累计坏账户数占总坏账户数比率(bad%)。 </p></li><li><p>计算每个评分区间累计坏账户占比与累计好账户占比差的绝对值（累计good%-累计bad%），然后对这些绝对值取最大值即得此评分卡的K-S值。</p></li></ol></li></ul><pre><code><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ks</span><span class="params">(df, y_true, y_pre, num=<span class="number">10</span>, good=<span class="number">0</span>, bad=<span class="number">1</span>)</span>:</span></span><br><span class="line">  <span class="string">'''</span></span><br><span class="line"><span class="string">  df为包含真实label和预测的概率值的DataFrame</span></span><br><span class="line"><span class="string">  y_true</span></span><br><span class="line"><span class="string">  '''</span></span><br><span class="line">    <span class="comment"># 1.将数据从小到大平均分成num组</span></span><br><span class="line">    df_ks = df.sort_values(y_pre).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line">    df_ks[<span class="string">'rank'</span>] = np.floor((df_ks.index / len(df_ks) * num) + <span class="number">1</span>)</span><br><span class="line">    df_ks[<span class="string">'set_1'</span>] = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 2.统计结果</span></span><br><span class="line">    result_ks = pd.DataFrame()</span><br><span class="line">    result_ks[<span class="string">'group_sum'</span>] = df_ks.groupby(<span class="string">'rank'</span>)[<span class="string">'set_1'</span>].sum()</span><br><span class="line">    result_ks[<span class="string">'group_min'</span>] = df_ks.groupby(<span class="string">'rank'</span>)[y_pre].min()</span><br><span class="line">    result_ks[<span class="string">'group_max'</span>] = df_ks.groupby(<span class="string">'rank'</span>)[y_pre].max()</span><br><span class="line">    result_ks[<span class="string">'group_mean'</span>] = df_ks.groupby(<span class="string">'rank'</span>)[y_pre].mean()</span><br><span class="line">    <span class="comment"># 3.最后一行添加total汇总数据</span></span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'group_sum'</span>] = df_ks[<span class="string">'set_1'</span>].sum()</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'group_min'</span>] = df_ks[y_pre].min()</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'group_max'</span>] = df_ks[y_pre].max()</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'group_mean'</span>] = df_ks[y_pre].mean()</span><br><span class="line">    <span class="comment"># 4.好用户统计</span></span><br><span class="line">    result_ks[<span class="string">'good_sum'</span>] = df_ks[df_ks[y_true] == good].groupby(<span class="string">'rank'</span>)[<span class="string">'set_1'</span>].sum()</span><br><span class="line">    result_ks.good_sum.replace(np.nan, <span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'good_sum'</span>] = result_ks[<span class="string">'good_sum'</span>].sum()</span><br><span class="line">    result_ks[<span class="string">'good_percent'</span>] = result_ks[<span class="string">'good_sum'</span>] / result_ks.loc[<span class="string">'total'</span>, <span class="string">'good_sum'</span>]</span><br><span class="line">    result_ks[<span class="string">'good_percent_cum'</span>] = result_ks[<span class="string">'good_sum'</span>].cumsum() / result_ks.loc[<span class="string">'total'</span>, <span class="string">'good_sum'</span>]</span><br><span class="line">    <span class="comment"># 5.坏用户统计</span></span><br><span class="line">    result_ks[<span class="string">'bad_sum'</span>] = df_ks[df_ks[y_true] == bad].groupby(<span class="string">'rank'</span>)[<span class="string">'set_1'</span>].sum()</span><br><span class="line">    result_ks.bad_sum.replace(np.nan, <span class="number">0</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'bad_sum'</span>] = result_ks[<span class="string">'bad_sum'</span>].sum()</span><br><span class="line">    result_ks[<span class="string">'bad_percent'</span>] = result_ks[<span class="string">'bad_sum'</span>] / result_ks.loc[<span class="string">'total'</span>, <span class="string">'bad_sum'</span>]</span><br><span class="line">    result_ks[<span class="string">'bad_percent_cum'</span>] = result_ks[<span class="string">'bad_sum'</span>].cumsum() / result_ks.loc[<span class="string">'total'</span>, <span class="string">'bad_sum'</span>]</span><br><span class="line">    <span class="comment"># 6.计算ks值</span></span><br><span class="line">    result_ks[<span class="string">'diff'</span>] = result_ks[<span class="string">'bad_percent_cum'</span>] - result_ks[<span class="string">'good_percent_cum'</span>]</span><br><span class="line">    <span class="comment"># 7.更新最后一行total的数据</span></span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'bad_percent_cum'</span>] = np.nan</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'good_percent_cum'</span>] = np.nan</span><br><span class="line">    result_ks.loc[<span class="string">'total'</span>, <span class="string">'diff'</span>] = result_ks[<span class="string">'diff'</span>].max()</span><br><span class="line">result_ks = result_ks.reset_index()</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> result_ks</span><br></pre></td></tr></table></figure></code></pre><h4 id="GINI"><a href="#GINI" class="headerlink" title="GINI"></a>GINI</h4><ul><li>GINI统计值衡量坏账户数在好账户数上的的累积分布与随机分布曲线之间的面积，好账户与坏账户分布之间的差异越大，GINI指标越高，表明模型的风险区分能力越强。</li><li>GINI系数的计算步骤如下： <ol><li>计算每个评分区间的好坏账户数。 </li><li>计算每个评分区间的累计好账户数占总好账户数比率（累计good%）和累计坏账户数占总坏账户数比率(累计bad%)。 </li><li>按照累计好账户占比和累计坏账户占比得出下图所示曲线ADC。 </li><li>计算出图中阴影部分面积，阴影面积占直角三角形ABC面积的百分比，即为GINI系数。</li></ol></li></ul><p><img src="https://img-blog.csdn.net/20171012171836445?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdTAxMzQyMTYyOQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DS绪论</title>
      <link href="/2019/11/19/DS-%E7%BB%AA%E8%AE%BA/"/>
      <url>/2019/11/19/DS-%E7%BB%AA%E8%AE%BA/</url>
      
        <content type="html"><![CDATA[<ul><li><p>时间复杂度：<br>bigO bigΩ bigθ</p><p>分别为上、下、中</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> DS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/11/19/7%E6%9C%8824%E6%97%A5/"/>
      <url>/2019/11/19/7%E6%9C%8824%E6%97%A5/</url>
      
        <content type="html"><![CDATA[<h2 id="与滴滴金融的会议"><a href="#与滴滴金融的会议" class="headerlink" title="与滴滴金融的会议"></a>与滴滴金融的会议</h2><h3 id="乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉及到POI特征数据？讨论POI数据缺失的可能替换方案。"><a href="#乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉及到POI特征数据？讨论POI数据缺失的可能替换方案。" class="headerlink" title="乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉及到POI特征数据？讨论POI数据缺失的可能替换方案。"></a>乘客行为分在分析身份特征／平台贡献度／行为健康度／平台忠诚度这几个维度时，有没有涉及到POI特征数据？讨论POI数据缺失的可能替换方案。</h3><p>当前有几类方案。</p><h3 id="金融视角，产品形态是车辆抵押贷款，需要聚焦在平台内有车乘客和橘子司机，我们需要重点围绕稳定性、收入、反欺诈、行为类四大类进行分析，具体情况开会介绍。"><a href="#金融视角，产品形态是车辆抵押贷款，需要聚焦在平台内有车乘客和橘子司机，我们需要重点围绕稳定性、收入、反欺诈、行为类四大类进行分析，具体情况开会介绍。" class="headerlink" title="金融视角，产品形态是车辆抵押贷款，需要聚焦在平台内有车乘客和橘子司机，我们需要重点围绕稳定性、收入、反欺诈、行为类四大类进行分析，具体情况开会介绍。"></a>金融视角，产品形态是车辆抵押贷款，需要聚焦在平台内有车乘客和橘子司机，我们需要重点围绕稳定性、收入、反欺诈、行为类四大类进行分析，具体情况开会介绍。</h3><ul><li><p>通过出行平台数据判断乘客稳定性时，乘客行为分需要加入哪些特征。</p><p>稳定性：</p></li></ul><p><img src="https://pic1.zhimg.com/80/v2-fea6a1111cb1274ca3368fad8d2e7a88_hd.jpg" alt="img"></p><ul><li>收入（消费水平）：平台价值维度。</li></ul><p><img src="https://pic4.zhimg.com/80/v2-88541a4212c7678e589df5badace3a07_hd.jpg" alt="img"></p><ul><li><p>反欺诈&amp;行为：</p><p>| 维度     | 子维度             | 因子                                         | 行为好坏                            | 备注 |<br>| ——– | —————— | ——————————————– | ———————————– | —- |<br>| 履约能力 | 历史借贷及履约表现 | 1.最近7天逾期支付率                          | 好：连续n单按时支付（3，5，10，20） |      |<br>|          |                    | 2.最近30天逾期支付率                         | 坏：7天内支付率                     |      |<br>|          |                    | 3.30天以上逾期支付率                         |                                     |      |<br>|          | 行为规范           | 1.平台作弊行为及表现（刷单，历史封禁）；     |                                     |      |<br>|          | 身份属性           | 1.实名认证？2.是否有司机账号                 |                                     |      |<br>|          | 发单习惯           | 平台发单行为（活跃度，发单习惯，收入稳定性） |                                     |      |<br>|          |                    |                                              |                                     |      |<br>|          |                    |                                              |                                     |      |<br>|          |                    |                                              |                                     |      |</p></li></ul><h3 id="乘客行为分在滴滴金融车抵贷产品形态下，数据风控层面的应用，从特征提取，数据建模，到输出决策。"><a href="#乘客行为分在滴滴金融车抵贷产品形态下，数据风控层面的应用，从特征提取，数据建模，到输出决策。" class="headerlink" title="乘客行为分在滴滴金融车抵贷产品形态下，数据风控层面的应用，从特征提取，数据建模，到输出决策。"></a>乘客行为分在滴滴金融车抵贷产品形态下，数据风控层面的应用，从特征提取，数据建模，到输出决策。</h3><h3 id="会议记录"><a href="#会议记录" class="headerlink" title="会议记录"></a>会议记录</h3><h4 id="进度"><a href="#进度" class="headerlink" title="进度"></a>进度</h4><h5 id="金融"><a href="#金融" class="headerlink" title="金融"></a>金融</h5><pre><code>- 以大数据风控作为下半年发展方向。- 识别乘客的信用风险、需求，数据不是金融的，判断一个人是否有还贷意向。一部分是乘客，一部分是司机，并不是活跃用户，那如何判断他们还贷的概率需要数据。</code></pre><ul><li>信用分的场景与典押有点类似，探讨可以协同的地方。</li></ul><h4 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h4><ul><li><p>不活跃的用户如何区分：80-20定律，大部分用户数据量很少；解决方法：提高数据丰度</p></li><li><p>如何区分是否虚假/恶意投诉：投诉率、进线率太高，人工进行审核</p></li></ul><h4 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h4><ul><li>平台有车乘客规模，乘客是否有房、是否有车的标签</li></ul><h4 id="介绍四方面的特征"><a href="#介绍四方面的特征" class="headerlink" title="介绍四方面的特征"></a>介绍四方面的特征</h4><ol><li><p>稳定性</p><p>异地高频生活、POI特征、统计类数据</p></li><li><p>反欺诈</p><p>是否高危职业客户</p></li><li><p>收入</p><p>车辆持有满3个月，固定收入预估区间，名下车产信息是否一致…</p></li><li><p>行为</p><p>近期生活轨迹重大变故、喜好预测…</p></li></ol><h4 id="介绍行为分"><a href="#介绍行为分" class="headerlink" title="介绍行为分"></a>介绍行为分</h4><p>初始化、权重计算、影响因子、增量更新</p><p>行为健康度介绍</p><p>用于免押的应用场景</p><h4 id="芝麻分"><a href="#芝麻分" class="headerlink" title="芝麻分"></a>芝麻分</h4><h2 id="待办"><a href="#待办" class="headerlink" title="待办"></a>待办</h2><p>排序之前的问题</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2019/11/19/7%E6%9C%8823%E6%97%A5/"/>
      <url>/2019/11/19/7%E6%9C%8823%E6%97%A5/</url>
      
        <content type="html"><![CDATA[<p>##吴文栋邮件</p><h3 id="先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。"><a href="#先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。" class="headerlink" title="先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。"></a>先问能不能？再问如何？“滴滴如何树立自己在出行方面的权威形象？”滴滴虽然是一家平台，但只提供撮合交易的价值，是司机／乘客之外的第三方，但不一定能满足权威性要求。</h3><ul><li>提供了平台，就得对平台进行管控，矛盾不可避免，而出现司乘之间无法自行调和的矛盾，则只能靠平台进行决断，因此这个权威性必须树立。要做的应该是尽量减少矛盾的出现，但矛盾一旦出现，要有足够的权威给予适当的调节手段。</li><li>减少矛盾：淘宝也是第三方平台，参考淘宝，是否可以有乘客自主选单的功能，设置距离优先、服务优先、车辆类型优先等，然后再按此派单，并在出现距离较远等情况时提醒用户是否确定发单，来减少取消订单的情况。</li><li>管控：司乘之间的协商，检测到绕路、高度收费时提醒司乘，并在客户端提示乘客做选择。</li><li>坏账能不能接入政府或其他的信用评分，以此增大乘客毁约的成本。</li></ul><h3 id="民不究官不治，与上同理，第三方平台，只有司乘出现自身无法调和的矛盾时才发挥作用。"><a href="#民不究官不治，与上同理，第三方平台，只有司乘出现自身无法调和的矛盾时才发挥作用。" class="headerlink" title="民不究官不治，与上同理，第三方平台，只有司乘出现自身无法调和的矛盾时才发挥作用。"></a>民不究官不治，与上同理，第三方平台，只有司乘出现自身无法调和的矛盾时才发挥作用。</h3><h2 id="网约车"><a href="#网约车" class="headerlink" title="网约车"></a>网约车</h2><h3 id="乘客信用分需要透传的底层数据必须要实锤，可用卷宗数据-也存在非实锤的问题-所有使用客服的数据的业务都存在这个问题，相信这个问题终会解决-卷宗？"><a href="#乘客信用分需要透传的底层数据必须要实锤，可用卷宗数据-也存在非实锤的问题-所有使用客服的数据的业务都存在这个问题，相信这个问题终会解决-卷宗？" class="headerlink" title="乘客信用分需要透传的底层数据必须要实锤，可用卷宗数据(也存在非实锤的问题, 所有使用客服的数据的业务都存在这个问题，相信这个问题终会解决);卷宗？"></a>乘客信用分需要透传的底层数据必须要实锤，可用卷宗数据(也存在非实锤的问题, 所有使用客服的数据的业务都存在这个问题，相信这个问题终会解决);卷宗？</h3><ul><li>乘客信用很重要，是我们的抓手之一，但是好像跟我们的业务关系不太大，希望定制化分数; 信用是历史的表现行为的累计，用历史的行为来判责判罚当次发生的问题，持谨慎态度”</li><li>那如何解决呢？对判责仅提供参考，主要还是用来例如降低押金或免押金，或恶意乘客识别（说明历史上有多次恶意或不好的行为）</li><li>理想中是有很多作用，具体实施要考虑到业务（足够的依据）、用户的…等</li><li>目前定位是分层管控，但是管控却没有足够的依据。判责的标准不好改变，是否可以考虑多次不好的行为后实施惩罚或者账号的某些动作限制？例如信用分太低限制在发单几分钟之后取消订单？ </li></ul><h3 id="一期信用分的目的是对乘客去尾，尾部乘客可以分成两部分，其中一部分是高频小问题的乘客。"><a href="#一期信用分的目的是对乘客去尾，尾部乘客可以分成两部分，其中一部分是高频小问题的乘客。" class="headerlink" title="一期信用分的目的是对乘客去尾，尾部乘客可以分成两部分，其中一部分是高频小问题的乘客。"></a>一期信用分的目的是对乘客去尾，尾部乘客可以分成两部分，其中一部分是高频小问题的乘客。</h3><ul><li>因为客服不会对服务类投诉判责(比如抽烟、醉酒、超载等)所以无实锤证据，由于我们的实锤机制，我们无法对这类行为做扣分处理，这就会导致高频小问题的乘客不会被我们管控。不做管控，但间接提醒，例如既然同类问题高频出现，那认为此人确实有此类问题的置信度比较高，因此可以考虑发送短信提醒（如果在提醒中加上，如若再犯会影响平台对你之后行为的判责以及打车发单的优先级，这样之后再应用行为分做管控的时候合理吗？或者不做判责）</li></ul><h3 id="方案细节"><a href="#方案细节" class="headerlink" title="方案细节"></a>方案细节</h3><h4 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h4><ul><li>分5个等级评价，但是大部分人是默认好评，直接混入好评差评率作为特征会引入噪声吧，是否可以考虑标记默认好评数，再排除默认好评求取差评率好评率。</li><li>很多行为，例如抽烟、吃东西、喝酒等，并不是每位司机都会在平台上报备，这样的话直接拿这些行为用作特征，这个合理吗？因为这虽然是现实数据，但其实并不是实际中的真实分布（有人抽烟，但平台记录为无），所以此类行为加入到模型中怎样应用呢？</li><li>以投诉或差评为label，如果是恶意投诉或恶意差评呢？怎样去躁？或者提示模型的鲁棒性？这个用起来应该更长久一点。</li><li>性骚扰、盗窃、抢劫，这些极端恶劣性行为不一定有石锤，考虑直接客服介入？</li></ul><h4 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h4><ul><li>一次坏行为，一月坏行为率，行为*权重。这是用于评分卡模型的。有没有其他模型呢？更多的模型就能考虑更多要素。</li><li>所有新用户有一个基本分，后续增量变化，以此生成信用分，但是这个现实吗？早期数据够不够？</li></ul><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><ul><li>信用分增量更新的具体考量。高分速率放缓、加减分权重…</li><li>相当于异常检测，所以直接做统计检验来产生概率是否可以作为一个模型</li></ul><h2 id="信用分调研2019"><a href="#信用分调研2019" class="headerlink" title="信用分调研2019"></a>信用分调研2019</h2><h3 id="项目推进节奏"><a href="#项目推进节奏" class="headerlink" title="项目推进节奏"></a>项目推进节奏</h3><ul><li><p>乘客行为分 推动 乘客露出 比较难，水下则会容易得多。出现这种情况的原因主要在于，乘客行为分本质需要乘客完成隐私授权，作为对等交换，应当通过提供权益的方式达成这一目的。之前推动乘客侧的露出，主要是希望达成管控目标，这和乘客的期待有本质的冲突。</p><p>就是说向乘客露出了行为分那就得给乘客提供相应权益（乘客付出了隐私；乘客对信用分没有需求，所以用其他手段刺激需求），免押？打折/优惠券？满级奖励？接入芝麻分/信用评级？</p></li></ul><h3 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h3><h2 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h2><ul><li><p><strong>评分卡模型</strong></p><p><em>F: 未来一段时间内是否出现恶劣行为 是很好的建模目标么？如何评估一个建模目标的好坏？</em></p><p>A: </p><ul><li>评分卡模型本质是一个二分类，因此离线指标为分类模型可用的指标：AUC/Recall/Precision/F1/Accuracy…</li><li>模型生成的分数在线上评估时有两个维度，区分度评估：K-S, GINI（还有其他的吗？树模型里面选择特征以及分裂点的指标例如信息增益、信息增益比等本质也是衡量区分度的，应该改改可以用吧）；稳定度评估：中位数</li><li>业务当中，为评价此指标的引入创造的价值还应该有业务指标的评价：坏账率、注册完单率、CPO、司机&amp;乘客NPS、GMV…（多个维度）<strong>业务上的评价应该是根本性的评价</strong></li></ul></li><li><p>信用分是历史行为的积累，但是用历史的行为来判责当次发生的问题显得依据不够，因此在内部推行的时候也遇到了不少困难，是否可以考虑对判责仅提供参考，但可以应用于例如降低押金、免押金、或恶意乘客识别（说明历史上有多次恶意行为）</p><p><em>F: “仅提供参考”是高价值／高优先级 的目标么？如果不是，哪些是高价值／高优先级 的目标。</em></p><p>A: 既然是只提供参考，那就不是高价值 / 高优先级的目标，高价值 / 高优先级的目标：履约能力（免押金、免预付，减少坏账）；减少取消率；提高司机NPS。BTW: 既然是在判责的时候用历史行为显得依据不够，那能不能把信用分应用于一个单子生命后期的更早期呢，例如预防坏账、预防取消、增强司机的体验…。</p></li><li><p>目前信用分对内部的定位是分层管控，但是管控却没有足够的依据，那是否可以考虑多次恶意行为后实施惩罚或者账号的某些动作限制，例如信用分太低则适当限制在发单之后取消订单等操作。 </p><p><em>F: 信用分的定位</em></p><p>A: </p></li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>RF &amp; GBDT &amp; XGBoost面试题目</title>
      <link href="/2019/07/03/RF%E3%80%81GBDT%E3%80%81XGBoost/"/>
      <url>/2019/07/03/RF%E3%80%81GBDT%E3%80%81XGBoost/</url>
      
        <content type="html"><![CDATA[<h1 id="RF与GBDT之间的区别"><a href="#RF与GBDT之间的区别" class="headerlink" title="RF与GBDT之间的区别"></a>RF与GBDT之间的区别</h1><h2 id="相同点"><a href="#相同点" class="headerlink" title="相同点"></a>相同点</h2><ul><li>都是由多棵树组成</li><li>最终的结果都是由多棵树一起决定</li></ul><h2 id="不同点"><a href="#不同点" class="headerlink" title="不同点"></a>不同点</h2><ul><li>组成随机森林的树可以分类树也可以是回归树，而GBDT只由回归树组成</li><li>组成随机森林的树可以并行生成，而GBDT是串行生成</li><li>随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和</li><li>随机森林对异常值不敏感，而GBDT对异常值比较敏感</li><li>随机森林是通过减少模型的方差来提高性能，而GBDT是减少模型的偏差来提高性能的</li><li>随机森林不需要进行数据预处理，即特征归一化。而GBDT则需要进行特征归一化</li></ul><h3 id="分类树和回归树的区别"><a href="#分类树和回归树的区别" class="headerlink" title="分类树和回归树的区别"></a>分类树和回归树的区别</h3><ul><li>回归树和分类树的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。</li><li>分类树使用信息增益或增益比率来划分节点；每个节点样本的类别情况投票决定测试样本的类别。</li><li>回归树使用最小化均方差划分节点；每个节点样本的均值作为测试样本的回归预测值</li></ul><h1 id="Xgboost和GBDT的区别"><a href="#Xgboost和GBDT的区别" class="headerlink" title="Xgboost和GBDT的区别"></a>Xgboost和GBDT的区别</h1><ul><li>传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题）。</li><li>节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的。</li><li>Xgboost在代价函数里加入了正则项，用于控制模型的复杂度，降低了过拟合的可能性。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。</li><li>shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）</li><li>列采样</li><li><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。</p><ul><li>为什么xgboost要用泰勒展开，优势在哪里？xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。</li></ul></li><li><p>Xgboost工具支持并行。boosting不是一种串行的结构吗?怎么并行的？注意xgboost的并行<strong>不是tree粒度的并行</strong>，xgboost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。<strong>xgboost的并行是在特征粒度上的</strong>。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，<strong>预先对数据进行了排序</strong>，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行</p></li></ul><h2 id="N问GBDT"><a href="#N问GBDT" class="headerlink" title="N问GBDT"></a>N问GBDT</h2><ul><li>GBDT的核心就在于，每一棵树学的是之前所有树结论和的残差，这个残差就是一个加预测值后能得真实值的累加量</li><li>怎样设置单棵树的停止生长条件？<ul><li>A. 节点分裂时的最小样本数</li><li>B. 最大深度</li><li>C. 最多叶子节点数</li><li>D. loss满足约束条件</li></ul></li><li>如何评估特征的权重大小？<ul><li>A. 通过计算每个特征在训练集下的信息增益，最后计算每个特征信息增益与所有特征信息增益之和的比例为权重值。</li><li>B. 借鉴投票机制。用相同的gbdt参数对w每个特征训练出一个模型，然后在该模型下计算每个特征正确分类的个数，最后计算每个特征正确分类的个数与所有正确分类个数之和的比例为权重值。</li></ul></li><li>当增加样本数量时，训练时长是线性增加吗？<ul><li>不是。因为生成单棵决策树时，损失函数极小值与样本数量N不是线性相关</li></ul></li><li>当增加树的棵数时，训练时长是线性增加吗？<ul><li>不是。因为每棵树的生成的时间复杂度不一样。</li></ul></li><li>当增加一个棵树叶子节点数目时，训练时长是线性增加吗？<ul><li>不是。叶子节点数和每棵树的生成的时间复杂度不成正比。</li></ul></li><li>每个节点上都保存什么信息？<ul><li>中间节点保存某个特征的分割值，叶结点保存预测是某个类别的概率。</li></ul></li><li>如何防止过拟合？<ul><li>a. 增加样本（data bias or small data的缘故），移除噪声。</li><li>b. 减少特征，保留重要的特征（可以用PCA等对特征进行降维）。</li><li>c. 对样本进行采样（类似bagging）。就是建树的时候，不是把所有的样本都作为输入，而是选择一个子集。</li><li>d. 对特征进行采样。类似样本采样一样, 每次建树的时候，只对部分的特征进行切分。</li></ul></li><li>gbdt在训练和预测的时候都用到了步长，这两个步长一样么？都有什么用，如果不一样，为什么？怎么设步长的大小？（太小？太大？）在预测时，设太大对排序结果有什么影响？跟shrinking里面的步长一样么这两个步长一样么？<ul><li>训练跟预测时，两个步长是一样的，也就是预测时的步长为训练时的步长，从训练的过程可以得知（更新当前迭代模型的时候）。</li><li>都有什么用，如果不一样，为什么？答：它的作用就是使得每次更新模型的时候，使得loss能够平稳地沿着负梯度的方向下降，不至于发生震荡。</li><li>那么怎么设步长的大小?<ul><li>有两种方法，一种就是按策略来决定步长，另一种就是在训练模型的同时，学习步长。</li><li>策略：a.每个树步长恒定且相等，一般设较小的值；b.开始的时候给步长设一个较小值，随着迭代次数动态改变，或者说衰减。</li><li>学习：因为在训练第k棵树的时候，前k-1棵树时已知的，而且求梯度的时候是利用前k-1棵树来获得。所以这个时候，就可以把步长当作一个变量来学习。</li></ul></li><li>（太小？太大？）在预测时，对排序结果有什么影响？<ul><li>如果步长过大，在训练的时候容易发生震荡，使得模型学不好，或者完全没有学好，从而导致模型精度不好。</li><li>而步长过小，导致训练时间过长，即迭代次数较大，从而生成较多的树，使得模型变得复杂，容易造成过拟合以及增加计算量。</li></ul></li><li>跟shrinking里面的步长一样么？<ul><li>这里的步长跟shrinking里面的步长是一致的。</li></ul></li></ul></li><li>boosting的本意是是什么？跟bagging，random forest，adaboost，gradient boosting有什么区别？<ul><li>Bagging<ul><li>放回抽样，多数表决（分类）或简单平均（回归）</li><li>可以看成是一种圆桌会议，或是投票选举的形式。通过训练多个模型，将这些训练好的模型进行加权组合来获得最终的输出结果(分类/回归)，一般这类方法的效果，都会好于单个模型的效果。在实践中，在特征一定的情况下，大家总是使用Bagging的思想去提升效果。例如kaggle上的问题解决，因为大家获得的数据都是一样的，特别是有些数据已经过预处理。</li><li>基本的思路：训练时，使用replacement的sampling方法，sampling一部分训练数据k次并训练k个模型；预测时，使用k个模型，如果为分类，则让k个模型均进行分类并选择出现次数最多的类（每个类出现的次数占比可以视为置信度）；如为回归，则为各分类器返回的结果的平均值。在该处，Bagging算法可以认为每个分类器的权重都一样由于每次迭代的采样是独立的，所以bagging可以并行。</li></ul></li><li>Random forest<ul><li>随机森林在bagging的基础上做了修改。<ul><li>A. 从样本集散用Boostrap采样选出n个样本，预建立CART</li><li>B. 在树的每个节点上，从所有属性中随机选择k个属性/特征，选择出一个最佳属性/特征作为节点</li><li>C. 重复上述两步m次，i.e.build m棵cart</li><li>D. 这m棵cart形成random forest。</li></ul></li><li>随机森林可以既处理属性是离散的量，比如ID3算法，也可以处理属性为连续值得量，比如C4.5算法。这里的random就是指：<ul><li>A. boostrap中的随机选择样本</li><li>B. random subspace的算法中从属性/特征即中随机选择k个属性/特征，每棵树节点分裂时，从这随机的k个属性/特征，选择最优的。</li></ul></li></ul></li><li>Boosting:<ul><li>一般Boosting算法都是一个迭代的过程，每一次新的训练都是为了改进上一次的结果。</li><li>boosting的采样或者更改样本的权重依赖于上一次迭代的结果，在迭代层面上是不能并行的。</li><li>boosting在选择hyperspace的时候给样本加了一个权值，使得loss function尽量考虑那些分错类的样本（如分错类的样本weight大）。怎么做的呢？<ul><li>boosting重采样的不是样本，而是样本的分布，对于分类正确的样本权值低，分类错误的样本权值高(通常是边界附近的样本)，最后的分类器是很多弱分类器的线性叠加(加权组合)。</li></ul></li></ul></li><li>Adaboosting<ul><li>对一份数据，建立M个模型(比如分类)，而一般这种模型比较简单，称为弱分类器(weak learner)。每次分类都将上一次分错的数据权重提高一点，对分对的数据权重降低一点，再进行分类。这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的效果。</li><li>每次迭代的样本是一样的，即没有采样过程，不同的是不同的样本权重不一样。(当然也可以对样本/特征进行采样，这个不是adaboosting的原意)。</li><li>另外，每个分类器的步长由在训练该分类器时的误差来生成。</li></ul></li><li>Gradient boosting<ul><li>每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在残差减少的梯度 (Gradient)方向上建立一个新的模型。所以说在Gradient Boost中，每个新模型是为了使之前模型的残差往梯度方向减少，与传统Boost对正确，错误的样本进行加权有着很大的区别。</li></ul></li></ul></li><li>gbdt中哪些部分可以并行？<ul><li>A. 计算每个样本的负梯度</li><li>B. 分裂挑选最佳特征及其分割点时，对特征计算相应的误差及均值时</li><li>C. 更新每个样本的负梯度时</li><li>D. 最后预测过程中，每个样本将之前的所有树的结果累加的时候</li></ul></li><li>树生长成畸形树，会带来哪些危害，如何预防？<ul><li>在生成树的过程中，加入树不平衡的约束条件。这种约束条件可以是用户自定义的。例如对样本集中分到某个节点，而另一个节点的样本很少的情况进行惩罚。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROC AUC, Accuracy, Recall, F1</title>
      <link href="/2019/07/02/%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/"/>
      <url>/2019/07/02/%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86/</url>
      
        <content type="html"><![CDATA[<ul><li>树的遍历，分深度优先：前序，中序，后序；广度优先：层次遍历。</li><li>前序：根、左、右</li><li>中序：左、根、右</li><li>后序：左、右、根</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Node</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""节点类"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, elem=<span class="number">-1</span>, lchild=None, rchild=None)</span>:</span></span><br><span class="line">        self.elem = elem</span><br><span class="line">        self.lchild = lchild</span><br><span class="line">        self.rchild = rchild</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tree</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""树类"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.root = Node()</span><br><span class="line">        self.myQueue = []</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add</span><span class="params">(self, elem)</span>:</span></span><br><span class="line">        <span class="string">"""为树添加节点"""</span></span><br><span class="line">        node = Node(elem)</span><br><span class="line">        <span class="keyword">if</span> self.root.elem == <span class="number">-1</span>:  <span class="comment"># 如果树是空的，则对根节点赋值</span></span><br><span class="line">            self.root = node</span><br><span class="line">            self.myQueue.append(self.root)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            treeNode = self.myQueue[<span class="number">0</span>]  <span class="comment"># 此结点的子树还没有齐。</span></span><br><span class="line">            <span class="keyword">if</span> treeNode.lchild == <span class="literal">None</span>:</span><br><span class="line">                treeNode.lchild = node</span><br><span class="line">                self.myQueue.append(treeNode.lchild)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                treeNode.rchild = node</span><br><span class="line">                self.myQueue.append(treeNode.rchild)</span><br><span class="line">                self.myQueue.pop(<span class="number">0</span>)  <span class="comment"># 如果该结点已经有了右子树，将此结点丢弃，从下一个节点插入。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">front_digui</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用递归实现树的先序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        <span class="keyword">print</span> root.elem,</span><br><span class="line">        self.front_digui(root.lchild)</span><br><span class="line">        self.front_digui(root.rchild)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle_digui</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用递归实现树的中序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.middle_digui(root.lchild)</span><br><span class="line">        <span class="keyword">print</span> root.elem,</span><br><span class="line">        self.middle_digui(root.rchild)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">later_digui</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用递归实现树的后序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        self.later_digui(root.lchild)</span><br><span class="line">        self.later_digui(root.rchild)</span><br><span class="line">        <span class="keyword">print</span> root.elem,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">front_stack</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用堆栈实现树的先序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        myStack = []</span><br><span class="line">        node = root</span><br><span class="line">        <span class="keyword">while</span> node <span class="keyword">or</span> myStack:</span><br><span class="line">            <span class="keyword">while</span> node:                     <span class="comment">#从根节点开始，一直找它的左子树</span></span><br><span class="line">                <span class="keyword">print</span> node.elem,</span><br><span class="line">                myStack.append(node)</span><br><span class="line">                node = node.lchild</span><br><span class="line">            node = myStack.pop()            <span class="comment">#while结束表示当前节点node为空，即前一个节点没有左子树了</span></span><br><span class="line">            node = node.rchild                  <span class="comment">#开始查看它的右子树</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">middle_stack</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用堆栈实现树的中序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        myStack = []</span><br><span class="line">        node = root</span><br><span class="line">        <span class="keyword">while</span> node <span class="keyword">or</span> myStack:</span><br><span class="line">            <span class="keyword">while</span> node:                     <span class="comment">#从根节点开始，一直找它的左子树</span></span><br><span class="line">                myStack.append(node)</span><br><span class="line">                node = node.lchild</span><br><span class="line">            node = myStack.pop()            <span class="comment">#while结束表示当前节点node为空，即前一个节点没有左子树了</span></span><br><span class="line">            <span class="keyword">print</span> node.elem,</span><br><span class="line">            node = node.rchild                  <span class="comment">#开始查看它的右子树</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">later_stack</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用堆栈实现树的后序遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        myStack1 = []</span><br><span class="line">        myStack2 = []</span><br><span class="line">        node = root</span><br><span class="line">        myStack1.append(node)</span><br><span class="line">        <span class="keyword">while</span> myStack1:                   <span class="comment">#这个while循环的功能是找出后序遍历的逆序，存在myStack2里面</span></span><br><span class="line">            node = myStack1.pop()</span><br><span class="line">            <span class="keyword">if</span> node.lchild:</span><br><span class="line">                myStack1.append(node.lchild)</span><br><span class="line">            <span class="keyword">if</span> node.rchild:</span><br><span class="line">                myStack1.append(node.rchild)</span><br><span class="line">            myStack2.append(node)</span><br><span class="line">        <span class="keyword">while</span> myStack2:                         <span class="comment">#将myStack2中的元素出栈，即为后序遍历次序</span></span><br><span class="line">            <span class="keyword">print</span> myStack2.pop().elem,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">level_queue</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="string">"""利用队列实现树的层次遍历"""</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        myQueue = []</span><br><span class="line">        node = root</span><br><span class="line">        myQueue.append(node)</span><br><span class="line">        <span class="keyword">while</span> myQueue:</span><br><span class="line">            node = myQueue.pop(<span class="number">0</span>)</span><br><span class="line">            <span class="keyword">print</span> node.elem,</span><br><span class="line">            <span class="keyword">if</span> node.lchild != <span class="literal">None</span>:</span><br><span class="line">                myQueue.append(node.lchild)</span><br><span class="line">            <span class="keyword">if</span> node.rchild != <span class="literal">None</span>:</span><br><span class="line">                myQueue.append(node.rchild)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="string">"""主函数"""</span></span><br><span class="line">    elems = range(<span class="number">10</span>)           <span class="comment">#生成十个数据作为树节点</span></span><br><span class="line">    tree = Tree()          <span class="comment">#新建一个树对象</span></span><br><span class="line">    <span class="keyword">for</span> elem <span class="keyword">in</span> elems:                  </span><br><span class="line">        tree.add(elem)           <span class="comment">#逐个添加树的节点</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'队列实现层次遍历:'</span></span><br><span class="line">    tree.level_queue(tree.root)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n\n递归实现先序遍历:'</span></span><br><span class="line">    tree.front_digui(tree.root)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n递归实现中序遍历:'</span> </span><br><span class="line">    tree.middle_digui(tree.root)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n递归实现后序遍历:'</span></span><br><span class="line">    tree.later_digui(tree.root)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n\n堆栈实现先序遍历:'</span></span><br><span class="line">    tree.front_stack(tree.root)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n堆栈实现中序遍历:'</span></span><br><span class="line">    tree.middle_stack(tree.root)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'\n堆栈实现后序遍历:'</span></span><br><span class="line">    tree.later_stack(tree.root)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>十大排序算法python实现</title>
      <link href="/2019/07/02/%E9%9D%A2%E8%AF%95/"/>
      <url>/2019/07/02/%E9%9D%A2%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<h1 id="ML-DL"><a href="#ML-DL" class="headerlink" title="ML/DL"></a>ML/DL</h1><h2 id="过拟合、欠拟合"><a href="#过拟合、欠拟合" class="headerlink" title="过拟合、欠拟合"></a>过拟合、欠拟合</h2><ul><li>理解</li><li>过拟合<ul><li>增加样本（增加的样本分布要与原始样本的分布尽可能不同）</li><li>减少特征数量</li><li>正则化，例如l2正则化后，虽然参数数量没有变化，但是参数趋近于0，则特征的重要性减小了</li><li>样本采样</li><li>特征采样</li></ul></li></ul><h2 id="分类问题中正负例"><a href="#分类问题中正负例" class="headerlink" title="分类问题中正负例"></a>分类问题中正负例</h2><ul><li>过采样</li><li>降采样</li><li>AUC ROC</li><li>调整阈值</li></ul><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>作用：本来是wx+b是线性的，加入非线性函数，使得理论上可以逼近所有函数。</p><ul><li>sigmoid f(x) = 1/(1+exp(-x))<ul><li>幂运算，计算量大</li><li>梯度消失与梯度爆炸：BP时每层都要求激活函数的导，这个导数如果小于1经过多次之后就会趋近于0，梯度消失；如果大于1，多次之后会非常大，梯度爆炸</li><li>其输出是非零均值的，例如某个神经元经过sigmoid之后的输出都大于0，此时输入到下一层后，因为wx+b，所以对w的导数为x，即导数大于0，导致的结果就是BP时w都正方向更新</li></ul></li><li>tanh f(x) = (1-exp(-2x)) / (1+exp(-2x))<ul><li>幂运算，计算量大</li><li>解决了非零均值的问题</li><li>梯度消失与梯度爆炸仍然存在</li></ul></li><li>ReLU f(x) = max(0,x)<ul><li>速度快（输出速度，收敛速度）</li><li>正区间解决了梯度消失问题</li><li>非零均值</li><li>Dead ReLU：某些神经元永远都不会被激活</li></ul></li><li>Leaky ReLU<ul><li>解决了Dead ReLU问题 </li></ul></li></ul><h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><ul><li>如何优化的？loss f 策略</li><li>如何避免过拟合<ul><li>增加样本（增加的样本分布要与原始样本的分布尽可能不同）</li><li>减少特征数量</li><li>正则化，例如l2正则化后，虽然参数数量没有变化，但是参数趋近于0，则特征的重要性减小了</li><li>每次迭代，调整学习速率</li></ul></li></ul><h2 id="线性模型解决非线性问题"><a href="#线性模型解决非线性问题" class="headerlink" title="线性模型解决非线性问题"></a>线性模型解决非线性问题</h2><ul><li>例如LR，SVM都是线性模型，但是使用非线性的核函数可以实现对非线性问题的解决</li></ul><h1 id="语言"><a href="#语言" class="headerlink" title="语言"></a>语言</h1><h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><ul><li>python数据类型：Numbers（数字）、String（字符串）、List（列表）、Tuple（元组）、Dictionary（字典）</li><li>元组和数组的差别：<ul><li>元组()不能修改，数组[]可以修改</li><li>字典{}，集合set()是无序的</li></ul></li></ul><h1 id="DS"><a href="#DS" class="headerlink" title="DS"></a>DS</h1><ul><li>二叉树的遍历</li><li><p>很大的一个文件寻找频率TOP-k的词</p></li><li><p>一个非递减序列，寻找某个数最后出现的位置</p></li><li><p>归并排序</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(num) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> num</span><br><span class="line">    mid = num // <span class="number">2</span></span><br><span class="line">    left = merge_sort(num[:mid]) <span class="comment"># 从下往上的递归中，每次递归得到的left和right是排好序的，需要对两者合并后做排序</span></span><br><span class="line">    right = merge_sort(num[mid:])</span><br><span class="line">    <span class="keyword">return</span> merge_result(left, right)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_result</span><span class="params">(left, right)</span>:</span></span><br><span class="line">    i,j = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">while</span> i &lt; len(left) <span class="keyword">and</span> j &lt; len(right):</span><br><span class="line">        <span class="keyword">if</span> left[i] &lt;= right[j]:</span><br><span class="line">            result.append(left[i])</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result.append(right[j])</span><br><span class="line">            j += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> result+left[i:]+right[j:]</span><br></pre></td></tr></table></figure></li><li><p>快排</p>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort1</span><span class="params">(num,left,right)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> left &gt;= right:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    low = left</span><br><span class="line">    high = right</span><br><span class="line">    pivot = num[left]</span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> num[right] &gt; pivot:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        num[left] = num[right]</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> num[left] &lt;= pivot:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        num[right] = num[left]</span><br><span class="line">    num[right] = pivot</span><br><span class="line">    quick_sort1(num, low, left<span class="number">-1</span>)</span><br><span class="line">    quick_sort1(num, right+<span class="number">1</span>, high)</span><br></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort2</span><span class="params">(num,left,right)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        p = partition(num, left, right)</span><br><span class="line">        quick_sort2(num, left, p<span class="number">-1</span>)</span><br><span class="line">        quick_sort2(num, p+<span class="number">1</span>, right)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(num, left, right)</span>:</span></span><br><span class="line">    pivot = num[right] <span class="comment"># 先挪左指针，用右边界作为pivot</span></span><br><span class="line">    i = left</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(left,right):</span><br><span class="line">        <span class="keyword">if</span> num[j] &lt;= pivot:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            num[i], num[j] = num[j], num[i]</span><br><span class="line">    num[i+<span class="number">1</span>], num[right] = num[right], num[i+<span class="number">1</span>] <span class="comment"># 把基准数移过来</span></span><br><span class="line">    <span class="keyword">return</span> i+<span class="number">1</span></span><br></pre></td></tr></table></figure>  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort3</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(num) &lt;= <span class="number">1</span>:</span><br><span class="line">    <span class="keyword">return</span> num</span><br><span class="line">    pivot = num[<span class="number">0</span>]</span><br><span class="line">    left = [num[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)) <span class="keyword">if</span> num[i] &lt; pivot]</span><br><span class="line">    right = [num[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)) <span class="keyword">if</span> num[i] &gt; pivot]</span><br><span class="line">    <span class="keyword">return</span> quick_sort3(left) + pivot + quick_sort3(right)</span><br></pre></td></tr></table></figure></li><li><p>O(N)复杂度在一个数组中寻找两数和为指定数的下标</p></li></ul><h1 id="逻辑"><a href="#逻辑" class="headerlink" title="逻辑"></a>逻辑</h1><ul><li>一个筛子产生0~9的随机数，要求概率相等</li><li>理发师数量估计</li></ul><h1 id="操作系统-计网"><a href="#操作系统-计网" class="headerlink" title="操作系统 + 计网"></a>操作系统 + 计网</h1><ul><li><p>进程和线程</p><p>进程和线程的主要差别在于它们是不同的操作系统资源管理方式。进程有独立的地址空间，一个进程崩溃后，在保护模式下不会对其它进程产生影响，而线程只是一个进程中的不同执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但在进程切换时，耗费资源较大，效率要差一些。但对于一些要求同时进行并且又要共享某些变量的并发操作，只能用线程，不能用进程。<br>1) 简而言之,一个程序至少有一个进程,一个进程至少有一个线程。<br>2) 线程的划分尺度小于进程，使得多线程程序的并发性高。<br>3) 进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。<br>4) 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。<br>5) 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别。</p></li><li>同步和异步<ul><li>消息的通知机制</li><li>涉及到IO通知机制；所谓同步，就是发起调用后，被调用者处理消息，必须等处理完才直接返回结果，没处理完之前是不返回的，调用者主动等待结果；所谓异步，就是发起调用后，被调用者直接返回，但是并没有返回结果，等处理完消息后，通过状态、通知或者回调函数来通知调用者，调用者被动接收结果。</li></ul></li><li>阻塞和非阻塞<ul><li>程序等待调用结果时的状态</li><li>涉及到CPU线程调度；所谓阻塞，就是调用结果返回之前，该执行线程会被挂起，不释放CPU执行权，线程不能做其它事情，只能等待，只有等到调用结果返回了，才能接着往下执行；所谓非阻塞，就是在没有获取调用结果时，不是一直等待，线程可以往下执行，如果是同步的，通过轮询的方式检查有没有调用结果返回，如果是异步的，会通知回调。</li></ul></li><li>TCP和UDP<ul><li>基于连接（TCP）与无连接（UDP）； </li><li>对系统资源的要求（TCP较多，UDP少）； </li><li>UDP程序结构较简单； </li><li>流模式与数据报模式 ；</li><li>TCP保证数据正确性，UDP可能丢包，TCP保证数据顺序，UDP不保证。</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性模型和非线性模型</title>
      <link href="/2019/07/02/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
      <url>/2019/07/02/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E5%92%8C%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<ol><li>判断非线性模型和线性模型：决策边界是否是直线 或 一个变量是否被一个参数所影响<br>（决策空间中，坐标是参数w，而原来的输入x，是该空间中的坐标点）</li></ol><ul><li>Logistic Regression是线性模型，本身是wx+b，每个变量都由一个参数决定，决策边界为wx+b &lt; c，决策面是wx+b=y,这是线性的，然后再把结果做了一个映射，映射到0~1，相当于分类的置信度。</li><li>神经网络则是典型的非线性网络，因为一个变量由多个参数决定，且参数之间有交互。</li><li>SVM，有线性和非线性版本。线性SVM，其模型本身就是在寻求一个超平面，只是策略是找到间隔最大的那个超平面。而非线性SVM，虽说在特征空间上仍是分类超平面，但是先采用了<strong>核技巧</strong>从输入空间向特征空间进行了非线性映射。</li><li>MLP，其嵌套函数的特点就反映了，它的非线性更像 LR ，即从每层来看，输入并没有进行 SVM 那样的非线性特征变换，但在输出时进行了非线性映射，那么多层重叠，也就实现了特征的非线性交叉。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>十大排序算法python实现</title>
      <link href="/2019/07/02/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python/"/>
      <url>/2019/07/02/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95python/</url>
      
        <content type="html"><![CDATA[<ul><li>下文中的稳定是指：若a=b，而排序后的ab顺序与原来的ab顺序一样</li><li>交换排序：冒泡、快排；<br>选择排序：选择、堆；<br>插入排序：插入、希尔；<br>归并排序、基数排序</li><li>总结：</li><li>排序算法 | 时间复杂度（平均） | 时间（最短） | 时间（最长） | 空间复杂度 | 是否稳定<br>— | — | — | — | — | —<br>冒泡 | O(N2) | O(N) | O(N2) | O(1) | 是<br>选择 | O(N2) | O(N2) | O(N2) | 0(1) | 否<br>插入 | O(N2) | O(N) | O(N2) | O(1) | 是<br>希尔 | O(n·log(n)2) | O(n3/2) | O(N2) | O(1) | 否<br>快速 | </li></ul><ol><li>冒泡排序</li></ol><ul><li>迭代n-1次，两个相邻元素两两相比，每次迭代将最大的元素放在该迭代序列的顶端。</li><li>优化后，对于最优的情况，即已经正序排列的，算法复杂度为O(N)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)<span class="number">-1</span>):</span><br><span class="line">        flag = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(num)-i<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> num[j] &gt; num[j+<span class="number">1</span>]:</span><br><span class="line">                num[j], num[j+<span class="number">1</span>] = num[j+<span class="number">1</span>], num[j]</span><br><span class="line">                flag = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> flag == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> num</span><br><span class="line">    <span class="keyword">return</span> num</span><br></pre></td></tr></table></figure></li></ul><ol start="2"><li>选择排序</li></ol><ul><li>迭代n-1次，每次选择出最小的放到前面</li><li>不稳定，因为选择出最小的之后会跟原有数交换顺序，因此会破坏原来的顺序，例如5 3 5 2，第一次之后为2 3 5 5，此时5跟5的顺序变了</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_sort</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)<span class="number">-1</span>):</span><br><span class="line">        min_index = i   </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i,len(num)<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> num[j+<span class="number">1</span>] &lt; num[min_index]:</span><br><span class="line">                min_index = j+<span class="number">1</span></span><br><span class="line">        num[i], num[min_index] = num[min_index], num[i]</span><br><span class="line">    <span class="keyword">return</span> num</span><br></pre></td></tr></table></figure><ol start="3"><li>插入排序</li></ol><ul><li>跟打牌类似</li><li>最快情况是O(N)，如果大部分数据已经排好序了，while pre_index &gt;= 0 and cur_num &lt; num[pre_index]这句的迭代次数会大大减少，会比较快</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(num)<span class="number">-1</span>):</span><br><span class="line">        pre_index = i <span class="comment"># 前一个数</span></span><br><span class="line">        cur_num = num[i+<span class="number">1</span>] <span class="comment"># 当前待插入数 </span></span><br><span class="line">        <span class="keyword">while</span> pre_index &gt;= <span class="number">0</span> <span class="keyword">and</span> cur_num &lt; num[pre_index]:</span><br><span class="line">            num[pre_index+<span class="number">1</span>] = num[pre_index] <span class="comment"># 往后挪一位</span></span><br><span class="line">            pre_index -= <span class="number">1</span>   </span><br><span class="line">        num[pre_index+<span class="number">1</span>] = cur_num</span><br><span class="line">    <span class="keyword">return</span> num</span><br></pre></td></tr></table></figure><ol start="4"><li>希尔排序</li></ol><ul><li>对插入排序的优化，使用了递减的增量序列</li><li>如上所述插入排序中：如果大部分数据已经排好序了，while pre_index &gt;= 0 and cur_num &lt; num[pre_index]这句的迭代次数会大大减少，会比较快 –&gt; 所以希尔排序就是针对这个做了优化，即先减少需要排序的数量，再逐步对其排序</li><li>希尔排序是不稳定的算法，它满足稳定算法的定义。对于相同的两个数，可能由于分在不同的组中而导致它们的顺序发生变化。</li><li>希尔排序的性能根据其选取的序列而变化</li><li>使用动态增量序列的代码如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shell_sort</span><span class="params">(num)</span>:</span></span><br><span class="line">    gap = <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> gap &lt; len(num) // <span class="number">3</span>: <span class="comment">#gap &lt; (3a,3a+1,3a+2)//3: (a-1)*3+1=3a-2</span></span><br><span class="line">        gap = gap*<span class="number">3</span> + <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> gap &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(gap,len(num)):</span><br><span class="line">            cur_num = num[gap]</span><br><span class="line">            pre_index = i-gap</span><br><span class="line">            <span class="keyword">while</span> pre_index &gt;= <span class="number">0</span> <span class="keyword">and</span> cur_num &lt; num[pre_index]:</span><br><span class="line">                num[pre_index+gap] = num[pre_index] </span><br><span class="line">                pre_index -= gap</span><br><span class="line">            num[pre_index+gap] = cur_num</span><br><span class="line">        gap //= <span class="number">3</span></span><br></pre></td></tr></table></figure><ol start="5"><li>归并排序</li></ol><ul><li>递归</li><li>终止条件：剩一个元素时，返回该元素；再上一层对返回的两个元素比较排序</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left,right)</span>:</span> <span class="comment"># left和right本身是已经排序好的</span></span><br><span class="line">        result = []</span><br><span class="line">        i=j=<span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len(left) <span class="keyword">and</span> j &lt; len(right):</span><br><span class="line">            <span class="keyword">if</span> left[i] &lt;= right[j]:</span><br><span class="line">                result.append(left)</span><br><span class="line">                i+=<span class="number">1</span>            </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                result.append(right)</span><br><span class="line">                j+=<span class="number">1</span></span><br><span class="line">        result = result + left[i:] + right[j:]</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(num) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> num</span><br><span class="line">    mid = len(num) // <span class="number">2</span></span><br><span class="line">    left = merge_sort(num[:mid])</span><br><span class="line">    right = merge_sort(num[mid:])</span><br><span class="line">    <span class="keyword">return</span> merge(left,right)</span><br></pre></td></tr></table></figure><ol start="6"><li>快速排序</li></ol><ul><li>冒泡+二分+递归分治</li><li>核心：每次迭代使选取的基准值插入到序列中，该序列中基准值左边的值小于基准值，右边的值大于基准值，然后再对两边分别迭代</li><li>基准数选择以及指针移动顺序：最终两个指针相遇时，要把基准数和相遇的位置交换，此时该位置左边的数小于基准数，右边大于基准数；若选择最左边的数为基准数，肯定要跟比它小的数交换，因此只有右指针先动才能找到比它小的（例如算法2里右指针相遇时找到的一定是上一轮左指针的交换结果，一定是小于基准数的）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 二分之后不断地递归，每次递归求出基准值的具体位置</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort1</span><span class="params">(num)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(num) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> num</span><br><span class="line">    pivot = num[<span class="number">0</span>]</span><br><span class="line">    left = [num[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(num)) <span class="keyword">if</span> num[i] &lt;= pivot]</span><br><span class="line">    right = [num[i] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(num)) <span class="keyword">if</span> num[i] &gt; pivot]</span><br><span class="line">    <span class="keyword">return</span> quick_sort1(left) + pivot + quick_sort2(right)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort2</span><span class="params">(num,left,right)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> left &gt;= right: <span class="comment"># 两指针相遇，则终止</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    low = left</span><br><span class="line">    high = right</span><br><span class="line">    pivot = num[left]</span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> num[right] &gt; pivot: <span class="comment"># 右指针向左移动直到找到小于pivot的</span></span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        num[left] = num[right] <span class="comment"># 右边有小于基准值的，调整到左边</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> num[left] &lt;= pivot: <span class="comment"># 右指针向左移动直到找到小于pivot的</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        num[right] = num[left] <span class="comment"># 左边有小于基准值的，调整到右边</span></span><br><span class="line">    num[right] = pivot </span><br><span class="line">    quick_sort2(num, low, left<span class="number">-1</span>)</span><br><span class="line">    quick_sort2(num, right+<span class="number">1</span>, high)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort3</span><span class="params">(num,left,right)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> left &lt; right:</span><br><span class="line">        p = partition(num, left, right)</span><br><span class="line">        quick_sort3(num, left, p<span class="number">-1</span>)</span><br><span class="line">        quick_sort3(num, p+<span class="number">1</span>, right)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partition</span><span class="params">(num, left, right)</span>:</span></span><br><span class="line">    pivot = num[right] <span class="comment"># 这里是从左边开始移动指针的，因此是right</span></span><br><span class="line">    i = left - <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(left, right):</span><br><span class="line">        <span class="keyword">if</span> num[j] &lt;= pivot:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            num[i], num[j] = num[j], num[i]</span><br><span class="line">    num[i+<span class="number">1</span>], num[right] = num[right], num[i+<span class="number">1</span>] <span class="comment"># 把基准数移过来</span></span><br><span class="line">    <span class="keyword">return</span> i+<span class="number">1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DS </tag>
            
            <tag> 排序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>logistic regression, softmax regression</title>
      <link href="/2019/06/19/logstic%20regression,%20softmax%20regression/"/>
      <url>/2019/06/19/logstic%20regression,%20softmax%20regression/</url>
      
        <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/google19890102/article/details/41594889" target="_blank" rel="noopener">post</a></p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>信息熵、交叉熵、条件熵、相对熵(K-L散度)、互信息</title>
      <link href="/2019/05/27/%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81%E6%9D%A1%E4%BB%B6%E7%86%B5%E3%80%81%E4%BA%92%E4%BF%A1%E6%81%AF/"/>
      <url>/2019/05/27/%E4%BF%A1%E6%81%AF%E7%86%B5%E3%80%81%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81%E6%9D%A1%E4%BB%B6%E7%86%B5%E3%80%81%E4%BA%92%E4%BF%A1%E6%81%AF/</url>
      
        <content type="html"><![CDATA[<ul><li>设随机变量X，有n个事件$x_i$ –&gt; $x_n$，概率分布为p(x)</li></ul><ol><li><p>信息</p><ul><li>某随机变量X取值为xi的信息为 $I(X=xi)=\log_2\frac{1}{p(x_i)}=-\log_2p(x_i)$<br>：某事件xi的信息代表这个事件能提供的信息，一个发生概率越小的事件能够提供的信息量越大。</li></ul></li><li><p>信息熵</p><ul><li>信息代表一个事件的不确定性，信息熵是整个随机变量X不确定性的度量：<strong>信息的期望</strong>。<br>$H(X)=\sum_0^np(x_i)*I(x_i)=-\sum_0^np(x_i)\log_2(p(x_i))$</li><li>信息熵只与变量X的分布有关，与其取值无关。例如二分类中，两取值的概率均为0.5时，其熵最大，也最难预测某时刻哪一类别会发生。</li><li>如何通俗的解释交叉熵与相对熵? - CyberRep的回答 - 知乎<br><a href="https://www.zhihu.com/question/41252833/answer/195901726" target="_blank" rel="noopener">https://www.zhihu.com/question/41252833/answer/195901726</a></li><li>对于一个系统而言，若获知其真实分布，则我们能够找到一个最优策略，以最小的代价来消除系统的不确定性，而这个最小的代价（猜题次数、编码长度等）就是信息熵。</li></ul></li><li><p>条件熵</p><ul><li>定义为：给定条件X下，Y的分布（Y|X）的熵对X的数学期望：$H(Y|X)=\sum_xp(x)H(Y|X=x)$</li><li>在ML中，即选定某个特征X(X有n类)后，label(Y)的条件概率熵求期望：<strong>给定X特征的条件下Y的信息熵</strong>。</li><li>条件熵越小，代表在这个特征下，label的信息熵越小，也就是说要解决问题的代价越小。</li></ul></li><li><p>信息增益 — ID3</p><ul><li>$IG(Y|X)=H(Y)-H(Y|X)$</li><li>在决策树中作为选择特征的指标，IG越大，这个特征的选择性越好，也可以理解为：待分类的集合的熵和选定某个特征的条件熵之差越大，这个特征对整个集合的影响越大。</li><li>对于条件熵来说，条件熵越小，分类后的纯度越高，但是问题是：X的取值越多，每个取值下Y的纯度越高，H(Y|X)越小，但此时并不有利于Y的区分。信息增益也是如此。–&gt; 信息增益率。</li></ul></li><li><p>信息增益率/信息增益比 — C4.5</p><ul><li>偏好取值少的特征。C4.5：先选择高于平均水平信息增益的特征，再在其中选择最高信息增益率的特征。</li><li>见<a href="https://chenzk1.github.io/2019/03/14/Decision%20Tree/">Decision Tree</a></li></ul></li><li><p>基尼系数 — CART</p><ul><li><p>表示数据的不纯度。既有分类也有回归，既要确定特征，也要确定特征的分叉值。</p></li><li><p>见<a href="https://chenzk1.github.io/2019/03/14/Decision%20Tree/">Decision Tree</a></p></li></ul></li><li><p>交叉熵</p><ul><li>前面提到：信息熵是最优策略下，消除系统不确定性的最小代价。这里的前提是：<strong>我们得到了系统的真实分布</strong>。</li><li>实际中，一般难以获知系统真实分布，所以要以假设分布去近似。<strong>交叉熵：用来衡量在给定的真实分布下，使用非真实分布所指定的策略消除系统的不确定性所需要付出的努力的大小</strong>。$CEH(p,q)=\sum_{k=1}^np_k\log_2\frac{1}{q_k}$，注意这里log中是q，是基于非真实分布q的信息量对真实分布的期望。</li><li>当假设分布$q_k$与真实分布$p_k$相同时，交叉熵最低，等于信息熵，所以得到的策略为最优策略。<blockquote><p>在机器学习中的分类算法中，我们总是最小化交叉熵，因为交叉熵越低，就证明由算法所产生的策略最接近最优策略，也间接证明我们算法所算出的非真实分布越接近真实分布。</p></blockquote></li></ul><blockquote><p>例如：在逻辑斯蒂回归或者神经网络中都有用到交叉熵作为评价指标，其中p即为真实分布的概率，而q为预测的分布，以此衡量两不同<strong>分布</strong>的相似性。 </p><ul><li>如何衡量不同<strong>策略</strong>的差异：相对熵</li></ul></blockquote></li><li><p>相对熵/K-L散度</p><ul><li>用来衡量两个取值为正的函数或概率分布之间的差异。两者相同相对熵为0</li><li>使用非真实分布q的交叉熵，与使用真实分布p的的信息熵的差值：相对熵，又称K-L散度。</li><li>$KL(p,q)=CEH(p,q)-H(p)=\sum_{i=1}^np(x_i)\log\frac{p(x_i)}{q(x_i)}$</li></ul></li><li><p>联合熵</p><ul><li>H(X,Y) 随机变量X,Y联合表示的信息熵</li></ul></li><li><p>互信息</p><ul><li>H（X；Y）俩变量交集，也记作I(X;Y)</li><li>H（X；Y) = H(X,Y)-H(Y|X)-H(X|Y)</li><li>I(X;Y)=KL(P(X,Y), P(X)P(Y))</li></ul></li></ol><ul><li>互信息越小，两变量独立性越强，P(X,Y)与P(X)P(Y)差异越小，P(X,Y)与P(X)P(Y)的相对熵越小  </li><li>相对熵(p,q) = 信息熵(p) - 交叉熵(p,q)</li><li>信息增益(Y|X) = 信息熵(Y) - 条件熵(Y|X)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CTR_LR/Poly2/FM/FFM</title>
      <link href="/2019/05/26/CTR/"/>
      <url>/2019/05/26/CTR/</url>
      
        <content type="html"><![CDATA[<ol><li>LR</li></ol><ul><li>问题：特征之间无相关性</li></ul><ol start="2"><li>Ploy2</li></ol><ul><li>暴力加入两两特征组合（权重*两特征点积）</li><li>问题：大部分特征是稀疏的，得到的特征值都是0，所以梯度更新时，因为大部分feature为0所以梯度并不会更新</li></ul><ol start="3"><li>FM(Factorization Machine、因子机)</li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> CTR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tips in DS Competition</title>
      <link href="/2019/04/11/Tips%20in%20DS%20Competition/"/>
      <url>/2019/04/11/Tips%20in%20DS%20Competition/</url>
      
        <content type="html"><![CDATA[<h2 id="处理缺失数据"><a href="#处理缺失数据" class="headerlink" title="处理缺失数据"></a>处理缺失数据</h2><ul><li>删除</li><li>imputation: mean mode …</li><li>imputation + missing_flag</li><li>…</li></ul><h2 id="Categorial-Columns"><a href="#Categorial-Columns" class="headerlink" title="Categorial Columns"></a>Categorial Columns</h2><ul><li>对于种类不是很多的：onehot encoder<ul><li>sklearn.preprocessing.OneHotEncoder: 如果使用线性模型，存在一个问题就是生成的n列是线性相关的，因此要满足线性无关就要删除其中一列。该类提供了drop_first参数</li></ul></li><li>不用label encoder的原因：label encoder引入了大小顺序</li></ul><h2 id="XGBOOST"><a href="#XGBOOST" class="headerlink" title="XGBOOST"></a>XGBOOST</h2>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> DataScience </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVD&amp;PCA</title>
      <link href="/2019/04/06/SVD&amp;PCA/"/>
      <url>/2019/04/06/SVD&amp;PCA/</url>
      
        <content type="html"><![CDATA[<h3 id="特征值分解-Eigen-Value-Decomposition"><a href="#特征值分解-Eigen-Value-Decomposition" class="headerlink" title="特征值分解(Eigen Value Decomposition)"></a>特征值分解(Eigen Value Decomposition)</h3><p>Ax = λx</p><p>-&gt; A = WΣW^(-1)</p><p>其中Σ为对角阵，对角的值是A的特征值；W的列向量为对应的特征向量</p><p>对W标准化后，即Wi^(T).wi = 1</p><p>所以W^(T).W = I –&gt; W^(T) = W^(-1)</p><p><strong>W经标准化后为酉矩阵</strong></p><p>-&gt; A = WΣW^(T)</p><p><strong>!!A必须是方阵</strong></p><h3 id="SVD-Singular-Value-Decomposition"><a href="#SVD-Singular-Value-Decomposition" class="headerlink" title="SVD(Singular Value Decomposition)"></a>SVD(Singular Value Decomposition)</h3><ul><li>可以对<strong>非方阵</strong>分解</li><li>A = UΣV^(T), A: m x n, U: m x m, V: n x n, Σ: m x n; <strong>U和V都为酉矩阵</strong>，Σ主对角线上元素为奇异值</li><li>UVΣ的求解：<ul><li>U: AA^(T) = UΣ1U^(T)</li><li>V: A^(T)A = VΣ2V^(T)</li><li>Σ: A = UΣV^(T) =&gt; AV = UΣ =&gt; U^(T)AV = Σ =&gt; σi = Avi/ui</li></ul></li><li>性质：可以用几个最大的奇异值及其左右奇异向量近似原矩阵</li><li>应用：降维，数据压缩，去噪声；也可用于NLP，如LSA…</li></ul><h3 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h3><h4 id="基"><a href="#基" class="headerlink" title="基"></a>基</h4><p>由线性不相关的向量组成，有时会取正交。</p><h4 id="坐标变换-amp-矩阵相乘"><a href="#坐标变换-amp-矩阵相乘" class="headerlink" title="坐标变换&amp;矩阵相乘"></a>坐标变换&amp;矩阵相乘</h4><p>AB = C，B矩阵的每一个列向量变换到以A矩阵的行向量为基表示的空间中，最终得到的向量的维度（C的行数）取决于基的个数 –&gt; 可用于降、升维</p><h4 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h4><ul><li>降维的目标：维数变低&amp;尽量保留更多的信息。</li><li>对于二维降到一维，要保留更多的信息，则原始向量在基向量上的投影应相隔距离尽量远 –&gt; 大方差</li><li><p>对于高维数据，如3维到2维，若只遵循大方差的原则，则两个基向量会相隔很近 –&gt; 信息不够分散 –&gt; 基向量之间的相关系数应尽量小</p></li><li><p>方差：单个随机变量之间的离散程度；协方差：多个随机变量之间的相似性</p></li></ul><p><strong>综上 –&gt; 协方差矩阵</strong></p><h4 id="协方差矩阵"><a href="#协方差矩阵" class="headerlink" title="协方差矩阵"></a>协方差矩阵</h4><p>协方差矩阵对角线上是原矩阵的方差，其他位置的元素是原矩阵两两向量之间的协方差 –&gt; 协方差矩阵是实对称矩阵 –&gt; 可逆</p><ul><li>原始问题即协方差矩阵的对角化</li></ul><h4 id="协方差矩阵对角化"><a href="#协方差矩阵对角化" class="headerlink" title="协方差矩阵对角化"></a>协方差矩阵对角化</h4><ul><li>原向量X，对应的协方差矩阵为C，P为基向量组成的变换矩阵；X经变换后为Y，Y=PX，Y的协方差矩阵为D，则：设X Y的期望为0，<br>D = YY^(T) / m = PX X^(T)P^(T) / m = PCP^(T)</li><li>PCA即寻找矩阵P使得 PCP^(T)是一个对角矩阵，且对角线上的值从大到小排列，取前k个值，以及对应P中的k个向量，即可将原n维矩阵降维至k维 –&gt; D对角线上的值即特征值，P为特征向量组成的矩阵</li></ul><h4 id="PCA-1"><a href="#PCA-1" class="headerlink" title="PCA"></a>PCA</h4><ul><li>总结：寻找实现协方差矩阵对角化的矩阵P，并应用P对原有数据进行变换</li><li>算法步骤<br>设有m条n维数据：</li></ul><ol><li>将原始数据按列组成n行m列矩阵X</li><li>将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</li><li>求出协方差矩阵C = XX^(T) / m</li><li>求出协方差矩阵的特征值及对应的特征向量</li><li>将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</li><li>Y=PX即为降维到k维后的数据</li></ol><ul><li>优点：降低数据特征维度，减少数据存储量；加快运行速度</li><li>注意事项：<strong>量纲敏感性</strong>，最好进行量纲统一化；适用于大样本，小样本的话建议因子分析法</li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Topic Model__TF-IDF/LSA/pLSA/NMF</title>
      <link href="/2019/04/05/Topic%20Model__TF-IDF_LSA_pLSA_NMF/"/>
      <url>/2019/04/05/Topic%20Model__TF-IDF_LSA_pLSA_NMF/</url>
      
        <content type="html"><![CDATA[<h1 id="Topic-Model"><a href="#Topic-Model" class="headerlink" title="Topic Model"></a>Topic Model</h1><p><em><strong>主题模型即在大量文档中发现潜在主题的统计模型</strong></em></p><p>主题模型是一种<strong>生成式有向图</strong>模型，即文档以一定概率选择主题，而主题是单词的概率分布。</p><p>文档 –&gt; 主题 –&gt; 单词</p><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><ul><li>TF: term frequency —— (# occurrences of term t in document) / (# of words in documents)</li><li>IDF: inverse document frequency —— log(# of documents / # documents with term t in it)</li><li><p>TF * IDF</p></li><li><p>一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。一个词term的重要性与其在整个文件中出现的频率成正比；与它在语料库中的频率成反比。</p></li><li>可以用来提取关键词</li><li>有不足：比如某次在每个文件中都出现，这个词可能是is are之类的无用词，也可能是可以代表该文件库的关键词—-&gt;结合停用词处理？；没有考虑语义关联</li></ul><h2 id="LSA-LSI-Latent-Semantic-Analysis-Indexing"><a href="#LSA-LSI-Latent-Semantic-Analysis-Indexing" class="headerlink" title="LSA/LSI(Latent Semantic Analysis/Indexing)"></a>LSA/LSI(Latent Semantic Analysis/Indexing)</h2><p>潜在语义分析/检索</p><ul><li>假设有m个输入文档，每个文档有n个词项，则可以组成一个term-document的稀疏矩阵A∈Rm*n，它的行对应词项、列对应文档；Aij对应第i个文档的第j个词项，可以通过TF-IDF、词项在文档中出现的次数等方式确定矩阵每个元素的权重作为计算输入。经过SVD分解后将奇异值从大到小排列，取前k个最大的奇异值作为对原矩阵A的近似表示，Σ中的每个奇异值代表了潜在语义的重要度。</li><li>通过一次SVD分解就可以得到主题模型，同时解决语义的问题，但是计算得到的矩阵U、V中经常存在负数；可以通过计算词项（U的行）、文档（V的行or VT的列）之间的余弦相似度得到词项与词项、文档与文档之间的相似度；还可以对U、V中的词项和文档直接进行聚类，提取语义级别的近义词集合，便于搜索且减少数据存储量。</li><li>LSA适用于较小规模数据，可用于文档分类/聚类、同义词/多义词检索、跨语言搜索；SVD的计算很耗时，且潜在语义的数量k的选择对结果的影响非常大；LSA的原理简单但得到的不是概率模型，缺乏统计基础，矩阵中的负值难以解释，无法对应成现实中的概念。</li></ul><h2 id="pLSA-Potential-Latent-Semantic-Analysis-Indexing"><a href="#pLSA-Potential-Latent-Semantic-Analysis-Indexing" class="headerlink" title="pLSA(Potential Latent Semantic Analysis/Indexing)"></a>pLSA(Potential Latent Semantic Analysis/Indexing)</h2><p>引入了隐含变量，并使用了EM算法求解</p><h2 id="NMF-Non-negative-Matrix-Factorization"><a href="#NMF-Non-negative-Matrix-Factorization" class="headerlink" title="NMF(Non-negative Matrix Factorization)"></a>NMF(Non-negative Matrix Factorization)</h2><ul><li>V ≈ WH，其中V∈Rn<em>m为文档-单词矩阵，W∈Rn</em>r体现文档和主题的概率相关度，H∈Rr*m体现单词和主题的概率相关度。<br><img src="https://img-blog.csdn.net/20180713123152376?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpdXk5ODAz/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="V=WH"></li><li>目的：V与WH的误差最小化，度量方式可选择欧几里得距离、KL散度…</li><li>NMF的目标函数中共包含了n<em>r+r</em>m个参数，可以使用梯度下降法、拟牛顿法、坐标轴下降法等进行求解。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RNN &amp; LSTM &amp; GRU</title>
      <link href="/2019/04/04/LSTM/"/>
      <url>/2019/04/04/LSTM/</url>
      
        <content type="html"><![CDATA[<ol><li>RNN</li></ol><p><img src="https://pic1.zhimg.com/v2-206db7ba9d32a80ff56b6cc988a62440_r.jpg" alt="示意图"></p><ul><li>动机：RNN之前，语言模型主要是N-gram，N可以变动，其作用是某位置的词取决于前N个词，所以该方法对任意序列的文本处理存在问题。</li><li>RNN：<em>理论上</em>可以看到往前看/往后看任意的长度。</li><li>最简单的RNN，即t时刻隐藏层的值既取决于t时刻的输入，也取决于t-1时刻的隐藏层的值。</li><li>双向循环网络，应用了下文的信息；深度循环网络，使用了多层隐藏层，更复杂了。</li></ul><p>一个教程：<a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="noopener">RNN</a></p><ol start="2"><li>LSTM(Long Short Term)</li></ol><ul><li>动机：由RNN的机理可知，如果每层向下一个时刻传输的权重w大于1，则容易产生梯度爆炸问题，小于1，则会产生梯度消失<br>一个对比图<br><img src="https://pic4.zhimg.com/v2-e4f9851cad426dfe4ab1c76209546827_r.jpg" alt="RNN&amp;LSTM"><br>RNN只有一个传递状态ht，而LSTM有两个传递状态：cell state, hidden state。<br>其中cell state是上一序列的隐藏层输出加上一些东西（有点类似于RNN中的ht）；而ht变化会比较大。</li><li>过程：<ul><li>计算四个状态：<br><img src="https://pic4.zhimg.com/80/v2-15c5eb554f843ec492579c6d87e1497b_hd.jpg" alt="z&amp;zi"><br><img src="https://pic1.zhimg.com/80/v2-d044fd0087e1df5d2a1089b441db9970_hd.jpg" alt="zo&amp;zf"><br>z是输入，用了tanh，取值范围为-1，相当于归一化；zi&amp;zo&amp;zf为门控，用了sigmoid，输出为0~1，与某值相乘后是对该值的选择，其中i是information，f是forget，o是output；</li><li>四个状态在LSTM中的应用：<br><img src="https://pic2.zhimg.com/80/v2-556c74f0e025a47fea05dc0f76ea775d_hd.jpg" alt="LSTM结构"><ul><li>忘记：zf点乘ct，即通过zf忘记不重要的</li><li>选择记忆：对于输入信息z，点乘zi，进行有选择的记忆</li><li>输出：对上一阶段的c通过tanh进行放缩，再通过zo控制输出</li></ul></li></ul></li></ul><ol start="3"><li>GRU(Gate Recurrent Unit)</li></ol><ul><li>同样为了解决梯度问题</li><li>比LSTM：运算量小</li><li>使用了同一个门控状态控制记忆和忘记</li></ul><p>具体：<br><a href="https://zhuanlan.zhihu.com/p/32481747" target="_blank" rel="noopener">原文</a></p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FP Tree算法</title>
      <link href="/2019/03/19/FP%20Tree/"/>
      <url>/2019/03/19/FP%20Tree/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/pinard/p/6307064.html" target="_blank" rel="noopener">blog</a></p><ol><li><p>FP Tree是Aprior算法的优化</p><p>优化点在于：Aprior需要多次扫描数据以查询频繁项，FP Tree使用了树结构，提高了算法运行效率。</p></li><li><p>FP Tree数据结构</p><ul><li>项头表。格式为： | 项 | 该项出现的次数（降序排列） </li><li>FP Tree。将原始数据集映射为一个FP Tree。</li><li>节点链表。所有项头表里的1项频繁集都是一个节点链表的头，它依次指向FP树中该1项频繁集出现的位置。这样做主要是方便项头表和FP Tree之间的联系查找和更新，也好理解。</li></ul></li><li><p>项头表建立</p><ul><li>第一次扫描数据：得到频繁一项集的计数，删除支持度小于阈值的项，并将剩余频繁集放入项头表，降序排列。</li><li><p>第二次扫描数据：从<strong>原始数据</strong>中删除非频繁项集，并对<strong>每一条数据</strong>按支持度降序排列其中的非频繁项集。</p></li><li><p>e.g.<br><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119161846125-505903867.png" alt="e.g."></p></li></ul></li><li><p>FP Tree建立</p><ul><li>以null为根节点</li><li><p>按照项头表的顺序，从第一条数据开始插入频繁项集，每条数据中排序靠前的为祖先节点，反之为子孙节点，每个节点均置1<br><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119163935296-1386696266.png" alt="e.g."> </p></li><li><p>接下来进行下一条数据的插入。当已存在频繁项时，在原有数字上加一即可（注意在插入时，是每一层每一层插入，不能跳过某层直接在已有节点上加一）</p></li><li>…</li><li><img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119165427593-1237891371.png" alt="最终"></li></ul></li><li><p>节点链表</p><p>节点链表即项头表中每个频繁项集都有一个指针指向FP Tree的相应节点。</p></li><li><p>FP Tree挖掘</p><p>得到了FP树和项头表以及节点链表，我们首先要从项头表的底部项依次向上挖掘。对于项头表对应于FP树的每一项，我们要找到它的条件模式基。所谓条件模式基是以我们要挖掘的节点作为叶子节点所对应的FP子树。得到这个FP子树，我们将子树中每个节点的的计数设置为叶子节点的计数，并删除计数低于支持度的节点。从这个条件模式基，我们就可以递归挖掘得到频繁项集了。</p><ul><li>先从最底下的F节点开始，我们先来寻找F节点的条件模式基，由于F在FP树中只有一个节点，因此候选就只有下图左所示的一条路径，对应{A:8,C:8,E:6,B:2, F:2}。我们接着将所有的祖先节点计数设置为叶子节点的计数，即FP子树变成{A:2,C:2,E:2,B:2, F:2}。一般我们的条件模式基可以不写叶子节点，因此最终的F的条件模式基如下图右所示。 <img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119170723421-1812925376.png" alt="e.g."></li><li>通过它，我们很容易得到F的频繁2项集为{A:2,F:2}, {C:2,F:2}, {E:2,F:2}, {B:2,F:2}。递归合并二项集，得到频繁三项集为{A:2,C:2,F:2}，{A:2,E:2,F:2},…还有一些频繁三项集，就不写了。当然一直递归下去，最大的频繁项集为频繁5项集，为{A:2,C:2,E:2,B:2,F:2}</li><li>F挖掘完了，我们开始挖掘D节点。D节点比F节点复杂一些，因为它有两个叶子节点，因此首先得到的FP子树如下图左。我们接着将所有的祖先节点计数设置为叶子节点的计数，即变成{A:2, C:2,E:1 G:1,D:1, D:1}此时E节点和G节点由于在条件模式基里面的支持度低于阈值，被我们删除，最终在去除低支持度节点并不包括叶子节点后D的条件模式基为{A:2, C:2}。通过它，我们很容易得到D的频繁2项集为{A:2,D:2}, {C:2,D:2}。递归合并二项集，得到频繁三项集为{A:2,C:2,D:2}。D对应的最大的频繁项集为频繁3项集。 <img src="https://images2015.cnblogs.com/blog/1042406/201701/1042406-20170119171924093-1331841220.png" alt="e.g."></li></ul></li><li><p>算法具体步骤</p><ul><li>扫描数据，得到所有频繁一项集的的计数。然后删除支持度低于阈值的项，将1项频繁集放入项头表，并按照支持度降序排列。</li><li>扫描数据，将读到的原始数据剔除非频繁1项集，并按照支持度降序排列。</li><li>读入排序后的数据集，插入FP树，插入时按照排序后的顺序，插入FP树中，排序靠前的节点是祖先节点，而靠后的是子孙节点。如果有共用的祖先，则对应的公用祖先节点计数加1。插入后，如果有新节点出现，则项头表对应的节点会通过节点链表链接上新节点。直到所有的数据都插入到FP树后，FP树的建立完成。</li><li>从项头表的底部项依次向上找到项头表项对应的条件模式基。从条件模式基递归挖掘得到项头表项项的频繁项集。</li><li>如果不限制频繁项集的项数，则返回步骤4所有的频繁项集，否则只返回满足项数要求的频繁项集。</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apriori 关联规则挖掘</title>
      <link href="/2019/03/18/Apriori%20%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98/"/>
      <url>/2019/03/18/Apriori%20%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99%E6%8C%96%E6%8E%98/</url>
      
        <content type="html"><![CDATA[<ol><li><p>问题引入</p><ul><li>经常被同时购买的商品可以摆近一点，刺激购买欲望</li><li>通过附属产品优惠的方式，刺激主产品的销售</li></ul></li><li><p>总体思想</p><p>逐层搜索迭代，通过K-1项集迭代出K项集。<br>Aprior是用于压缩搜索空间。</p></li><li><p>概念</p><ul><li>支持度：<br>关联规则A-&gt;B的支持度support=P(AB)，同时发生的概率</li><li>置信度：<br>confidence=P(A|B)</li><li>k项集：<br>事件A中包含k个元素，成为k项集；若事件A满足最小支持度阈值，称其为频繁k项集</li><li>由频繁项集产生强关联规则<ul><li>K维数据项集LK是频繁项集的必要条件是它所有K-1维子项集也为频繁项集，记为LK-1　</li><li>如果K维数据项集LK的任意一个K-1维子集Lk-1，不是频繁项集，则K维数据项集LK本身也不是最大数据项集。</li><li>Lk是K维频繁项集，如果所有K-1维频繁项集合Lk-1中包含LK的K-1维子项集的个数小于K，则Lk不可能是K维最大频繁数据项集。</li><li>同时满足最小支持度阀值和最小置信度阀值的规则称为强规则。</li></ul></li><li>e.g.<br>顾客购买记录的数据库D，包含6个事务。项集I={网球拍,网球,运动鞋,羽毛球}。考虑关联规则：网球拍网球，事务1,2,3,4,6包含网球拍，事务1,2,6同时包含网球拍和网球，支持度，置信度。若给定最小支持度，最小置信度，关联规则网球拍网球是有趣的，认为购买网球拍和购买网球之间存在强关联。</li></ul></li><li><p>算法步骤</p><p>Apriori算法过程分为两个步骤：</p><ul><li>第一步通过迭代，检索出事务数据库中的所有频繁项集，即支持度不低于用户设定的阈值的项集；</li><li>第二步利用频繁项集构造出满足用户最小信任度的规则。</li></ul><p>具体做法就是：</p><p>首先找出频繁1-项集，记为L1；然后利用L1来产生候选项集C2，对C2中的项进行判定挖掘出L2，即频繁2-项集；不断如此循环下去直到无法发现更多的频繁k-项集为止。每挖掘一层Lk就需要扫描整个数据库一遍。算法利用了一个性质：</p><p>Apriori 性质：任一频繁项集的所有非空子集也必须是频繁的。意思就是说，生成一个k-itemset的候选项时，如果这个候选项有子集不在(k-1)-itemset(已经确定是frequent的)中时，那么这个候选项就不用拿去和支持度判断了，直接删除。具体而言：</p><ul><li><p>连接步:<br>为找出Lk（所有的频繁k项集的集合），通过将Lk-1（所有的频繁k-1项集的集合）与自身连接产生候选k项集的集合。候选集合记作Ck。设l1和l2是Lk-1中的成员。记li[j]表示li中的第j项。假设Apriori算法对事务或项集中的项按字典次序排序，即对于（k-1）项集li，li[1]&lt;li[2]&lt;……….&lt;li[k-1]。将Lk-1与自身连接，如果(l1[1]=l2[1])&amp;&amp;( l1[2]=l2[2])&amp;&amp;……..&amp;&amp; (l1[k-2]=l2[k-2])&amp;&amp;(l1[k-1]&lt;l2[k-1])，(这里的作用是为了保证不产生重复的k项集)，那认为l1和l2是可连接。连接l1和l2 产生的结果是{l1[1],l1[2],……,l1[k-1],l2[k-1]}。</p></li><li><p>剪枝步:<br>CK是LK的超集，也就是说，CK的成员可能是也可能不是频繁的。通过扫描所有的事务（交易），确定CK中每个候选的计数，判断是否小于最小支持度计数，如果不是，则认为该候选是频繁的。为了压缩Ck,可以利用Apriori性质：任一频繁项集的所有非空子集也必须是频繁的，反之，如果某个候选的非空子集不是频繁的，那么该候选肯定不是频繁的，从而可以将其从CK中删除。</p></li></ul></li><li><p>e.g.</p><p><a href="https://blog.csdn.net/u011067360/article/details/24810415" target="_blank" rel="noopener">例子</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KNN</title>
      <link href="/2019/03/14/knn/"/>
      <url>/2019/03/14/knn/</url>
      
        <content type="html"><![CDATA[<ol><li><p>算法步骤</p><ul><li>给定已经分好类的训练集</li><li>对给定的测试数据，计算其与训练集中每一个样本的距离</li><li>取距离最小的前K个，按多数表决的方法决定属于哪一类</li></ul></li><li><p>注意事项</p><ul><li>数据要fair</li><li>可以通过对距离近的样本点加更大的权重优化</li><li>计算量较大</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> KNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ROC AUC, Accuracy, Recall, F1</title>
      <link href="/2019/03/14/ROC%20%20AUC/"/>
      <url>/2019/03/14/ROC%20%20AUC/</url>
      
        <content type="html"><![CDATA[<p>类不平衡（class imbalance）现象：即负样本比正样本多很多（或者相反）</p><ol><li>ACCURACY/RECALL/F1</li></ol><ul><li><p>Accuracy：TP/(TP+TN) 即预测对的数据个数与总数据个数的比</p><p>  <strong><em>A和R的问题在于如果样本不平衡，则参考意义很小</em></strong></p></li><li><p>准确率（Precision）：P=TP/(TP+FP)。通俗地讲，就是预测正确的正例数据占预测为正例数据的比例。</p><p>  <strong>针对判别结果 查准率</strong></p></li><li><p>召回率（Recall）：R=TP/(TP+FN)。通俗地讲，就是预测为正例的数据占实际为正例数据的比例。</p><p>  <strong>针对样本 查全率</strong></p></li><li>F1 = 2*P*R/(P+R)：既有P又有R</li><li><p>PRC， precision recall curve，与下面的ROC一样，先看是否光滑，光滑的话说明过拟合不大。越往右上越好。</p><p>  <strong><em>A和R的问题在于如果样本不平衡，则参考意义很小</em></strong></p></li></ul><ol start="2"><li>ROC AUC（receiver operating characteristic curve）</li></ol><p>TPR=TP/(TP+FN)=TP/actual positives 也就是Recall<br>FPR=FP/(FP+TN)=FP/actual negatives<br>ROC是由点（TPR,FPR）组成的曲线，AUC就是ROC的面积。AUC越大越好。</p><ul><li><p>画法：ROC曲线其实是多个混淆矩阵的结果组合，如果在上述模型中我们没有定好阈值，而是将模型预测结果从高到低排序，将每个概率值依次作为阈值，那么就有多个混淆矩阵。对于每个混淆矩阵，我们计算两个指标TPR（True positive rate）和FPR（False positive rate），TPR=TP/(TP+FN)=Recall，TPR就是召回率。FPR=FP/(FP+TN)，FPR即为实际为好人的人中，预测为坏人的人占比。我们以FPR为x轴，TPR为y轴画图，就得到了ROC曲线。</p><p><strong><em>也就是说，ROC曲线的阈值不是多次运行模型得到的，是同一个模型中通过对所得结果按照概率的排序得到了一个threshold</em></strong></p></li><li>原理：<ul><li>在画图描点过程中，每取一个样本，会以此样本被预测为1的概率作为阈值，概率排序在此之上的样本认为是1，之下的样本会被认为是0（解释了ROC为何与样本预测值的排序有关）。</li><li>AUC的值代表了：取真实label为1和为0的两个样本，其中真样本被预测为1的概率大于假样本被预测为1的概率这一事件的概率。假设有M个真样本，N个假样本，M个真样本的预测概率升序排序，其值从大到小分别为rank_1…rank_m,例如6个真4个假，假设真样本概率最高的排序为10，则比它低的假样本有10-6=4个，下一个排序为8，则比它低的假样本有8-(6-1)=3个，此概率为：（rank_1 - m + rank_2 - (m-1) + rank_m - 1）/ M*N</li><li>AUC=0.5时，任意一个样本被判断为真和判断为假的概率相等</li><li>e.g 当threshold取为0.5时：<br><a href="https://cdn-images-1.medium.com/max/1600/1*Uu-t4pOotRQFoyrfqEvIEg.png" target="_blank" rel="noopener">!ex0</a><br>此时AUC=1：<br><a href="https://cdn-images-1.medium.com/max/800/1*HmVIhSKznoW8tFsCLeQjRw.png" target="_blank" rel="noopener">!ex00</a><br>AUC=0.7:<br><a href="https://cdn-images-1.medium.com/max/1600/1*yF8hvKR9eNfqqej2JnVKzg.png" target="_blank" rel="noopener">!</a><br><a href="https://cdn-images-1.medium.com/max/800/1*-tPXUvvNIZDbqXP0qqYNuQ.png" target="_blank" rel="noopener">!</a><br>AUC=0.5:<br><a href="https://cdn-images-1.medium.com/max/1600/1*iLW_BrJZRI0UZSflfMrmZQ.png" target="_blank" rel="noopener">!</a><br><a href="https://cdn-images-1.medium.com/max/800/1*k_MPO2Q9bLNH9k4Wlk6v_g.png" target="_blank" rel="noopener">!</a><br>AUC=0:<br><a href="https://cdn-images-1.medium.com/max/1600/1*aUZ7H-Lw74KSucoLlj1pgw.png" target="_blank" rel="noopener">!</a><br><a href="https://cdn-images-1.medium.com/max/800/1*H7JGQbaa06BUab6tvGNZKg.png" target="_blank" rel="noopener">!</a></li></ul></li><li>一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting。</li><li><p>越往左上越好。</p><p>  所以使用ROC的话，它会先对预测到的结果进行排序，然后再根据排序的结果画图，所以他的曲线形状不会因为数据不平衡而发生大的改变。<br>  <strong>但是当数据极度不平衡时，ROC仍然有问题，下面的PRC表现更好。</strong></p></li></ul><p><a href="https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5" target="_blank" rel="noopener">参考1</a><br><a href="https://www.zhihu.com/question/39840928" target="_blank" rel="noopener">参考2</a></p><ol><li>PRC， precision recall curve，与上面的ROC一样，先看是否光滑，光滑的话说明过拟合不大。越往右上越好。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Metrics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Presentation of WNSP and IS</title>
      <link href="/2019/03/14/Presentation%20of%20WNSP%20and%20IS/"/>
      <url>/2019/03/14/Presentation%20of%20WNSP%20and%20IS/</url>
      
        <content type="html"><![CDATA[<h1 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h1><ul><li>Behavioral biomertics is the study of individual patterns(hand-writing, typing, mouse movements).</li><li>最早：二战中的电报员keying pattern</li><li>password hardening(secondary authentication)、password-less logins<ul><li>Banks use typing information as an additional layer of security.</li><li>Google is developing methods to authenticate users on mobile devices without passwords.</li></ul></li><li>human chosen passwords are far from safe -&gt; additional authentication -&gt; explicit methods are usually disruptive to the user -&gt; use behavioral biometrics</li></ul><h1 id="goal-design-a-adversarial-algorithms"><a href="#goal-design-a-adversarial-algorithms" class="headerlink" title="goal: design a adversarial algorithms"></a>goal: design a adversarial algorithms</h1><ul><li>前提：the attacker has access to the user’s password, but needs to overcome a keystroke dynamics based authentication layer</li></ul><h1 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h1><h2 id="behavioral-biometrics"><a href="#behavioral-biometrics" class="headerlink" title="behavioral biometrics"></a>behavioral biometrics</h2><blockquote><p>hand-writing, typing, mouse movements, touchscreen swipes, gait analysis</p></blockquote><h1 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h1><h2 id="Experimental-Setup"><a href="#Experimental-Setup" class="headerlink" title="Experimental Setup"></a>Experimental Setup</h2><h3 id="Protocols"><a href="#Protocols" class="headerlink" title="Protocols"></a>Protocols</h3><p><em>for collecting new data, and selecting new samples for training and testing</em></p><ol><li><p>collectign MTurk dataset</p><ul><li>pre-processing: drop any malformed samples(due to a combination of reasons that include: different behavior of browsers, differences in internet speed, or other noise as the subjects took the study simultaneously)</li><li>describe the protocol for selecting samples for training and testing, and creating adversarial samples across all datasets.</li><li></li></ul><p>生物行为学有两种分类器：一类分类器和两类分类器。前者只用正确样本，后者还会用到假样本。一般用一类分类器：1. because it is very impractical to expect negative samples for an arbitrary password.2. both the two class classifiers, and one class classifiers appear to give similar EER scores</p></li><li><p>Genuine User Samples真实样本<br>use the first half of the samples for training, the second half of the samples for testing</p></li><li>imposter training samples虚假训练样本<br>随机选择，与真实样本同数量</li><li>imposter testing samples虚假测试样本<br>DSN: first four samples of every user besides the genuine user<br>MYurk and touchscreen swipes dataset: randomly sampled the same number of impostor samples as the genuine user’s test samples</li><li>adversary对抗样本<br>The <strong>Targeted K-means++ adversary</strong> used all the samples from the data set excluding the ones from the target user and the ones used for training and testing the user’s classifier. For <strong>the Indiscriminate K-means++ </strong>adversary, we conducted a new MTurk study, as described before, a few months after the original study. We used all the samples from this new study. In Algorithm 2, we set the parameter “SAMPLE-SIZE” to 20000.</li></ol><h3 id="Detection-Algorithms-检测算法"><a href="#Detection-Algorithms-检测算法" class="headerlink" title="Detection Algorithms 检测算法"></a>Detection Algorithms 检测算法</h3><h4 id="One-class-classifiers"><a href="#One-class-classifiers" class="headerlink" title="One class classifiers"></a>One class classifiers</h4><ul><li>Manhattan distance<br>$$ \sum_{i=1}^m\frac{\left|{x_i-y_i}\right|}m $$</li><li>Gaussian高斯<br>training samples are modeled as a Gaussian distribution based on their mean and standard deviation</li><li><p>Gaussian mixture高斯混合模型</p><blockquote><p><a href="https://blog.csdn.net/jinping_shi/article/details/59613054" target="_blank" rel="noopener">https://blog.csdn.net/jinping_shi/article/details/59613054</a> </p></blockquote><blockquote><p>高斯混合模型（Gaussian Mixed Model）指的是多个高斯分布函数的线性组合，理论上GMM可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布的情况（或者是同一类分布但参数不一样，或者是不同类型的分布，比如正态分布和伯努利分布）。<br>$$ \sum_{k=1}^Kπ_kN\left(x|μ_k,\sum_k\right) $$<br>$$ \sum_{k=1}^Kπ_k=1 $$<br>$$ 0 ≤ Kπ_k ≤ 1 $$<br>即$π_k$相当于每个分量的权重<br>GMM常用于聚类。如果要从 GMM 的分布中随机地取一个点的话，实际上可以分为两步：首先随机地在这 K 个 Component 之中选一个，每个 Component 被选中的概率实际上就是它的系数πkπk \pi_k ，选中 Component 之后，再单独地考虑从这个 Component 的分布中选取一个点就可以了──这里已经回到了普通的 Gaussian 分布，转化为已知的问题。将GMM用于聚类时，假设数据服从混合高斯分布（Mixture Gaussian Distribution），那么只要根据数据推出 GMM 的概率分布来就可以了；然后 GMM 的 K 个 Component 实际上对应KKK个 cluster 。根据数据来推算概率密度通常被称作 density estimation 。</p></blockquote></li><li>one class SVM<br>used the Support Vector Machine(SVM) implementation in sklearn, with radial basis function (RBF) kernel, and kernel parameter 0.9.</li><li>Autoencoder and Contractive Autoencoder自动编码和收缩性自动编码<br>With the advent of deep learning, researchers have started using variants of neural networks in the domain of cybersecurity. One of the key structures used in the past are autoencoders and contractive autoencoders<br>随着深度学习的到来，研究人员开始在网络安全领域使用神经网络的变体。过去使用的关键结构之一是自动编码器和压缩自动编码器</li></ul><h4 id="Two-class-classifiers"><a href="#Two-class-classifiers" class="headerlink" title="Two class classifiers"></a>Two class classifiers</h4><ul><li>Random Forest<br>used a model similar to the one described by Antal et al [4]. Random Forests with 100 trees was their best-performing classifier on the touchscreen swipes dataset. We used the Random Forest implementation in sklearn<br>我们使用了一个类似于Antal et al[4]所描述的模型。随机森林与100棵树是他们在触摸屏滑动数据集上表现最好的分类器。我们在sklearn中使用了Random Forest实现</li><li>Nearest Neighbor<br>Here we classify a test sample based on the majority label among a fixed number of its nearest neighbors in the training set. The neighbours are determined using Euclidean distance. We used the implementation in [32]<br>在测试样本中用最近邻</li><li>Fully Connected Neural Net全连接NN<br>We experimented with multiple variants of multi layer perceptron by using different hyper parameters. The network that performed the best had two hidden layers with 15 neurons each computing scores for genuine and impostor classes. There was no significant improvement in the performance of the network by increasing the number of layers or neurons per layer in the architecture of the neural network.<br>我们使用不同的超参数对多层感知器的多个变体进行了实验。表现最好的网络有两个隐藏层，每个层有15个神经元，计算真实和冒名顶替类的分数。在神经网络体系结构中，每层增加层数或神经元数量，网络性能没有显著改善。</li></ul><h4 id="Monaco’s-Normalization-Technique-标准化"><a href="#Monaco’s-Normalization-Technique-标准化" class="headerlink" title="Monaco’s Normalization Technique 标准化"></a>Monaco’s Normalization Technique 标准化</h4><p>The <strong>key insight of this technique</strong> was that a user’s classifier could normalize future input samples based only on the genuine user’s data given to it at the start. Essentially, this acts like a filtering step - and features that are too far from the mean of the genuine user’s fitting data get filtered out.<br>后续样本基于刚开始给定的真实输入样本来做标准化<br>这个标准化很重要，没这个就无法得出结果we do not even mention our results without this<br>normalization.</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><h3 id="Equal-error-rate"><a href="#Equal-error-rate" class="headerlink" title="Equal error rate"></a>Equal error rate</h3><table><thead><tr><th>Name of Classifier</th><th>DSN EER</th><th>MTurk EER</th></tr></thead><tbody><tr><td>Manhattan</td><td>0.091</td><td>0.097</td></tr><tr><td>SVM</td><td>0.087</td><td>0.097</td></tr><tr><td>Gaussian</td><td>0.121</td><td>0.109</td></tr><tr><td>Gaussian Mixture</td><td>0.137</td><td>0.135</td></tr><tr><td>…</td><td>…</td><td>…</td></tr></tbody></table><hr><p>(具体见表2)</p><p>注：没有标准化的EER都在0.15左右</p><h3 id="Keystroke-Results"><a href="#Keystroke-Results" class="headerlink" title="Keystroke Results"></a>Keystroke Results</h3><p>In this section we discuss the results of testing our adversaries on the DSN and MTurk datasets, which are summarized in Tables III, IV. We conducted the tests independently on each of the five passwords in the MTurk dataset, but for a more compact presentation, we average the results of all passwords. A few interesting highlights based on these results are given below<br>在本节中,我们讨论的结果,测试DSN和MTurk数据集,总结在表III、IV。我们进行独立测试在MTurk数据集中的密码。为了更紧凑的表示,我们把所有的结果平均之后显示出来。</p><h4 id="MasterKey-VS-K-means"><a href="#MasterKey-VS-K-means" class="headerlink" title="MasterKey VS K-means++"></a>MasterKey VS K-means++</h4><p>K-means++ performs better than MasterKey.<br>Figure 2 展示了最好的一类分类器和二类分类器下，Target K-means++和Indiscriminate K-means++以及MasterKey的性能对比<br>Targeted K-means++ seems to essentially <strong>be able<br>to compromise the security of all the users</strong> in the limit.</p><p>Table3展示了K-means++强于asterKey</p><p>本文中用到的样本量更大，选择train sample和test sample的protocol也不一样，但是EER与原文差不多。如图5所示。</p><p>As can be seen by Table V, and Figure 4, the results on this dataset show the same trends as seen in the keystroke dynamics datasets before. The first try which hits the mean of the impostor samples is not very successful here. This is particularly bad for an adversary like MasterKey which stays around the mean of the distribution, and is reflected in the results in Table V. But the K-means++ adversary is quickly able to explore the sample space to find more challenging queries and in 10 tries itself, breaks into a sizeable proportion of the  classifiers as in the keystrokes dataset. And in the limit, essentially all the user’s classifiers are compromised.由表V和图4可以看出，该数据集上的结果显示了与之前击键动力学数据集相同的趋势。第一次尝试就击中了冒名顶替样本的均值，但并不是很成功。这是特别糟糕的敌人像万能钥匙保持周围分布的均值,并反映在结果表诉。但k - means + +对手很快就能够探索样本空间中找到更有挑战性的查询和10次尝试本身,闯进了一相当大的比例的数据集分类器的按键。在极限情况下，基本上所有用户的分类器都被破坏了。</p><h1 id="Conclusion-and-future-work"><a href="#Conclusion-and-future-work" class="headerlink" title="Conclusion and future work"></a>Conclusion and future work</h1><p>Behavioral biometrics is a promising field of research, but it is not a reliable solution for authentication in its current state. <strong>行为生物识别技术是一个很有前途的研究领域，但在目前的状态下，它并不是一个可靠的认证解决方案。</strong>We proposed two adversarial agents that require a different amount of effort from the adversary. <strong>Both attack methods performed clearly better than the previously studied attack methods</strong> in the literature and show that current state of the art classifiers add little protection against such adversaries. In the case of Indiscriminate K-means++, more than its success rate, it is worrying for the keystroke dynamics systems that such an adversary could conduct its attack without any additional cost incurred to collect samples. Past research has focused much more on improving the classifiers against naive adversaries, but this work shows that a lot more research from the adversarialperspective is required before such authentication systems can be adopted in sensitive contexts.<br>The design of our K-means++ adversaries utilizes a <strong>common intuition about human behavior, which is that a person’s behavioral data belongs to a “cluster”, rather than being absolutely unique</strong>. Thus it is natural to expect such techniques to generalize to other types of behavioral data. The results on the touchscreen touchscreen swipes dataset also supports this claim.<br>我们提出了两种敌对代理人，它们需要不同于对手的努力。这两种攻击方法的性能明显优于文献中先前研究的攻击方法，表明当前的艺术分类器对这类敌人的保护很少。在不加区别的K-means++的情况下，对于击键动力学系统来说，这样的对手可以进行攻击而不需要额外的成本来收集样本，这比其成功率更令人担忧。过去的研究更多地关注于改进针对天真的对手的分类器，但这项工作表明，在这种身份验证系统可以在敏感的上下文中采用之前，需要从adversarialperspective的角度进行更多的研究。我们的k -means++敌人的设计利用了一种关于人类行为的共同直觉，即一个人的行为数据属于一个“集群”，而不是绝对独一无二的。因此，很自然地期望这些技术可以推广到其他类型的行为数据。触屏触摸屏上的结果也支持这一说法。<br>Of course, from a practical perspective, it is much harder to simulate an attack on a touchscreen based system, as opposed to a keystroke dynamics system, because of the diversity of the touchscreen features like pressure, finger size and so on. Unlike keystrokes - we can’t just write an easily automated script to carry out such an attack. This implies that a swipes based classifier is more secure for now. But given enough motivation, it is possible that methods could be devised to bypass such limitations. For instance, such attacks could be carried out by feeding false information to the android sensors, or in an extreme example, by building a robotic arm.<br>当然，从实际角度来看，由于触摸屏的压力、手指大小等特性的多样性，模拟攻击基于触摸屏的系统要比模拟击键动力学系统困难得多。与击键不同的是，我们不能仅仅编写一个易于自动化的脚本来执行这样的攻击。这意味着基于滑动的分类器现在更安全。但只要有足够的动力，就有可能设计出绕过这些限制的方法。例如，这种攻击可以通过向android传感器提供虚假信息来实施，或者在一个极端的例子中，通过制造机械手臂来实施。<br>Previous research has relied exclusively on the average Equal Error Rate scores across all subjects to measure the robustness of classifiers. To develop more robust behavioral biometric classifiers, it would be useful to <strong>benchmark against the adversarial agents proposed in this paper instead.</strong> For instance, one class classifiers have been the dominant method researched in the keystroke dynamics literature as they perform as well as the two class classifiers in terms of EER, while the two class classifiers are not practical because one can not expect impostor samples for arbitrary passwords. Yet, against both the adversarial algorithms, the two class classifiers performed clearly better than the one class classifiers. This suggests that a future direction of research would be to bridge the gap between the idealized and practical versions of such two class classifiers as explained in section IV A.<br>以往的研究完全依赖于所有科目的平均等错误率分数来衡量分类器的鲁棒性。为了开发出更健壮的行为生物特征分类器，我们将对本文提出的抗辩剂进行基准测试。例如，在击键力学文献中，一类分类器是主要的研究方法，因为它们的性能和EER的两个类分类器一样好，而这两个类分类器是不实用的，因为人们不能指望冒名顶替者样本来处理任意的密码。然而，与两种对抗性算法相比，这两个类分类器的性能明显优于一个类分类器。这表明，今后的研究方向将是弥补第四节a所解释的这两类分类器的理想化版本和实际版本之间的差距。<br>From the adversarial perspective, one possibility for future work would be to extend these methods to free text based classifiers. Free text classifiers utilize a continuous stream of input text, as opposed to fixed text passwords, in order to classify keystroke patterns. This leads to differences in the features and algorithms that are utilized for these classifiers. But conceptually, the Indiscriminate K-means++ adversary should be well suited to generate adversarial samples against free text classifiers as well.从敌对的角度来看，未来工作的一种可能是将这些方法扩展到基于自由文本的分类器。自由文本分类器使用连续的输入文本流(与固定文本密码相反)来分类击键模式。这导致了这些分类器所使用的特性和算法的差异。但从概念上讲，不加区分的K-means++对手也应该非常适合针对自由文本分类器生成对抗性样本。</p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> WNSP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python面向对象</title>
      <link href="/2019/03/14/python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/"/>
      <url>/2019/03/14/python%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/</url>
      
        <content type="html"><![CDATA[<h3 id="属性命名"><a href="#属性命名" class="headerlink" title="属性命名"></a>属性命名</h3><ul><li>属性以双下划线开头，类内变量，实例无法访问。但可以通过某些方式访问，例如Student例中定义了__name变量，可以用_Student_name来实现访问，但不建议，因为不同的解释器的转化方式不一样。</li><li>单下划线可以打开，但需要注意不能随意更改。</li><li>双下划线结尾与开头，特殊变量，类内可以访问，实例不知。</li><li><h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3>开闭原则：定义一个类Animal及其多个之类Dog/Cat/…，当定义一个函数或操作时：</li></ul><ul><li>对扩展开放：允许新增Animal的子类；</li><li>对修改封闭：不需要修改依赖Animal类型的run_twice()等函数，仍然可以传入Dog/Cat等类。<br>事实上，不需要继承也可以实现多态————鸭子类型。</li></ul><h3 id="若干方法"><a href="#若干方法" class="headerlink" title="若干方法"></a>若干方法</h3><ul><li>isinstance(object,class) 判断是否属于某个类</li><li>dir() 列举出一个对象的属性和方法</li><li>getattr()、setattr()、hasattr()可以获得、添加、查询是否需要某个属性<ul><li>__slots__ 限制可以添加的属性，__slots__ = (‘name’, ‘age’) # 用tuple定义允许绑定的属性名称</li></ul></li><li>装饰器</li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kaggle相关</title>
      <link href="/2019/03/14/kaggle%E7%9B%B8%E5%85%B3/"/>
      <url>/2019/03/14/kaggle%E7%9B%B8%E5%85%B3/</url>
      
        <content type="html"><![CDATA[<h1 id="如何在-Kaggle-首战中进入前-10"><a href="#如何在-Kaggle-首战中进入前-10" class="headerlink" title="如何在 Kaggle 首战中进入前 10%"></a>如何在 Kaggle 首战中进入前 10%</h1><p><a href="https://dnc1994.com/2016/04/rank-10-percent-in-first-kaggle-competition/" target="_blank" rel="noopener">原文</a></p><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><h3 id="Exploration-Data-Analysis-EDA"><a href="#Exploration-Data-Analysis-EDA" class="headerlink" title="Exploration Data Analysis(EDA)"></a>Exploration Data Analysis(EDA)</h3><h4 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h4><p>matplotlib + seaborn</p><ul><li>查看目标变量的分布。当分布不平衡时，根据评分标准和具体模型的使用不同，可能会严重影响性能。</li><li>对 Numerical Variable，可以用 Box Plot 来直观地查看它的分布。</li><li>对于坐标类数据，可以用 Scatter Plot 来查看它们的分布趋势和是否有离群点的存在。</li><li>对于分类问题，将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助。</li><li>绘制变量之间两两的分布和相关度图表。</li></ul><p><a href="https://www.kaggle.com/benhamner/python-data-visualizations" target="_blank" rel="noopener">example_visualization</a></p><h4 id="Statistical-Tests"><a href="#Statistical-Tests" class="headerlink" title="Statistical Tests"></a>Statistical Tests</h4><p>可视化为定性，这里专注于定量，例如对于新创造的特征，可以将其加入原模型当中，看结果的变化。</p><p>在某些比赛中，由于数据分布比较奇葩或是噪声过强，Public LB(Leader board)的分数可能会跟 Local CV(Cross Validation)的结果相去甚远。可以根据一些统计测试的结果来粗略地建立一个阈值，用来衡量一次分数的提高究竟是实质的提高还是由于数据的随机性导致的。</p><h3 id="Data-Preprossing"><a href="#Data-Preprossing" class="headerlink" title="Data Preprossing"></a>Data Preprossing</h3><p>处理策略主要依赖于EDA中得到的结论。</p><ul><li>有时数据会分散在几个不同的文件中，需要 Join 起来。</li><li>处理 Missing Data。</li><li>处理 Outlier。</li><li>必要时转换某些 Categorical Variable 的表示方式。例如应用one-hot encoding(pd.get_dummies)将categorical variable转化为数字变量。</li><li>有些 Float 变量可能是从未知的 Int 变量转换得到的，这个过程中发生精度损失会在数据中产生不必要的 Noise，即两个数值原本是相同的却在小数点后某一位开始有不同。这对 Model 可能会产生很负面的影响，需要设法去除或者减弱 Noise。</li></ul><h3 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h3><h4 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h4><p>总的来说，应该<strong>生成尽量多的 Feature，相信 Model 能够挑出最有用的 Feature</strong>。但有时先做一遍 Feature Selection 也能带来一些好处：</p><ul><li>Feature 越少，训练越快。</li><li>有些 Feature 之间可能存在线性关系，影响 Model 的性能。</li><li>通过挑选出最重要的 Feature，可以将它们之间进行各种运算和操作的结果作为新的 Feature，可能带来意外的提高。</li><li>Feature Selection 最实用的方法也就是看 Random Forest 训练完以后得到的 Feature Importance 了。其他有一些更复杂的算法在理论上更加 Robust，但是缺乏实用高效的实现。从原理上来讲，增加 Random Forest 中树的数量可以在一定程度上加强其对于 Noisy Data 的 Robustness。</li></ul><p>看 Feature Importance 对于某些数据经过脱敏处理的比赛尤其重要。这可以免得你浪费大把时间在琢磨一个不重要的变量的意义上。(脱敏：数据脱敏(Data Masking),又称数据漂白、数据去隐私化或数据变形。百度百科对数据脱敏的定义为：指对某些敏感信息通过脱敏规则进行数据的变形，实现敏感隐私数据的可靠保护。在涉及客户安全数据或者一些商业性敏感数据的情况下，在不违反系统规则条件下，对真实数据进行改造并提供测试使用，如身份证号、手机号、卡号、客户号等个人信息都需要进行数据脱敏。)</p><h4 id="Feature-Encoding"><a href="#Feature-Encoding" class="headerlink" title="Feature Encoding"></a>Feature Encoding</h4><p>假设有一个 Categorical Variable 一共有几万个取值可能，那么创建 Dummy Variables 的方法就不可行了。这时一个比较好的方法是根据 Feature Importance 或是这些取值本身在数据中的出现频率，为最重要（比如说前 95% 的 Importance）那些取值（有很大可能只有几个或是十几个）创建 Dummy Variables，而所有其他取值都归到一个“其他”类里面。</p><h3 id="Model-Selection"><a href="#Model-Selection" class="headerlink" title="Model Selection"></a>Model Selection</h3><p>Base Model:</p><ul><li>SVM</li><li>Linear Regression</li><li>Logistic Regression</li><li>Neural Networks</li></ul><p>Most Used Models:</p><ul><li>Gradient Boosting</li><li>Random Forest</li><li><p>Extra Randomized Trees</p><p><strong>XGBoost</strong></p></li></ul><h4 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h4><p>通过Grid Search来确定模型的最佳参数。<br>e.g.</p><ul><li>sklearn 的 RandomForestClassifier 来说，比较重要的就是随机森林中树的数量 n_estimators 以及在训练每棵树时最多选择的特征数量 max_features。</li><li><p>Xgboost 的调参。通常认为对它性能影响较大的参数有：</p><ul><li>eta：每次迭代完成后更新权重时的步长。越小训练越慢。</li><li>num_round：总共迭代的次数。</li><li>subsample：训练每棵树时用来训练的数据占全部的比例。用于防止 Overfitting。</li><li>colsample_bytree：训练每棵树时用来训练的特征的比例，类似 RandomForestClassifier 的 max_features。</li><li>max_depth：每棵树的最大深度限制。与 Random Forest 不同，Gradient Boosting 如果不对深度加以限制，最终是会 Overfit 的。</li><li>early_stopping_rounds：用于控制在 Out Of Sample 的验证集上连续多少个迭代的分数都没有提高后就提前终止训练。用于防止 Overfitting。</li></ul><p>一般的调参步骤是：</p><ol><li>将训练数据的一部分划出来作为验证集。</li><li>先将 eta 设得比较高（比如 0.1），num_round 设为 300 ~ 500。</li><li>用 Grid Search 对其他参数进行搜索。</li><li>逐步将 eta 降低，找到最佳值。</li><li>以验证集为 watchlist，用找到的最佳参数组合重新在训练集上训练。注意观察算法的输出，看每次迭代后在验证集上分数的变化情况，从而得到最佳的 early_stopping_rounds。</li></ol><p><em>所有具有随机性的 Model 一般都会有一个 seed 或是 random_state 参数用于控制随机种子。得到一个好的 Model 后，在记录参数时务必也记录下这个值，从而能够在之后重现 Model。</em></p></li></ul><h4 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross Validation"></a>Cross Validation</h4><p>一般5-fold。</p><p>fold越多训练越慢。</p><h4 id="Ensemble-Generation"><a href="#Ensemble-Generation" class="headerlink" title="Ensemble Generation"></a>Ensemble Generation</h4><p>常见的 Ensemble 方法有这么几种：</p><ul><li>Bagging：使用训练数据的不同随机子集来训练每个 Base Model，最后进行每个 Base Model 权重相同的 Vote。也即 Random Forest 的原理。</li><li>Boosting：迭代地训练 Base Model，每次根据上一个迭代中预测错误的情况修改训练样本的权重。也即 Gradient Boosting 的原理。比 Bagging 效果好，但更容易 Overfit。</li><li>Blending：用不相交的数据训练不同的 Base Model，将它们的输出取（加权）平均。实现简单，但对训练数据利用少了。</li><li>Stacking：接下来会详细介绍。</li></ul><p>从理论上讲，Ensemble 要成功，有两个要素：</p><ul><li>Base Model 之间的相关性要尽可能的小。这就是为什么非 Tree-based Model 往往表现不是最好但还是要将它们包括在 Ensemble 里面的原因。Ensemble 的 Diversity 越大，最终 Model 的 Bias 就越低。</li><li>Base Model 之间的性能表现不能差距太大。这其实是一个 Trade-off，在实际中很有可能表现相近的 Model 只有寥寥几个而且它们之间相关性还不低。但是实践告诉我们即使在这种情况下 Ensemble 还是能大幅提高成绩。</li></ul><h3 id="Pipeline"><a href="#Pipeline" class="headerlink" title="Pipeline"></a>Pipeline</h3><p>workflow比较复杂，因此一个高自动化的pipeline比较重要。</p><p>这里是以一个例子：<a href="https://github.com/ChenglongChen/Kaggle_CrowdFlower" target="_blank" rel="noopener">example</a></p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Kaggle </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LightGBM</title>
      <link href="/2019/03/14/LightGBM/"/>
      <url>/2019/03/14/LightGBM/</url>
      
        <content type="html"><![CDATA[<h1 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h1><ul><li>可以接受categorical features：LightGBM 可以直接使用 categorical features（分类特征）作为 input（输入）. 它不需要被转换成 one-hot coding（独热编码）, 并且它比 one-hot coding（独热编码）更快（约快上 8 倍）。注意: 在你构造 Dataset 之前, 你应该将分类特征转换为 int 类型的值.</li></ul><h1 id="xgboost的优点"><a href="#xgboost的优点" class="headerlink" title="xgboost的优点"></a>xgboost的优点</h1><ul><li>XGB利用了二阶梯度来对节点进行划分，相对其他GBM来说，精度更加高。</li><li>利用局部近似算法对分裂节点的贪心算法优化，取适当的eps时，可以保持算法的性能且提高算法的运算速度。</li><li>在损失函数中加入了L1/L2项，控制模型的复杂度，提高模型的鲁棒性。</li><li>提供并行计算能力，主要是在树节点求不同的候选的分裂点的Gain Infomation（分裂后，损失函数的差值）</li><li>Tree Shrinkage，column subsampling等不同的处理细节。</li></ul><h1 id="xgboost的缺点"><a href="#xgboost的缺点" class="headerlink" title="xgboost的缺点"></a>xgboost的缺点</h1><ul><li>每次迭代需要多次遍历整个训练数据。若读入内存，则对训练数据的大小有限制；不读入内存的话，非常慢。</li><li>预排序方法：预排序需要大量的空间站占用和时间占用：需要保存数据的特征值，也要保存特征排序的结果，即需要两倍数据的内存占用；遍历每个分割点需要计算每个分割点的增益。此外，对cache优化不友好。在预排序后，特征对梯度的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对cache进行优化。同时，在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的cache miss。</li></ul><h1 id="lightgbm的优化"><a href="#lightgbm的优化" class="headerlink" title="lightgbm的优化"></a>lightgbm的优化</h1><ul><li>基于histogram的决策树算法</li><li>带深度限制的leaf-wise的叶子生长策略</li><li>直方图做差加速</li><li>支持类别特征</li><li>cache命中率优化</li><li>基于直方图的稀疏特征优化</li><li>多线程优化</li></ul><h2 id="Histogram算法"><a href="#Histogram算法" class="headerlink" title="Histogram算法"></a>Histogram算法</h2><p>先把连续的浮点特征值离散化成k个整数，同时构造一个宽度为k的直方图。在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</p><h2 id="带深度限制的leaf-wise的叶子生长策略"><a href="#带深度限制的leaf-wise的叶子生长策略" class="headerlink" title="带深度限制的leaf-wise的叶子生长策略"></a>带深度限制的leaf-wise的叶子生长策略</h2><p>Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。</p><p>Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。</p><h2 id="多线程优化"><a href="#多线程优化" class="headerlink" title="多线程优化"></a>多线程优化</h2><h3 id="特征并行"><a href="#特征并行" class="headerlink" title="特征并行"></a>特征并行</h3><h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><p>减少了数据通信的成本</p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> DOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Naive Bayes及其sklearn实现</title>
      <link href="/2019/03/14/Naive%20Bayes/"/>
      <url>/2019/03/14/Naive%20Bayes/</url>
      
        <content type="html"><![CDATA[<p>P(B|A) = P(A|B)*P(B)/P(A)</p><p>朴素：特征之间相互独立</p><h1 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h1><ol><li>x = {a1, a2, …, am}为待分类项，a是特征。</li><li>类别集合C = {y1, …, yn}.</li><li>计算P(y1|x), P(y2|x) …</li><li>P(yk|x) = max{P(yi|x)}，则x属于yk类</li></ol><p><strong>总结：</strong>某类在待分类项出现的条件下的概率是所有类中最大的，这个分类项就属于这一类。</p><p>e.g.判断一个黑人来自哪个洲，求取每个洲黑人的比率，非洲最高，选非洲。</p><p>其中x = {a1, a2, …, am}，即P(C|a1,a2…) = P(C)*P(a1,a2,…|C)/P(a1,a2…)。posterior = prior * likelihood / evidence, 这里evidence是常数，不影响。</p><p>—–&gt;求解P(C) * P(a1,a2,a3…|C)</p><p>—–&gt;链式法则：P(C) * P(a2,a3…|C, a1) * P(a1|C)</p><p>—&gt; …</p><p>—&gt; P(C) * P(a1|C) * P(a2|C, a1) * P(a3|C, a1, a2)…<br>由于特征之间的相互独立性，a2发生于a1无关，转化为</p><p>—&gt; P(C) * P(a1|C) * P(a2|C) …  * P(am|C)</p><p>—–&gt;问题转化为求取条件概率：</p><ol><li>找到一个已知分类的待分类项集合，这个集合叫做训练样本集。</li><li>统计得到在各类别下各个特征属性的条件概率估计。</li></ol>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GBDT &amp; XGBoost</title>
      <link href="/2019/03/14/GBDT%20&amp;%20XGBoost/"/>
      <url>/2019/03/14/GBDT%20&amp;%20XGBoost/</url>
      
        <content type="html"><![CDATA[<p><a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html" target="_blank" rel="noopener">Doc</a><br><a href="https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf" target="_blank" rel="noopener">Slides</a></p><h1 id="为何要推导出目标函数而不是直接增加树"><a href="#为何要推导出目标函数而不是直接增加树" class="headerlink" title="为何要推导出目标函数而不是直接增加树"></a>为何要推导出目标函数而不是直接增加树</h1><p><img src="http://i.imgur.com/quPhp1K.png" alt="Objective function"></p><ul><li>理论上：搞清楚learning的目的，以及其收敛性。</li><li>工程上：<ul><li>gi和hi是对loss function的一次、二次导</li><li>目标函数以及整个学习过程只依赖于gi和hi</li><li>可以根据实际问题，自定义loss function</li></ul></li></ul><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p><img src="http://i.imgur.com/L7PhJwO.png" alt="Summary"></p><h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>$$ \text{obj} = \sum_{i=1}^n l(y_i, \hat{y}<em>i^{(t)}) + \sum</em>{i=1}^t\Omega(f_i)$$<br>l为loss，\ \Omega \ 为正则项</p><ul><li>loss：采用加法策略，第t颗树时：<br>$$ \hat{y}_i^{(0)} = 0 $$<br>$$ \hat{y}_i^{(1)} = f_1(x_i) = \hat{y}_i^{(0)} + f_1(x_i) $$<br>$$ \hat{y}_i^{(2)} = f_1(x_i) + f_2(x_i)= \hat{y}_i^{(1)} + f_2(x_i) $$<br>$$ \dots $$<br>$$ \hat{y}<em>i^{(t)} = \sum</em>{k=1}^t f_k(x_i)= \hat{y}_i^{(t-1)} + f_t(x_i) $$<br>在添加第t颗树时，需要优化的目标函数为：<br>$$ \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t) $$<br>其中h和f：<br>$$ g_i = \partial_{\hat{y}_i^{(t-1)}} l(y_i, \hat{y}_i^{(t-1)}) $$<br>$$ h_i = \partial_{\hat{y}_i^{(t-1)}}^2 l(y_i, \hat{y}_i^{(t-1)}) $$<br>note: 是对谁的导</li><li>正则项：复杂度：<br>$$ \Omega(f) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^T w_j^2 $$<br>其中w是叶子上的score vector，T是叶子数量</li></ul><h2 id="DART-Booster"><a href="#DART-Booster" class="headerlink" title="DART Booster"></a>DART Booster</h2><p>为了解决过拟合，会随机drop trees:</p><ul><li>训练速度可能慢于gbtree</li><li>由于随机性，早停可能不稳定</li></ul><h1 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h1><h2 id="Monotonic-Constraints单调性限制"><a href="#Monotonic-Constraints单调性限制" class="headerlink" title="Monotonic Constraints单调性限制"></a>Monotonic Constraints单调性限制</h2><ul><li><p>一个可选特性:<br>会限制模型的结果按照某个特征 单调的进行增减</p><p>也就是说可以降低模型对数据的敏感度，如果明确已知某个特征与预测结果呈单调关系时，那在生成模型的时候就会跟特征数据的单调性有关。</p></li></ul><h2 id="Feature-Interaction-Constraints单调性限制"><a href="#Feature-Interaction-Constraints单调性限制" class="headerlink" title="Feature Interaction Constraints单调性限制"></a>Feature Interaction Constraints单调性限制</h2><ul><li><p>一个可选特性：<br>不用时，在tree生成的时候，一棵树上的节点会无限制地选用多个特征</p><p>设置此特性时，可以规定，哪些特征可以有interaction（一般独立变量之间可以interaction，非独立变量的话可能会引入噪声）</p></li><li>好处：<ul><li>预测时更小的噪声</li><li>对模型更好地控制</li></ul></li></ul><h2 id="Instance-Weight-File"><a href="#Instance-Weight-File" class="headerlink" title="Instance Weight File"></a>Instance Weight File</h2><ul><li>规定了模型训练时data中每一条instance的权重</li><li>有些instance质量较差，或与前一示例相比变化不大，所以可以调节其所占权重</li></ul><h1 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h1><h2 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h2><p>与overfitting有关的参数：</p><ul><li>直接控制模型复杂度：max_depth, min_child_weight and gamma.</li><li>增加模型随机性以使得模型对噪声有更强的鲁棒性：<ul><li>subsample and colsample_bytree. </li><li>Reduce stepsize eta. Remember to increase num_round when you do so.</li></ul></li></ul><h2 id="Imbalanced-Dataset"><a href="#Imbalanced-Dataset" class="headerlink" title="Imbalanced Dataset"></a>Imbalanced Dataset</h2><ul><li>只关注测量指标的大小<ul><li>平衡数据集 via scale_pos_weight</li><li>使用AUC作为metric</li></ul></li><li>关注预测正确的概率<ul><li>此时不能re-balance数据集</li><li>Set parameter max_delta_step to a finite number (say 1) to help convergence</li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> DOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decisiong Tree:ID3 C4.5 CART</title>
      <link href="/2019/03/14/Decision%20Tree/"/>
      <url>/2019/03/14/Decision%20Tree/</url>
      
        <content type="html"><![CDATA[<ol><li><p>决策树<br><strong><em>问题：如何挑选用于分裂节点的特征–&gt;ID3 C4.5 …(一个标准：使分裂出来的节点尽可能纯，即一个分支尽可能属于同类)</em></strong></p></li><li><p>ID3<br><em><strong>信息增益</strong></em></p><p> 信息增益 = 信息熵 - 条件熵</p><ul><li>信息增益：针对每个 <em><strong>属性</strong></em></li><li>信息熵：整个样本空间的不确定度。其中Pk一定是label取值的概率。</li><li><p>条件熵：给定某个属性，求其信息熵</p><p>–&gt; 问题：某属性所包括的类别越多，信息增益越大。极限：每个类别仅有1个实例（label数量为1），log p = log1 = 0， 所以最终条件熵=0。或：属性类别越多，条件熵越小，其纯度越高。</p><p>–&gt; 信息增益准则其实是对可取值数目较多的属性有所偏好！</p><p>–&gt; 泛化能力不强</p></li></ul></li><li><p>C4.5 <em><strong>信息增益率+信息增益</strong></em></p><p> 属性a的信息增益率 = 属性a的信息增益 / a的某个固有统计量IV(a)</p><p> <img src="https://pic4.zhimg.com/80/v2-812104c0291d20935e910919a9fa5c27_hd.png" alt="IV(a)公式"></p><p> V为a的取值数目。<br> （实际上是属性a的信息熵）</p><ul><li>直接使用信息增益率：偏好取值数目小的属性。</li><li>先选择高于平均水平信息增益的属性，再选择最高信息增益率的属性。</li></ul></li><li><p>CART <em><strong>基尼系数+MAE/MSE</strong></em></p><p>与ID3、C4.5的不同：形成二叉树，因此 –&gt; 既要确定要分割的属性，也要确定要分割的值</p><p>回归树和分类树的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。</p><p>回归树和分类树在处理连续特征的时候有区别：</p><ul><li>回归树：MAE/MSE<ul><li>example(MSE)：<blockquote><ol><li>考虑数据集 D 上的所有特征 j，遍历每一个特征下所有可能的取值或者切分点 s，将数据集 D 划分成两部分 D1 和 D2</li><li>分别计算上述两个子集的平方误差和，选择最小的平方误差对应的特征与分割点，生成两个子节点。</li><li>对上述两个子节点递归调用步骤1 2,直到满足停止条件。</li></ol></blockquote></li></ul></li><li><p>分类树：(Gini)</p><p>$$ Gini(A) = \sum_{k=1}^K p_k(1-p_k)$$<br><img src="https://img-blog.csdn.net/20150109184544578?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvQW5kcm9pZGx1c2hhbmdkZXJlbg==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="某属性A的基尼系数"><br>基尼系数越小，纯度越高</p></li></ul><blockquote><ol><li>对每个特征 A，对它的所有可能取值 a，将数据集分为 A＝a，和 A!＝a 两个子集，计算集合 D 的基尼指数：<br>Gini(A) = D1/D <em> Gini(D1) + D2/D </em> Gini(D2)</li><li>遍历所有的特征 A，计算其所有可能取值 a 的基尼指数，选择 D 的基尼指数最小值对应的特征及切分点作为最优的划分，将数据分为两个子集。</li><li>对上述两个子节点递归调用步骤1 2, 直到满足停止条件。</li><li>生成 CART 决策树。</li></ol></blockquote><ul><li>使用基尼系数处理离散特征：不停地二分</li><li><p>使用基尼系数处理连续特征，将所有n个取值从大到小排列，得到n-1个切分点，求所有的切分点的基尼系数，取最小的，做到连续特征的离散化<br>  停止条件有：</p><ol><li>节点中的样本个数小于预定阈值;</li><li>样本集的Gini系数小于预定阈值（此时样本基本属于同一类）;</li><li>没有更多特征。</li></ol></li><li><p>剪枝</p></li><li>例子：<a href="https://www.jianshu.com/p/b90a9ce05b28" target="_blank" rel="noopener">example</a></li></ul></li><li><p>控制决策树过拟合的方法</p><ul><li>剪枝</li><li>控制终止条件，避免树形结构过细</li><li>构建随机森林</li></ul></li><li><p>剪枝</p></li></ol><ul><li>前剪枝：<ul><li>在生成树的时候，控制终止条件：最小样本数；样本集的最小基尼系数；树的深度</li><li>由于预剪枝不必生成整棵决策树，且算法相对简单，效率很高，适合解决大规模问题。但是尽管这一方法看起来很直接，但是怎样精确地估计何时停止树的增长是相当困难的。</li><li>预剪枝有一个缺点， 即视野效果问题 。 也就是说在相同的标准下，也许当前的扩展会造成过度拟合训练数据，但是更进一步的扩展能够满足要求，也有可能准确地拟合训练数据。这将使得算法过早地停止决策树的构造。</li></ul></li><li>后剪枝<ul><li>在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。</li></ul></li></ul><p>1）REP-错误率降低剪枝</p><p>对于决策树T的每棵非叶子树S, 用叶子替代这棵子树. 如果S被叶子替代后形成的新树关于D的误差等于或小于S关于D所产生的误差, 则用叶子替代子树S。</p><p><strong>优点：</strong></p><ul><li>REP 是当前最简单的事后剪枝方法之一。</li><li>它的计算复杂性是线性的。</li><li>和原始决策树相比，修剪后的决策树对未来新事例的预测偏差较小。</li></ul><p><strong>缺点：</strong></p><ul><li>但在数据量较少的情况下很少应用. REP方法趋于过拟合( overfitting) , 这是因为训练数据集中存在的特性在剪枝过程中都被忽略了, 当剪枝数据集比训练数据集小得多时 , 这个问题特别值得注意.</li></ul><p>2）PEP-悲观剪枝</p><ul><li><p>CCP-代价复杂度剪枝</p></li><li><p>MEP-最小错误剪枝</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Decision Tree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DBSCAN以及sklearn实现DBSCAN</title>
      <link href="/2019/03/14/DBSCAN/"/>
      <url>/2019/03/14/DBSCAN/</url>
      
        <content type="html"><![CDATA[<p><a href="https://www.cnblogs.com/pinard/p/6208966.html" target="_blank" rel="noopener">原文1</a><br><a href="https://www.cnblogs.com/pinard/p/6217852.html" target="_blank" rel="noopener">原文2</a><br>DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的<strong>密度聚类算法</strong>，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN<strong>既可以适用于凸样本集，也可以适用于非凸样本集</strong>。</p><h1 id="密度聚类"><a href="#密度聚类" class="headerlink" title="密度聚类"></a>密度聚类</h1><p>其原理为：同一类别的样本，其样本分布一定是紧密的；可以将各组紧密相连的样本划分为不同的类别来得到聚类类别结果。</p><h1 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h1><h2 id="关键概念"><a href="#关键概念" class="headerlink" title="关键概念"></a>关键概念</h2><p>参数(ϵ, MinPts)描述领域的样本分布紧密程度，其中ϵ描述了某一样本的领域<strong>距离阈值</strong>，MinPts描述某一样本的距离为ϵ的领域中样本<strong>个数的阈值</strong>。</p><p>假设样本集是D=(x1,x2,…,xm),则DBSCAN具体的密度描述定义如下：</p><ol><li>ϵ-邻域：对于xj∈D，其ϵ-邻域包含样本集D中与xj的距离不大于ϵ的子样本集，即Nϵ(xj)={xi∈D|distance(xi,xj)≤ϵ}, 这个子样本集的个数记为|Nϵ(xj)|</li><li>核心对象：对于任一样本xj∈D，如果其ϵ-邻域对应的Nϵ(xj)至少包含MinPts个样本，即如果|Nϵ(xj)|≥MinPts，则xj是核心对象。</li><li>密度直达：如果xi位于xj的ϵ-邻域中，且xj是核心对象，则称xi由xj密度直达。</li><li>密度可达：对于xi和xj,如果存在样本样本序列p1,p2,…,pT,满足p1=xi,pT=xj, 且pt+1由pt密度直达，则称xj由xi密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本p1,p2,…,pT−1均为核心对象，因为只有核心对象才能使其他样本密度直达。</li><li>密度相连：对于xi和xj,如果存在核心对象样本xk，使xi和xj均由xk密度可达，则称xi和xj密度相连。注意密度相连关系是满足对称性的。</li></ol><p>图中MinPts = 5。红点为核心对象。<br><img src="https://images2015.cnblogs.com/blog/1042406/201612/1042406-20161222112847323-1346197243.png" alt="示意图"></p><h2 id="聚类思想"><a href="#聚类思想" class="headerlink" title="聚类思想"></a>聚类思想</h2><p>由密度可达关系导出的最大密度相连的样本集合，即为最终聚类的一个类别。</p><p>方法：任意选择一个没有类别的核心对象作为种子，然后找到该核心对象密度可达的样本集合，为一个聚类。接着选择另一个没有类比的核心对象…直到所有核心对象都有类别。</p><p>问题：</p><ol><li>outlier.不在任何一个核心对象周围的点定义为异常样本点或噪声点，不考虑。</li><li>距离。少量样本而言，搜索周围样本一般用最近邻的方法；大量样本，可以用KD树，球树等搜索最近邻。</li><li>若某样本到两个核心对象的距离都小于ϵ，但这两个核心对象不可达，此时采取先来后到原则，标记其为先聚类的cluster类别。</li></ol><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>输入：样本集D=(x1,x2,…,xm)，邻域参数(ϵ,MinPts), 样本距离度量方式</p><p>输出： 簇划分C.　</p><ol><li>初始化核心对象集合Ω=∅, 初始化聚类簇数k=0，初始化未访问样本集合Γ = D,  簇划分C = ∅</li><li>对于j=1,2,…m, 按下面的步骤找出所有的核心对象：<ul><li>通过距离度量方式，找到样本xj的ϵ-邻域子样本集Nϵ(xj)</li><li>如果子样本集样本个数满足|Nϵ(xj)|≥MinPts， 将样本xj加入核心对象样本集合：Ω=Ω∪{xj}</li></ul></li><li>如果核心对象集合Ω=∅，则算法结束，否则转入步骤4.</li><li>在核心对象集合Ω中，随机选择一个核心对象o，初始化当前簇核心对象队列Ωcur={o}, 初始化类别序号k=k+1，初始化当前簇样本集合Ck={o}, 更新未访问样本集合Γ=Γ−{o}</li><li>如果当前簇核心对象队列Ωcur=∅，则当前聚类簇Ck生成完毕, 更新簇划分C={C1,C2,…,Ck}, 更新核心对象集合Ω=Ω−Ck， 转入步骤3。</li><li>在当前簇核心对象队列Ωcur中取出一个核心对象o′,通过邻域距离阈值ϵ找出所有的ϵ-邻域子样本集Nϵ(o′)，令Δ=Nϵ(o′)∩Γ, 更新当前簇样本集合Ck=Ck∪Δ, 更新未访问样本集合Γ=Γ−Δ,  更新Ωcur=Ωcur∪(Δ∩Ω)−o′，转入步骤5.</li></ol><p>输出结果为： 簇划分C={C1,C2,…,Ck}</p><h1 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h1><p>对比图：<img src="https://img-blog.csdn.net/20170419143546349?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvc2luYXRfMjY5MTczODM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="与其他算法的对比图"><br>一般用于数据集稠密时的情况，或数据集是非凸的。</p><p>DBSCAN的主要优点有：</p><ol><li>可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。</li><li>可以在聚类的同时发现异常点，对数据集中的异常点不敏感。</li><li>聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。</li></ol><p>DBSCAN的主要缺点有：</p><ol><li>如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。</li><li>如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。</li><li>调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。</li></ol><h1 id="sklearn-cluster-DBSCAN"><a href="#sklearn-cluster-DBSCAN" class="headerlink" title="sklearn.cluster.DBSCAN"></a>sklearn.cluster.DBSCAN</h1><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><p>按其算法，包括DBSCAN本身的参数，以及求取最近邻时的参数。</p><ol><li>eps：DBSCAN算法参数，ϵ-邻域的距离阈值。默认值是0.5.eps过大，则更多的点会落在核心对象的ϵ-邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。</li><li>min_samples：DBSCAN算法参数，上文的MinPts。默认值是5.通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。</li><li>metric：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。可以使用的距离度量参数有：<ul><li>欧式距离 “euclidean”</li><li>曼哈顿距离 “manhattan”</li><li>切比雪夫距离“chebyshev”</li><li>闵可夫斯基距离 “minkowski”</li><li>带权重闵可夫斯基距离 “wminkowski”</li><li>标准化欧式距离 “seuclidean”</li><li>马氏距离“mahalanobis”</li></ul></li><li>algorithm：最近邻搜索算法参数，算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现，‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。个人的经验，一般情况使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用”auto”建树时间可能会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。</li><li>leaf_size：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 因为这个值一般只影响算法的运行速度和使用内存大小，因此一般情况下可以不管它。</li><li>p: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。</li><li>n_jobs ：使用CPU格式，-1代表全开。</li></ol><p>输出：</p><ul><li>core_sample_indices_:核心样本指数。（此参数在代码中有详细的解释）</li><li>labels_:数据集中每个点的集合标签给,噪声点标签为-1。</li><li>components_ ：核心样本的副本</li></ul><p>主要是<strong>eps和min_samples</strong>的调参。</p><h2 id="代码实例"><a href="#代码实例" class="headerlink" title="代码实例"></a>代码实例</h2><p>原文2中有。<br><a href="https://www.cnblogs.com/pinard/p/6217852.html" target="_blank" rel="noopener">原文2</a></p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> Clustering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DS Competition_Coursera#1</title>
      <link href="/2019/03/14/D%20S%20competition_Coursera#1/"/>
      <url>/2019/03/14/D%20S%20competition_Coursera#1/</url>
      
        <content type="html"><![CDATA[<h1 id="Recap"><a href="#Recap" class="headerlink" title="Recap"></a>Recap</h1><h2 id="Linear-model"><a href="#Linear-model" class="headerlink" title="Linear model"></a>Linear model</h2><p>非常适合于高维稀疏数据<br>e.g.<br>SVM, Logistic</p><p>SVM也是非线性</p><h2 id="Tree-based"><a href="#Tree-based" class="headerlink" title="Tree-based"></a>Tree-based</h2><p>Decision Tree, Random Forest, GBDT</p>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CS231n-imgClassification</title>
      <link href="/2019/03/14/CS231n-img_classification/"/>
      <url>/2019/03/14/CS231n-img_classification/</url>
      
        <content type="html"><![CDATA[<h1 id="kNNs"><a href="#kNNs" class="headerlink" title="kNNs"></a>kNNs</h1><ul><li>没有参数需要训练</li><li>只计算与训练样本之间的距离</li></ul><h1 id="Linear-Classfication"><a href="#Linear-Classfication" class="headerlink" title="Linear Classfication"></a>Linear Classfication</h1><ul><li>可以将其理解为<strong>模板匹配</strong>，即最终的权重矩阵W的每一行为所学习得到的类别的模板，将其与test img做点积即做一次模板匹配。每个点积的值即为该分类所得的分数。</li></ul><p><img src="http://cs231n.github.io/assets/imagemap.jpg" alt="ex0"></p><p>  将权重矩阵的每一行重组后可以得到其信息，可以看出类似于某一类的模板：<br><img src="http://cs231n.github.io/assets/templates.jpg" alt="ex1"></p><blockquote><p>可以看出当同一个类别有很多图片时，得到的模板是这些图片的平均。也暴露了它不适合用于多变性复杂的分类。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ML </tag>
            
            <tag> CS231n </tag>
            
            <tag> kNN </tag>
            
            <tag> SVM </tag>
            
            <tag> imgClassification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN</title>
      <link href="/2019/03/14/CNN/"/>
      <url>/2019/03/14/CNN/</url>
      
        <content type="html"><![CDATA[<p><a href="https://zhuanlan.zhihu.com/p/27642620" target="_blank" rel="noopener">原贴</a></p><h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><p>卷积神经网络大致就是covolutional layer, pooling layer, ReLu layer, fully-connected layer的组合，例如下图所示的结构。<br><img src="https://pic4.zhimg.com/80/v2-cf87890eb8f2358f23a1ac78eb764257_hd.png" alt="ex-1"></p><h3 id="图片的识别"><a href="#图片的识别" class="headerlink" title="图片的识别"></a>图片的识别</h3><ul><li>生物所看到的景象并非世界的原貌，而是长期进化出来的适合自己生存环境的一种感知方式</li><li>画面识别实际上是寻找/学习动物的视觉关联形式（即将能量与视觉关联在一起的方式）</li><li>画面的识别取决于：<ul><li>图片本身</li><li>被如何观察</li></ul></li><li>图像不变性：<ul><li>rotation</li><li>viewpoint</li><li>size</li><li>illumination</li><li>…<h3 id="前馈的不足"><a href="#前馈的不足" class="headerlink" title="前馈的不足"></a>前馈的不足</h3></li></ul></li><li>当出现上述variance时，前馈无法做到适应，即前馈只能对同样的内容进行识别，若出现其他情况时，只能增加样本重新训练</li><li>解决方法可以是让图片中不同的位置有相同的权重——<strong>共享权重</strong><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><h4 id="局部连接"><a href="#局部连接" class="headerlink" title="局部连接"></a>局部连接</h4></li><li><strong>空间共享</strong>（引入的先验知识）</li><li><strong>局部连接</strong>（得到的下一层节点与该层并非全连接）</li><li>depth上是<strong>全连接</strong>的<blockquote><p>每个filter会在width维, height维上，以局部连接和空间共享，并贯串整个depth维的方式得到一个Feature Map。</p></blockquote></li></ul><h5 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h5><p><img src="https://pic3.zhimg.com/80/v2-23db15ec3f783bbb5cf811711e46dbba_hd.png" alt="cnn_example"></p><ul><li>在输入depth为1时：被filter size为2x2所圈中的4个输入节点连接到1个输出节点上。</li><li>在输入depth为3时：被filter size为2x2，但是贯串3个channels后，所圈中的12个输入节点连接到1个输出节点上。</li><li>在输入depth为n时：2x2xn个输入节点连接到1个输出节点上。<blockquote><p>三个channels的权重并不共享。 即当深度变为3后，权重也跟着扩增到了三组。</p></blockquote></li></ul><h5 id="zero-padding"><a href="#zero-padding" class="headerlink" title="zero padding"></a>zero padding</h5><p>有时为了保证feature map与输入层保持同样大小，会添加zero padding，一般3*3的卷积核padding为1，5*5为2</p><p>Feature Map的尺寸等于(input_size + 2 *padding_size − filter_size)/stride+1</p><h4 id="形状、概念抓取"><a href="#形状、概念抓取" class="headerlink" title="形状、概念抓取"></a>形状、概念抓取</h4><ul><li>卷积层可以对基础形状（包括边缘、棱角、模糊等）、对比度、颜色等概念进行抓取</li><li>可以通过多层卷积实现对一个较大区域的抓取</li><li>抓取的特征取决于卷积核的权重，而此权重由网络根据数据学习得到，即CNN会自己学习以什么样的方式观察图片</li><li>可以有多个filter，从而可以学习到多种特征<ul><li>此时卷积层的输出depth也就不是1了</li><li>卷积层的输入输出均为长方体：其中depth与filters个数相同<br><img src="https://pic1.zhimg.com/80/v2-a9983c3cee935b68c73965bc1abe268c_hd.png" alt="ex4"><br><img src="https://pic1.zhimg.com/80/v2-d11e1d2f2c41b6df713573f8155bc324_hd.png" alt="ex2"><h4 id="非线性（以ReLu为例）"><a href="#非线性（以ReLu为例）" class="headerlink" title="非线性（以ReLu为例）"></a>非线性（以ReLu为例）</h4>增强模型的非线性拟合能力<br><img src="https://pic3.zhimg.com/80/v2-54a469b2873542e75abf2bc5d8fcaa1a_hd.png" alt="ex3"><h4 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h4><em>比如以步长为2，2x2的 filter pool</em><br><img src="https://pic4.zhimg.com/80/v2-cd717414dcf32dac4df73c00f1e7c6c3_hd.jpg" alt="ex5"></li></ul></li><li>pooling的主要功能是downsamping，有助减少conv过程中的冗余<h4 id="全连接"><a href="#全连接" class="headerlink" title="全连接"></a>全连接</h4></li><li>当抓取到足以用来识别图片的特征后，接下来的就是如何进行分类。 全连接层（也叫前馈层）就可以用来将最后的输出映射到线性可分的空间。 通常卷积网络的最后会将末端得到的长方体平摊(flatten)成一个长长的向量，并送入全连接层配合输出层进行分类。<h4 id="一些变体中用到的技巧"><a href="#一些变体中用到的技巧" class="headerlink" title="一些变体中用到的技巧"></a>一些变体中用到的技巧</h4></li><li>1x1卷积核：选择不同的个数，用来降维或升维</li><li>残差<blockquote><p>所有的这些技巧都是对各种不变性的满足</p></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
